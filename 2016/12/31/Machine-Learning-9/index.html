<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <meta name="description" content="阿尔托利亚是我老婆~" />
  

  
  
  
  
  
  
  <title>Machine-Learning-9 | SaberDa的幻想乡</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Anomaly DetectionProblem MotivationJust like in other learning problems, we are given a dataset x(1),x(2),&amp;#x2026;,x(m)
We are then given a new example, xtest, and we want to know whether this new exa">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine-Learning-9">
<meta property="og:url" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/index.html">
<meta property="og:site_name" content="SaberDa的幻想乡">
<meta property="og:description" content="Anomaly DetectionProblem MotivationJust like in other learning problems, we are given a dataset x(1),x(2),&amp;#x2026;,x(m)
We are then given a new example, xtest, and we want to know whether this new exa">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/1.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/2.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/3.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/4.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/5.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/6.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/7.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/8.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/9.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/10.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/11.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/12.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/13.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/14.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/15.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/16.png">
<meta property="og:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/17.png">
<meta property="og:updated_time" content="2017-01-29T04:37:31.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine-Learning-9">
<meta name="twitter:description" content="Anomaly DetectionProblem MotivationJust like in other learning problems, we are given a dataset x(1),x(2),&amp;#x2026;,x(m)
We are then given a new example, xtest, and we want to know whether this new exa">
<meta name="twitter:image" content="https://saberda.github.io/2016/12/31/Machine-Learning-9/1.png">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
  <script src='//push.zhanzhang.baidu.com/push.js'></script>
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="SaberDa的幻想乡" rel="home">SaberDa的幻想乡</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">　　iOS/ ML　　|　　二次元　　|　　saberda@qq.com　　|　　从花时间省钱到花钱省时间</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives">所有文章</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">主页</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/编程/">编程</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/iOS/">iOS</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/ML/">ML</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/PGM/">PGM</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/MAC/">MAC</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/御宅文化/">御宅文化</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/LIFE/">LIFE</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-Machine-Learning-9" class="post-Machine-Learning-9 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      Machine-Learning-9
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://saberda.github.io/2016/12/31/Machine-Learning-9/" data-id="cjf3p7ger001sr1b3ysz3s1de" class="leave-reply bdsharebuttonbox" data-cmd="more">Share</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <h2 id="Anomaly-Detection"><a href="#Anomaly-Detection" class="headerlink" title="Anomaly Detection"></a>Anomaly Detection</h2><h2 id="Problem-Motivation"><a href="#Problem-Motivation" class="headerlink" title="Problem Motivation"></a>Problem Motivation</h2><p>Just like in other learning problems, we are given a dataset x<sup>(1)</sup>,x<sup>(2)</sup>,&#x2026;,x<sup>(m)</sup></p>
<p>We are then given a new example, x<sub>test</sub>, and we want to know whether this new example is abnormal/anomalous.</p>
<a id="more"></a>
<p>We define a &#x201C;model&#x201D; p(x) that tells us the probability the example is not anomalous. We also use a threshold &#x3F5; (epsilon) as a dividing line so we can say which examples are anomalous and which are not.</p>
<p>A very common application of anomaly detection is detecting fraud:</p>
<ul>
<li>x<sup>(i)</sup> = features of user i&#x2019;s activities</li>
<li>Model p(x) from the data.</li>
<li>Identify unusual users by checking which have p(x)&lt;&#x3F5;.</li>
</ul>
<p>If our anomaly detector is flagging <strong>too many</strong> anomalous examples, then we need to <strong>decrease</strong> our threshold &#x3F5;</p>
<h2 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h2><p>The Gaussian Distribution is a familiar bell-shaped curve that can be described by a function N(&#x3BC;,&#x3C3;<sup>2</sup>)</p>
<p>Let x&#x2208;&#x211D;. If the probability distribution of x is Gaussian with mean &#x3BC;, variance &#x3C3;<sup>2</sup> , then:</p>
<p>x ~ N(&#x3BC;,&#x3C3;<sup>2</sup>)</p>
<p>The little &#x223C; or &#x2018;tilde&#x2019; can be read as &#x201C;distributed as.&#x201D;</p>
<p>The Gaussian Distribution is parameterized by a mean and a variance.</p>
<p>Mu, or &#x3BC;, describes the center of the curve, called the mean. The width of the curve is described by sigma, or &#x3C3;, called the standard deviation.</p>
<p>The full function is as follows:</p>
<p><img src="/2016/12/31/Machine-Learning-9/1.png" alt=""></p>
<p>We can estimate the parameter &#x3BC; from a given dataset by simply taking the average of all the examples:</p>
<p><img src="/2016/12/31/Machine-Learning-9/2.png" alt=""></p>
<p>We can estimate the other parameter, &#x3C3;<sup>2</sup> , with our familiar squared error formula:</p>
<p><img src="/2016/12/31/Machine-Learning-9/3.png" alt=""></p>
<h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Given a training set of examples, {x<sup>(1)</sup>,x<sup>(2)</sup>,&#x2026;,x<sup>(m)</sup>} where each example is a vector, x &#x2208; &#x211D;<sup>n</sup></p>
<p><img src="/2016/12/31/Machine-Learning-9/4.png" alt=""></p>
<p>In statistics, this is called an &#x201C;independence assumption&#x201D; on the values of the features inside training example x.</p>
<p>More compactly, the above expression can be written as follows:</p>
<p><img src="/2016/12/31/Machine-Learning-9/5.png" alt=""></p>
<p><strong>The algorithm</strong></p>
<p>Choose features x<sub>i</sub> that you think might be indicative of anomalous examples.</p>
<p>Fit parameters &#x3BC;<sub>1</sub>,&#x2026;,&#x3BC;<sub>n</sub>,&#x3C3;<sup>2</sup><sub>1</sub>,&#x2026;,&#x3C3;<sup>2</sup><sub>n</sub></p>
<p>Calculate</p>
<p><img src="/2016/12/31/Machine-Learning-9/6.png" alt=""></p>
<p>Given a new example x, compute p(x):</p>
<p><img src="/2016/12/31/Machine-Learning-9/7.png" alt=""></p>
<p>Anomaly if p(x)&lt;&#x3F5;</p>
<p>A vectorized version of the calculation for &#x3BC;</p>
<p><img src="/2016/12/31/Machine-Learning-9/8.png" alt=""></p>
<p>You can vectorize &#x3C3;<sup>2</sup> similarly.</p>
<h2 id="Developing-and-Evaluating-an-Anomaly-Detection-System"><a href="#Developing-and-Evaluating-an-Anomaly-Detection-System" class="headerlink" title="Developing and Evaluating an Anomaly Detection System"></a>Developing and Evaluating an Anomaly Detection System</h2><p>To evaluate our learning algorithm, we take some labeled data, categorized into anomalous and non-anomalous examples ( y = 0 if normal, y = 1 if anomalous).</p>
<p>Among that data, take a large proportion of <strong>good</strong>, non-anomalous data for the training set on which to train p(x).</p>
<p>Then, take a smaller proportion of mixed anomalous and non-anomalous examples (you will usually have many more non-anomalous examples) for your cross-validation and test sets.</p>
<p>For example, we may have a set where 0.2% of the data is anomalous. We take 60% of those examples, all of which are good (y=0) for the training set. We then take 20% of the examples for the cross-validation set (with 0.1% of the anomalous examples) and another 20% from the test set (with another 0.1% of the anomalous).</p>
<p>In other words, we split the data 60/20/20 training/CV/test and then split the anomalous examples 50/50 between the CV and test sets.</p>
<p><strong>Algorithm evaluation:</strong></p>
<p>Fit model p(x) on training set {x<sup>(1)</sup>,&#x2026;,x<sup>(m)</sup>}</p>
<p>On a cross validation/test example x, predict:</p>
<p>If p(x) &lt; &#x3F5; (<strong>anomaly</strong>), then y=1</p>
<p>If p(x) &#x2265; &#x3F5; (<strong>normal</strong>), then y=0</p>
<p>Possible evaluation metrics (see &#x201C;Machine Learning System Design&#x201D; section):</p>
<ul>
<li>True positive, false positive, false negative, true negative.</li>
<li>Precision/recall</li>
<li>F<sub>1</sub> score</li>
</ul>
<p>Note that we use the cross-validation set to choose parameter &#x3F5;</p>
<h2 id="Anomaly-Detection-vs-Supervised-Learning"><a href="#Anomaly-Detection-vs-Supervised-Learning" class="headerlink" title="Anomaly Detection vs. Supervised Learning"></a>Anomaly Detection vs. Supervised Learning</h2><p>When do we use anomaly detection and when do we use supervised learning?</p>
<p>Use anomaly detection when&#x2026;</p>
<ul>
<li>We have a very small number of positive examples (y=1 &#x2026; 0-20 examples is common) and a large number of negative (y=0) examples.</li>
<li>&#x2022;    We have many different &#x201C;types&#x201D; of anomalies and it is hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we&#x2019;ve seen so far.</li>
</ul>
<p>Use supervised learning when&#x2026;</p>
<ul>
<li>We have a large number of both positive and negative examples. In other words, the training set is more evenly divided into classes.</li>
<li>We have enough positive examples for the algorithm to get a sense of what new positives examples look like. The future positive examples are likely similar to the ones in the training set.</li>
</ul>
<h2 id="Choosing-What-Features-to-Use"><a href="#Choosing-What-Features-to-Use" class="headerlink" title="Choosing What Features to Use"></a>Choosing What Features to Use</h2><p>The features will greatly affect how well your anomaly detection algorithm works.</p>
<p>We can check that our features are <strong>gaussian</strong> by plotting a histogram of our data and checking for the bell-shaped curve.</p>
<p>Some <strong>transforms</strong> we can try on an example feature x that does not have the bell-shaped curve are:</p>
<ul>
<li>log(x)</li>
<li>log(x+1)</li>
<li>log(x+c) for some constant</li>
<li>&#x221A;x</li>
<li>x<sup>1/3</sup></li>
</ul>
<p>We can play with each of these to try and achieve the gaussian shape in our data.</p>
<p>There is <strong>an error analysis procedure</strong> for anomaly detection that is very similar to the one in supervised learning.</p>
<p>Our goal is for p(x) to be large for normal examples and small for anomalous examples.</p>
<p>One common problem is when p(x) is similar for both types of examples. In this case, you need to examine the anomalous examples that are giving high probability in detail and try to figure out new features that will better distinguish the data.</p>
<p>In general, choose features that might take on unusually large or small values in the event of an anomaly.</p>
<h2 id="Multivariate-Gaussian-Distribution-Optional"><a href="#Multivariate-Gaussian-Distribution-Optional" class="headerlink" title="Multivariate Gaussian Distribution (Optional)"></a>Multivariate Gaussian Distribution (Optional)</h2><p>The multivariate gaussian distribution is an extension of anomaly detection and may (or may not) catch more anomalies.</p>
<p>Instead of modeling p(x<sub>1</sub>),p(x<sub>2</sub>)&#x2026;separately, we will model p(x) all in one go. Our parameters will be: &#x3BC; &#x2208; &#x211D;<sup>n</sup> and<br>&#x3A3; &#x2208; &#x211D;<sup>n*n</sup>.</p>
<p><img src="/2016/12/31/Machine-Learning-9/9.png" alt=""></p>
<p>The important effect is that we can model oblong gaussian contours, allowing us to better fit data that might not fit into the normal circular contours.</p>
<p>Varying &#x3A3; changes the shape, width, and orientation of the contours. Changing &#x3BC; will move the center of the distribution.</p>
<h2 id="Anomaly-Detection-using-the-Multivariate-Gaussian-Distribution-Optional"><a href="#Anomaly-Detection-using-the-Multivariate-Gaussian-Distribution-Optional" class="headerlink" title="Anomaly Detection using the Multivariate Gaussian Distribution (Optional)"></a>Anomaly Detection using the Multivariate Gaussian Distribution (Optional)</h2><p>When doing anomaly detection with multivariate gaussian distribution, we compute &#x3BC; and &#x3A3; normally. </p>
<p>We then compute p(x) using the new formula in the previous section and flag an anomaly if p(x) &lt; &#x3F5;.<br>The original model for p(x) corresponds to a multivariate Gaussian where the contours of p(x;&#x3BC;,&#x3A3;) are axis-aligned.</p>
<p>The multivariate Gaussian model can automatically capture correlations between different features of x.</p>
<p>However, the original model maintains some advantages: it is computationally cheaper (no matrix to invert, which is costly for large number of features) and it performs well even with small training set size (in multivariate Gaussian model, it should be greater than the number of features for &#x3A3; to be invertible).</p>
<h2 id="Recommender-Systems"><a href="#Recommender-Systems" class="headerlink" title="Recommender Systems"></a>Recommender Systems</h2><h2 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h2><p>Recommendation is currently a very popular application of machine learning.</p>
<p>Say we are trying to recommend movies to customers. We can use the following definitions</p>
<ul>
<li>n<sub>u</sub> = number of users</li>
<li>n<sub>m</sub> = number of movies</li>
<li>r(i, j) = 1 if user j has rated movie i</li>
<li>y(i, j) = rating given by user j to movie i (defined only if r(i,j)=1)</li>
</ul>
<h2 id="Content-Based-Recommendations"><a href="#Content-Based-Recommendations" class="headerlink" title="Content Based Recommendations"></a>Content Based Recommendations</h2><p>We can introduce two features, x<sub>1</sub> and x<sub>2</sub> which represents how much romance or how much action a movie may have (on a scale of 0&#x2212;1).</p>
<p>One approach is that we could do linear regression for every single user. For each user j, learn a parameter &#x398;<sup>(j)</sup> &#x2208; &#x211D;<sup>3</sup> . Predict user j as rating movie i with (&#x398;<sup>(j)</sup>)<sup>T</sup>x<sup>(i)</sup> stars.</p>
<ul>
<li>&#x398;<sup>(j)</sup> = parameter vector for user j</li>
<li>x<sup>(i)</sup> = feature vector for movie i</li>
</ul>
<p>For user j, movie i, predicted rating: (&#x398;<sup>(j)</sup>)<sup>T</sup>(x<sup>(i)</sup>)</p>
<ul>
<li>m<sup>(j)</sup> = number of movies rated by user j</li>
</ul>
<p>To learn &#x3B8;<sup>(j)</sup> ,we do the following</p>
<p><img src="/2016/12/31/Machine-Learning-9/10.png" alt=""></p>
<p>This is our familiar linear regression. The base of the first summation is choosing all i such that r(i,j)=1.</p>
<p>To get the parameters for all our users, we do the following:</p>
<p><img src="/2016/12/31/Machine-Learning-9/11.png" alt=""></p>
<p>We can apply our linear regression gradient descent update using the above cost function.</p>
<p>The only real difference is that we <strong>eliminate the constant</strong> 1/m.</p>
<h2 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h2><p>It can be very difficult to find features such as &#x201C;amount of romance&#x201D; or &#x201C;amount of action&#x201D; in a movie. To figure this out, we can use feature finders.</p>
<p>We can let the users tell us how much they like the different genres, providing their parameter vector immediately for us.</p>
<p>To infer the features from given parameters, we use the squared error function with regularization over all the users:</p>
<p><img src="/2016/12/31/Machine-Learning-9/12.png" alt=""></p>
<p>You can also <strong>randomly guess</strong> the values for theta to guess the features repeatedly. You will actually converge to a good set of features.</p>
<h2 id="Collaborative-Filtering-Algorithm"><a href="#Collaborative-Filtering-Algorithm" class="headerlink" title="Collaborative Filtering Algorithm"></a>Collaborative Filtering Algorithm</h2><p>To speed things up, we can simultaneously minimize our features and our parameters:</p>
<p><img src="/2016/12/31/Machine-Learning-9/13.png" alt=""></p>
<p>It looks very complicated, but we&#x2019;ve only combined the cost function for theta and the cost function for x.</p>
<p>Because the algorithm can learn them itself, the bias units where x<sub>0</sub>=1 have been removed, therefore x &#x2208; &#x211D;<sup>n</sup> and &#x3B8; &#x2208; &#x211D;<sup>n</sup>.</p>
<p>These are the steps in the algorithm:</p>
<p><img src="/2016/12/31/Machine-Learning-9/14.png" alt=""></p>
<h2 id="Vectorization-Low-Rank-Matrix-Factorization"><a href="#Vectorization-Low-Rank-Matrix-Factorization" class="headerlink" title="Vectorization: Low Rank Matrix Factorization"></a>Vectorization: Low Rank Matrix Factorization</h2><p>Given matrices X (each row containing features of a particular movie) and &#x398; (each row containing the weights for those features for a given user), then the full matrix Y of all predicted ratings of all movies by all users is given simply by: Y = X&#x3B8;<sup>T</sup>.</p>
<p>Predicting how similar two movies i and j are can be done using the distance between their respective feature vectors x. Specifically, we are looking for a small value of ||x<sup>(i)</sup> - x<sup>(j)</sup>||</p>
<h2 id="Implementation-Detail-Mean-Normalization"><a href="#Implementation-Detail-Mean-Normalization" class="headerlink" title="Implementation Detail: Mean Normalization"></a>Implementation Detail: Mean Normalization</h2><p>If the ranking system for movies is used from the previous lectures, then new users (who have watched no movies), will be assigned new movies incorrectly. Specifically, they will be assigned &#x3B8; with all components equal to zero due to the minimization of the regularization term. That is, we assume that the new user will rank all movies 0, which does not seem intuitively correct.</p>
<p>We rectify this problem by normalizing the data relative to the mean. First, we use a matrix Y to store the data from previous ratings, where the ith row of Y is the ratings for the ith movie and the jth column corresponds to the ratings for the jth user.</p>
<p>We can now define a vector</p>
<p>&#x3BC; = [&#x3BC;<sub>1</sub>,&#x3BC;<sub>2</sub>,&#x2026;,&#x3BC;<sub>n<sub>m</sub></sub>]</p>
<p>such that</p>
<p><img src="/2016/12/31/Machine-Learning-9/15.png" alt=""></p>
<p>Which is effectively the mean of the previous ratings for the ith movie (where only movies that have been watched by users are counted). We now can normalize the data by subtracting u, the mean rating, from the actual ratings for each user (column in matrix Y):</p>
<p>As an example, consider the following matrix Y and mean ratings &#x3BC;:</p>
<p><img src="/2016/12/31/Machine-Learning-9/16.png" alt=""></p>
<p>The resulting Y&#x2032; vector is:</p>
<p><img src="/2016/12/31/Machine-Learning-9/17.png" alt=""></p>
<p>Now we must slightly modify the linear regression prediction to include the mean normalization term:</p>
<p>(&#x398;<sup>(j)</sup>)<sup>T</sup>x<sup>(i)</sup> + &#x3BC;<sub>i</sub></p>
<p>Now, for a new user, the initial predicted values will be equal to the &#x3BC; term instead of simply being initialized to zero, which is more accurate.</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2016/12/31/Machine-Learning-9/">
    <time datetime="2016-12-31T04:54:43.000Z" class="entry-date">
        2016-12-31
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/ML/">ML</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2016/12/31/2016年终总结/" rel="prev"><span class="meta-nav">←</span> Legend 2016</a></span>
    
    
        <span class="nav-next"><a href="/2016/12/31/Machine-Learning-8/" rel="next">Machine-Learning-8 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/LIFE/">LIFE</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/MAC/">MAC</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML/">ML</a><span class="category-list-count">27</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/PGM/">PGM</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/iOS/">iOS</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/御宅文化/">御宅文化</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">35</span></li></ul>
    </div>
  </aside>

  
    
<div class="widget tag">
<h3 class="title">blogroll</h3>
<ul class="entry">


<li><a href="https://github.com/" target="_blank">我的github</a></li>


<li><a href="http://www.jianshu.com/users/41cd7711ed44/latest_articles" target="_blank">我的简书主页</a></li>


<li><a href="http://uuzdaisuki.com" target="_blank">leticia’s blog</a></li>


<li><a href="http://www.helloyzy.cn" target="_blank">acery</a></li>


<li><a href="http://wxjackie.com" target="_blank">WXJACKIE</a></li>


<li><a href="http://www.stephenzhang.me" target="_blank">stephenzhang</a></li>


<li><a href="blog.keybrl.com" target="_blank">keybrl</a></li>


<li><a href="http://blog.ciaran.cn" target="_blank">Ciaran</a></li>


<li><a href="http://1.dev.blog.qinka.pro" target="_blank">Qinka</a></li>


<li><a href="http://tobiasLee.top" target="_blank">TobiasLee</a></li>


<li><a href="http://blog.boileryao.com" target="_blank">bingo</a></li>

</ul>
</div>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2018/03/23/EMTC-md/">Extreme Multi-label Text Classification:Kim-CNN &amp; XML-CNN</a>
          </li>
        
          <li>
            <a href="/2017/12/17/FastRCNN-FasterRCNN/">谈一谈 Fast R-CNN 和 Faster R-CNN</a>
          </li>
        
          <li>
            <a href="/2017/11/17/如何在Xcode中添加-bits-stdc-h-头文件/">如何在Xcode中添加&lt;bits/stdc++.h&gt;头文件</a>
          </li>
        
          <li>
            <a href="/2017/11/02/LSTM/">RNN 与 LSTM</a>
          </li>
        
          <li>
            <a href="/2017/11/01/循环神经网络-RNN/">循环神经网络_RNN</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cloud-Computing/">Cloud Computing</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cocoapods/">Cocoapods</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matlab/">Matlab</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Networks/">Neural Networks</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Graphical-Models/">Probabilistic Graphical Models</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIColor/">UIColor</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIView/">UIView</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIView-圆角/">UIView-圆角</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vim/">Vim</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bug/">bug</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/openCV/">openCV</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/二维码/">二维码</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/指针/">指针</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/操作系统/">操作系统</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据结构/">数据结构</a><span class="tag-list-count">19</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a><span class="tag-list-count">8</span></li></ul>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2018 SaberDa
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
</footer>
    <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

<script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
</body>
</html>