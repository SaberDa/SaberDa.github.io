<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>SaberDa的幻想乡</title>
  <icon>https://www.gravatar.com/avatar/0a06c41b158eb3c87b3d334e94cd39a4</icon>
  <subtitle>　　iOS/ ML　　|　　二次元　　|　　saberda@qq.com　　|　　明明是己不由心 怎能说身不由己</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://saberda.github.io/"/>
  <updated>2019-03-09T13:45:57.793Z</updated>
  <id>https://saberda.github.io/</id>
  
  <author>
    <name>SaberDa</name>
    <email>630991493@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>入职半月，初窥门径</title>
    <link href="https://saberda.github.io/2019/03/09/%E5%85%A5%E8%81%8C%E5%8D%8A%E6%9C%88%EF%BC%8C%E5%88%9D%E7%AA%A5%E9%97%A8%E5%BE%84/"/>
    <id>https://saberda.github.io/2019/03/09/入职半月，初窥门径/</id>
    <published>2019-03-09T08:56:47.000Z</published>
    <updated>2019-03-09T13:45:57.793Z</updated>
    
    <content type="html"><![CDATA[<pre><code>如题，这篇文章写于入职后两周零两天，地点深圳图书馆。本来是想昨天写的，昨天与我对接的后端下午过节去了，原本应该是个清闲的下午，然而刚刚整理完开发文档和填坑笔记后，就来了新需求，所以挪到今天写了。</code></pre><p>本人以实习生身份进入一家公司，<del>利益相关，匿了</del>，工作两周有余，感慨颇深，遂写下此文。</p><p>我准备从工作、生活、学习三个方面总结体会。</p><h2 id="实习工作，初窥门径"><a href="#实习工作，初窥门径" class="headerlink" title="实习工作，初窥门径"></a>实习工作，初窥门径</h2><a id="more"></a><p>如果原计划顺利的话，我应该是不会进入我目前这家公司的，但是生活就是充满了变化与不确定。年前头条挂在了算法上，平安科技通知我他们实习生不要211，<del>cao</del>，然后腾讯的内推还一直没有消息，等过完了十五终于受不了了，索性来到了这里。</p><h2 id="公司环境"><a href="#公司环境" class="headerlink" title="公司环境"></a>公司环境</h2><p>我司是一家国家持股的混合型公司，条件很nice，工位很大<del>跟我租的房间差不多</del>，单位提供早中晚饭，而且每月给工牌里打饭钱，基本属于免费。工作之后，我甚至做到了在学校时都无法保证的“每天都去吃早饭”。</p><p>因为是合租，而且通勤时间长，索性每天就起的很早（不用和其他人抢厕所），然后慢慢悠悠的坐地铁（有时是公交，具体看心情），溜达到公司。到公司的时候基本没几个人，慢悠悠的吃完早饭，冲杯美式，开始工作。</p><p>每天下午还有下午茶，就是一些糕点什么的，我运气很好，下午茶的摆放地点就在我身后不到两米的地方，脚一蹬地，椅子过去，伸手一拿，再蹬回来，署实方便。</p><p>公司坐落在深圳湾生态科技园，写字楼三楼是一个超级大的平台，把这片的所有写字楼都链接了起来，绿化什么的也做得很好。公司二楼还有一个露天庭院，有时晚上吃完饭就去那里坐着吹吹风。但是现在发现了离我工位不远的地方就有一个露天阳台，这几天都是去那里吹风。</p><p>公司二楼还有一个水吧，之前去了一次看了眼价格就再也没去过。</p><h2 id="公司文化"><a href="#公司文化" class="headerlink" title="公司文化"></a>公司文化</h2><p>整个公司采取的是全透明办公性质，组长和院长甚至董事长的办公地点都是能直接用肉眼看到的。公司文化也不错，第一天组长就反复跟我说：“这里很开放的，没必要那么拘谨。”</p><p>虽然才入职两周，说完全了解公司的文化是不可能的，但是这里的加班文化署实让人震惊。原则上是晚上六点就下班了，然而我几乎没有一天离开公司的时间早于8点，最晚的一天出公司门是十点半。还行，他们挺照顾我的，没让我周日来加班，哭了。</p><p>我司每年都会举办技术分享大会，上到董事长下到基层，上去做技术分享会，历时三天，我很幸运刚入职就赶上了这个会议。但是因为我司不是纯正的互联网公司，很多分享的知识领域对我来说是盲区，就第一天听了下董事长的分享和我所属部门的头头的分享后就再去没去了。</p><h2 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h2><blockquote><p>实习生不是活少，是工资少</p></blockquote><p>上面那句话是昨天秦神说的，我当时犹如醍醐灌顶，把心里稍许的不平衡打消了。</p><p>入职前，家里人都告诉我，实习生没什么活的，也就是端端茶倒倒水，基本不会充当生产力的。然后入职当天上午，就直接参与了某个外包公司与我们单位的一个项目的对接，下午由于原本的头去开会了，前端这一部分就我独自参与的对接。</p><p>那时，我还天真的认为，我也就是给别人打打下手，应该不会让我直接接手的。显然，我没能理解组长见到我时说的那句话“诶，你会前端啊，正好我们缺人”。现在回想起来，甚至感觉当时组长的眼睛都在冒光。</p><p>是的，我作为组里唯一会些前端的人，承担起了项目对接后的前端负责人，说是负责人实际就我一个人。我前段就一个学校大作业的半吊子水平，没办法，只能硬着头皮上了。</p><p><img src="/2019/03/09/入职半月，初窥门径/1.JPG" alt=""></p><p>俗话说得好，<strong>一切能在互联网上找到答案的问题都不是问题</strong>，还有一句话是，<strong>除非你在网上找了很久都没有找到答案，再去问别人</strong>。望各位互联网的同僚深刻记住这句话。</p><p>说是前端，实际上是微信小程序，我之前只是知道微信小程序主要通过js开发，以为跟vue之类的框架类似，但当我打开官方文档的那一刻起，就发现我错了。</p><p>微信小程序说是用js开发，但是本质上与iOS和安卓开发没有多大区别，而且和传统的前端开发也有很大区别，虽然看着差不多。许多操作需要调用微信写的接口。简单说，就是用传统的前端方式来写iOS app。</p><p>这部分我之前是打算开个新坑更新博客的，但是后期发现，官方文档虽然说得不是很详细，但是网上这方面的博文已经很多甚至很成熟，自己重新写这些意义不大，索性就直接在本子上记录下一些奇奇怪怪的坑，没有整理成文章。</p><p>也不能说运气不好吧，刚入职就赶上了一个项目的落地，这个落地项目还是一个超级大的项目的先前部队，所以公司上层很重视这个落地的进展情况。而且这个部门里就我一个稍微有些前端基础，很自然的所有前端变动都落到我的头上。</p><p>不能说需求总是变动吧，单基本每天都有两三个新需求，有些需求简单，有些复杂的需要改动多个页面的逻辑。而且我也是刚开始学习小程序，公司里也没有做过小程序的前辈，有些不懂的我只能问同学，<strong>在这里再一次感谢提供帮助的游总和程总，等回学校请你两吃饭</strong>。</p><p>剩下的关于小程序这部分我放到后面去说。</p><p>职场新人，很多东西我都不了解，不会沟通不会拒绝，导致了入职第一周的周五身体出现了状况。当时是晚上八点左右，刚跟后端改完一个离奇的bug，产品（组长）过来跟我说今晚上线，上线前再把这个功能写出来。当时顿时感觉压力山大，再加上晚上没有吃饭，最后诊断是低血糖加上心理变化，心脏出现了应激性反应，反正当晚是直接去医院了，在医院待到11点多才离开。</p><p>这也是我需要学习的一个地方吧，调整心理并且敢于沟通善于沟通。</p><p>到了第二周，情况就好多了，虽然每天的新需求不断，但随着对小程序的不断理解，并加上和产品积极沟通，每天虽然很累但是感觉很充实。</p><p>而且我司不是纯正的互联网公司，就连我所属的部门都不是纯正的互联网部门，面对一个即将上线的产品，我站在外人的角度发现各个方面都有缺陷。</p><ul><li>首先是一个产品从开发到发布的环节链的缺失。</li></ul><p>我接手的小程序之前是由外包公司写的，我入职时正好是交付阶段，之后所有的新需求都交给我这个新人来写，而且前端部门就我一个，从某种角度来说我的官挺大的。然后是测试，缺少完整的测试环境与流程。我的工作在某一方面还包含部署，但是令我惊讶的是，竟然没有测试环境，我部署的话是直接部署到生产环境，当时我就想，这要是某个功能bug没有测试出来，出事了，那我就要被迫离职了啊。</p><ul><li>其次是需求的强制性下达。</li></ul><p>正常产品和开发中，至少是有一个人等级是对等的，这样讨论需求上能从开发的角度谈一谈实现该需求的可行性。但是我司是上层给下层下需求，有些需求难以实现，甚至需要将整个产品重构，这种事情放在互联网公司里可是要郑重决策的。</p><ul><li>缺少完整的开发团队与运营团队。</li></ul><p>按照工作性质来讲，我们部门算是算法研发部门，但是自从这个项目上线以来，他们除了承担算法，还包括后端，测试和运营。我现在接手的项目只是个大项目的前置落地项目，在其后面还有一个超级复杂的应用需要落地，虽说这个落地已经在我离职后了，但是到时若没有一个完整的开发团队，其中包含iOS和安卓的移动端、后端、测试、运营，落地的难度会及其的大，单单只靠外包公司是不行的。</p><ul><li>各个版本的需求说明不清楚。</li></ul><p>就拿我的任务来讲，缺少明确的版本说明，不知道下个版本要上线哪些功能，只知道这些需求的优先级，这让我写的时候就很迷茫。有些需求有很高的优先级，但是实现起来需要一周左右的时间，其次版本上线时间不明确，搞得我不知道是先写短时间内就能完成的，还是其他什么。</p><h2 id="为人处世，初窥门径"><a href="#为人处世，初窥门径" class="headerlink" title="为人处世，初窥门径"></a>为人处世，初窥门径</h2><p>以这个实习为界，在此之前所有的生活都是在象牙塔里，并没有那么多的规矩，随心所欲。但我发现，一旦到了职场上，你会不自觉的开始思考，我应该怎样称呼对方，用什么样子的方式去向对方表达自己的见解。</p><p>组里的同事都叫组长某博，我刚开始不知道啊，就知道个名字，索性就从组长变为了旭哥，正好我叫我高中班主任也这么叫，挺顺嘴的，就这么叫下去了。然后组里另一个产品，刚开始我直接张口“钱姐”，当我把这条消息发过去时就意识到了把对方叫老了，之后改口了，但是还是很尴尬的。</p><p>这类的事情到第二周还时有发生，之前的前端负责人组长叫他华强，但是对方比我大很多，我不能这么叫吧，索性叫“朱哥”，没办法，听上去怪怪的。</p><p>然后想说的就是之前说的沟通问题，特别是有关工作的沟通问题。</p><p>现在我摸索到了一些敲门，当上面让你完成一份文件或者ppt时，立马跟上去问截止时间和具体内容，以及其他一些需要注意的事情。着手工作后，遇到的或者间接遇到的问题，或者觉得之前没有问清楚的地方，或者你不明白的地方，一定要及早及时的询问，等一切都问明白了再动手。不要等到最后提交时才发现存在问题。提交之后马上问一下，或者估摸个时间对方看完了再问一下，所完成的工作是否存在什么问题。</p><blockquote><p>事前、事中、事后的及时沟通很重要。主动沟通是提高效率的关键</p></blockquote><p>还有关于开发方面上的沟通。我现在接手的前端，虽说按之前的那种方式开发没问题，但是在上周实现的某一个需求上，我深刻发现了与后端充分沟通的必要性。那个需求是更改支付逻辑，并且还要判断付款人的性质，根据不同性质执行不同的支付操作。</p><p>拿到需求后，我跟后端讨论了大约能有40分钟，把从传值到最后实现的各个细节都讨论清楚了。本来这是个大需求，而且涉及到支付后的一系列操作，但是充分沟通后感觉虽然复杂但是写起来特别简单，最后的实现也是基本做到了bug free。</p><p>其次就是和产品的沟通。虽然产品是我的组长，是上级，但是该沟通的还是要沟通，需求觉得不合理就要提出来，虽然每次跟组长讨论需求时都怕怕的，但还是要硬着头皮上。只有把需求讨论清楚了，才能写的舒服。而且一定要和产品亲自讨论需求，不要从后端或者其他人那里得到需求就去写。</p><p>举个例子，我接到的有个需求是让客户录入他的公司和部门名称。这个我刚开始是从后端那里得知的，我就很简答的用了input组件，没想到写完后给组长交付，他跟我说要做picker（下拉式菜单），所以大半个下午的工作需要重做。从那之后，所有的需求细节我都是直接去找产品询问。</p><p>生活是最好的老师，有些事情只有你踩过坑之后，才有所收获。技术亦如此。</p><h2 id="微信小程序，初窥门径"><a href="#微信小程序，初窥门径" class="headerlink" title="微信小程序，初窥门径"></a>微信小程序，初窥门径</h2><blockquote><p>任何技术想要速成，都是存在代价和隐患的。</p></blockquote><p>在入职第一天，组长问我，“能不能试试微信小程序”，当我回答“可以试试”的那一刻起，我就走上了一条不归路。</p><p>还好的是，微信小程序是腾讯弄的，官方文档是中文，而且国内相关的论坛已经趋向成熟，这种种便利条件给予了我很大帮助。</p><p>还是那句话</p><blockquote><p>一切能在互联网上找到答案的问题都不是问题</p></blockquote><p>入职后不久，我就开始了面向文档和谷歌编程，开始接手小程序的更改。</p><p><del>虽然这么做不地道，但是没办法</del></p><p>期间也是踩了很多坑。因为我们这个项目并不是原生小程序，是H5与小程序的结合，听说是最开始的技术选型是完全的H5，但是后面出于各种复杂的原因，变成了现在这个样子。</p><p>因为涉及公司秘密，下面有些关于项目的描述我只是略过。</p><p>项目里有很多关于H5和小程序的跳转，有些还是要先跳到小程序中拿到一些值回传给H5，这就导致有些H5页面因为要等小程序传值，渲染的过程有几秒的白屏，这个是硬伤，网上我也没找到什么好的解决办法，较好的办法就是重构。</p><p>说道重构，我其实是有这个意图的，但是没有跟组长说，我怕我说出来就不是个意图这么简单了。<u>同学，收起你那大胆的想法。</u></p><p>但是重构我还是想试试的，每个程序员心里都会说，“这写得啥玩意，我来重写一个”。我打算用连续几个周末的时间和离职后的时间，一步一步把H5的功能移植到小程序上，当然到目前为止这还只是个想法。未来几周的空闲时间，我打算先把整个架构构思好，然后再看看实现的复杂度。</p><p>因为我并不是个纯正的前端开发，很多问题都是第一次遇到，有些方法也不知道怎么调用。而且搜索引擎的功能也是有限的，有些问题只能找人来解决。<u><strong>这里没啥说的，再次感谢程总和游总吧。</strong></u></p><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>这个标题起的像是给领导展示PPT，23333。</p><p>现在我在生活与工作中还是找不到平衡，有时候加班到家已经快12点了（没办法，远，路上包含走路估计有70分钟，至于为什么这么远，因为房租便宜）。然后还存在一些其他的私人问题。</p><p>虽然入职这两周很累，是之前从未体验过的船新版本<del>（滑稽）</del>，但是收获很多，我在小程序这方面，还有前端，学到了相当多的新知识。这方面的知识我已经整理到自己的小本子上了，至于整不整理成博客，看日后心情吧。</p><p>我司虽然工作很累，但是总体上工作还是很开心的，认识了很多大佬，接触了很多新的知识领域，身边也有很多人都是有海外留学背景的，说不定将来我在国外遇到什么问题，还要向他们需求帮助。</p><p>而且我本意是想找家公司写C++的，因为正如在上篇文章中说的那样，因为将来绝对上研，有充足的时间了解学习各个方向。</p><p>你说我一个主写iOS、搞过深度学习的，怎么就去写前端了呢？<del>当时我就吟了一首诗</del> 虽这么说，但是我觉得我干完这份实习，我的前端水平将来应该也能去中等公司应聘了。</p><p>说到这，我对于我将来干什么方向什么岗位还是迷茫的，组长在平时闲聊之余也问过这个问题，也建议再考虑考虑算法岗，甚至是读博的问题。关于读博，身边有好多大佬保研都是直博，去日本的舍友也说将来准备赴美读博，家里的话也是支持我上博的，关于这个问题看将来吧，现在没法定夺。</p><p>就这样吧，写到这里，外面正下着大雨，深圳这一周都是雨天，虽说我喜欢听雨声，但是不喜欢在大雨里回家。很久没有写这方面的文章了，自己都感觉构思和措辞上大不如前，看来以后也要把写作锻炼加到日常表里。</p>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;如题，这篇文章写于入职后两周零两天，地点深圳图书馆。
本来是想昨天写的，昨天与我对接的后端下午过节去了，原本应该是个清闲的下午，然而刚刚整理完开发文档和填坑笔记后，就来了新需求，所以挪到今天写了。
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;本人以实习生身份进入一家公司，&lt;del&gt;利益相关，匿了&lt;/del&gt;，工作两周有余，感慨颇深，遂写下此文。&lt;/p&gt;
&lt;p&gt;我准备从工作、生活、学习三个方面总结体会。&lt;/p&gt;
&lt;h2 id=&quot;实习工作，初窥门径&quot;&gt;&lt;a href=&quot;#实习工作，初窥门径&quot; class=&quot;headerlink&quot; title=&quot;实习工作，初窥门径&quot;&gt;&lt;/a&gt;实习工作，初窥门径&lt;/h2&gt;
    
    </summary>
    
      <category term="LIFE" scheme="https://saberda.github.io/categories/LIFE/"/>
    
    
  </entry>
  
  <entry>
    <title>我的2018，记忆名为伽勒底</title>
    <link href="https://saberda.github.io/2018/12/31/%E6%88%91%E7%9A%842018%EF%BC%8C%E8%AE%B0%E5%BF%86%E5%90%8D%E4%B8%BA%E4%BC%BD%E5%8B%92%E5%BA%95/"/>
    <id>https://saberda.github.io/2018/12/31/我的2018，记忆名为伽勒底/</id>
    <published>2018-12-30T19:14:31.000Z</published>
    <updated>2018-12-30T19:37:43.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>今年发生了太多值得写下来的事情，今年的感想也特别多。今年的种种最终交汇成这篇文章，以文字和图片的形式记录下来。尽管文笔有限，但文字会如咒语般，唤醒背后存在的情感与记忆</code></pre><p>每年年末都要抽出一两天的时间来回忆今年我干了什么，还记得去年的总结写完后已经过完年了，就没有发。好长时间没有写这种文章了，文笔肯定略显平庸，反正是总结，凑活着写吧。</p><h2 id="时间线"><a href="#时间线" class="headerlink" title="时间线"></a>时间线</h2><p>先捋一遍时间线吧。</p><h2 id="一二月份"><a href="#一二月份" class="headerlink" title="一二月份"></a>一二月份</h2><p>今年的一、二月份，对于大多数 fate go 国服玩家来说都是一段不可明灭的记忆，从第七章的乌鲁克的救赎到终章的众志成城，蘑菇本人亲手执笔创造出来的剧本诚不欺我。</p><p>下面这部分是对这部分的一个回忆。</p><a id="more"></a><p>第七章重新对吉尔伽美什这个最古英雄王塑造了一次人物形象，摆脱了 fate 传统的二五仔形象，描写的是自恩奇都死后唤醒人生第二春的贤王。为了拯救即将毁灭的乌鲁克城区与群众并等待迦勒底主角等人的到来，付出巨大的代价召唤出从者现世。剧本还重现了吉尔伽美什过劳死这个传说结局，真是米索不达米亚平原的焦裕禄。当然，最感动的并不是作为第七章主角之一的吉尔伽美什，而是坚守到最后一刻的乌鲁克群众。明知道自己的结局逃不脱死亡，但还是以乐观的心态对待新的一天，主动放弃其他领土固守都城，直到最后一刻，即使面对神代强大的提亚马特，也坚信他们的王会带领乌鲁克走向最后的胜利。</p><p>“<strong>乌鲁克仍存于此</strong>”，当吉尔伽美什第二次复生，从冥界以 Archer 现世大喊出这句话时，我承认我当时泪目了。配合当时恢复记忆的金固（恩奇都），给提亚马特重创。以及付出了整个冠位魔力的王哈桑剥脱了提亚马特的不死神性，咕哒子才得以借助梅林之力将提亚马特重新打回封印之所。当然，我估计这也和蘑菇有意洗白提亚马特有关，即使最后重新被封印，她也念念不忘当初创造的那些子民。（总之很期待明年的动漫）</p><p><img src="/2018/12/31/我的2018，记忆名为伽勒底/1.jpg" alt=""></p><p>当然，恢弘壮烈的第七章只是为终章做的铺垫，FGO 的终章剧情可以说将这部手游封上神坛。</p><p>咕哒子一行人从乌鲁克刚刚回到迦勒底，就接到了与所罗门王所在的冠位时间神殿所在空间接触，来不及休息就直接走入宿命的碰撞。咕哒子和玛修进入后发现他们面对的是所罗门王麾下七十二魔神所化的无穷无尽的魔神柱，正当绝望之时，金色的圣光在咕哒子身旁绽放，贞德出现在战场后，举起高呼：</p><blockquote><p>“听着 在这个时空聚集起来的<br>一骑当千 万夫莫敌的英灵们啊<br>哪怕本为无法相容的敌人<br>哪怕本为没有交集的不同时代之人<br>现在也请互相把后背托付给对方吧<br>我们哪怕来自无法交融的时代<br>不是为了阻止人理烧却<br>我们并不是为了防止人类毁灭<br>而是为吾等的契约者开辟前行的道路<br>我的真名是贞德<br>在主的名义下<br>将成为汝等坚实之盾<br>吾主即在此地<br>集结于升旗之下怒吼吧”</p></blockquote><p>东风夜放花千树，更吹落，星如雨。英灵们随着颗颗流星现世，前七章与咕哒子一行人接下羁绊的英灵们纷纷在贞德后涌现，其中有敌人有挚友，但此时他们对咕哒子所说的都是一句话“往前走吧，背后交给我们”。在这之中，掺杂着无数复杂情感的诸多从者们，放弃了自身的立场，放弃了之间的恩怨，因与主角结下的一丝羁绊，主动被召唤到这片战场，为咕哒子阻拦住前方的障碍，让其直达神殿中间。</p><p>在神殿中间，面对着盖提亚的宝具，玛修虽知这不是自己能够承受的，“哪怕我的生命会在刹那之后终结，即便如此，我也想要见证未来，哪怕多一秒也好”，扔毅然决然的举起护盾保护在咕哒子前，即使肉体将无法承受光带的热量而蒸发，但那份精神、心灵不会遭任何侵扰。雪花之盾毫发无损，会一直保护她的主人。她不是什么勇敢的战士，也不是故事的主题。只是一个，极为普通的女孩子。</p><p>“迄今为止的旅程，从今往后的旅程。自己存在的过去，和自己不在了的，未来的梦”，在玛修最后回首对咕哒子说“我总是在被人保护，至少最后，希望能够帮上前辈一次忙”，我想保护的身影此刻牺牲了自己守护着我，“前辈，能再握一次我的手吗？”，我泪目了。</p><p><img src="/2018/12/31/我的2018，记忆名为伽勒底/2.jpg" alt=""></p><p>最后一刻，医生戴上了最后的那枚戒指，微微一笑，走进了灵子转移。面对这曾是自己的恶的盖提亚，释放了属于自己真正的宝具：</p><blockquote><p>诞生之时已至，以此修正万象 Ars Almadel Salomonis<br>加冕之时已至，以此启发万象 Ars Paunila<br>诀别之时已至，以此，舍弃世界 Ars Nova</p></blockquote><p>作为代价，医生燃烧了自己，剥夺了盖提亚的能力，咕哒子拿起玛修的盾上前手撕了盖提亚。医生作为所罗门成为人类的愿望体现，在最后一刻察觉到了人类的终焉，即人理的毁灭。这十年间用一个凡人的力量，去学习了各种知识，准备迎接未知的敌人。十年来以凡人之躯默默的承受着这一切，为的还是其所热爱的人类。正如最后所言：</p><blockquote><p>所罗门王虽拥有万能的戒指<br>却一次也没有使用过<br>以及 他最后凭借自己的意志 将这戒指还给了上天<br>就像是在宣告 从今往后命运将不再交给全能的神<br>而是人类凭自身意志活下去的时代到来了</p></blockquote><p>国服有着千里眼的性质，明知道最后的结局会是这样，但就像《海上钢琴师》说的，“明知道结局如何，但是只能放开手，做自己该做的事，不去干扰他的选择”。在今年上海举办的 FES 中，医生的王座前摆满了鲜花，以及他最爱吃的草莓蛋糕。</p><p><img src="/2018/12/31/我的2018，记忆名为伽勒底/3.png" alt=""></p><p>在逃离之刻，作为第四兽的芙芙，灾厄之兽凯茜帕鲁格，舍弃了自己的知性，仅仅因为在迦勒底中，在与玛修咕哒子的接触中，它明白了什么是美丽的存在，什么是不用厮杀就能打倒的恶，什么是不用流血才能到达的答案，什么是善良。</p><p>最后，“未返回者，一人”，其实医生的心，就在这里，跟着咕哒子和玛修，一同返回到了他所拯救的新世界里。</p><p>“<strong>啊，天空，仍是如此湛蓝</strong>”</p><p><img src="/2018/12/31/我的2018，记忆名为伽勒底/4.png" alt=""></p><p>终章这段我是重新回顾了一遍剧情才执笔的，很多复杂的感情我很难通过目前的文笔表达出来。我承认，我打终章那一晚，蜷缩在被窝中哭了多次。</p><p>FGO用一年的时间，谱写了一部关于爱与希望的诗歌。</p><h2 id="三四月"><a href="#三四月" class="headerlink" title="三四月"></a>三四月</h2><p>三、四月份时跟着同学“入赘”了物光院老师的一个军方项目，做的多无人机多无人车自主协同。提到这个项目就很气，我们辛辛苦苦把项目书写的完完整整的，我甚至把识别交通标识的算法都训练好了，最后老师跟我们说这个对于本科生太难了，把项目书丢给他的研究生让他们按着我们的项目书把项目做了，贼气。</p><h2 id="八九月"><a href="#八九月" class="headerlink" title="八九月"></a>八九月</h2><p>八、九月份时，合伙参加了中美联合创客大赛，队长是上面那个项目的队长，其余的有一个西交大的小伙伴负责建模，一个北邮的小伙伴与两个普度的小伙伴负责硬件以及信号采集。我在队内负责的是算法，将 EMG 采集到的肌电信号分析特征后分类，再对即将可能做出的手势做出预测。这段时间内我读了超多的论文，把一些论文中觉得我能用上的方法挪用到了自己的模型中来，最后使用的模型是渐进神经网络（Progressive Neural Network, PNN），是迁移学习中多任务学习的一种变形。因为我们能收集到的数据源过少（十几M，都是队员们自己的肌电信号），模型选择上从 RNN 最后过渡到了 PNN，使用的辅助数据集是一篇论文使用的数据集，我发现我们数据之间的差别不是很大，稍微洗了下就直接用了。总之，9月份的决赛还是很开眼界的，在北京中华世纪坛举办，伙食和住宿条件很棒（划重点）。</p><p>上大学以来，参加的各种比赛都是与互联网相关的，睁眼闭眼都是专业内涉及到的，唯独这次感受到了百家争鸣百花齐放。决赛作品涉及到各行各业，印象很深刻的是一个自动染布机，运行效果超酷炫。</p><h2 id="十月十一月"><a href="#十月十一月" class="headerlink" title="十月十一月"></a>十月十一月</h2><p>十月十一月主要是冲刺TG成绩，要在申请之前成功拿下。这段时间我感觉是目前人生中压力最大的时期，压力来源于各种方面，家里对我的期待，自己的预期甚至周围同学的成绩，无形中几乎使我崩溃。那段时间我感觉高考时都没这么难受，天天往返于家、图书馆和食堂之间，脑子里都是英语，吃饭时会考虑这个食物英语怎么说，走路时会不由自主的思考作文模板怎么写才好。一天陪伴我最长的就是TPO的听力，几乎所有在图书馆的时间除了背单词就是练听力，没办法，基础差的伤不起。</p><p>感觉周围每一个出国党，十月与十一月都是最难熬的一段时间，这段时间，周围的同学保研的保研，工作的工作，考研的也学的滋滋有味，唯独选择出国的，我当时根本感受不到我的未来，或者说不像其他人一样明确，很迷茫，对未来是否能够成功录取以及录取到什么样子的学校，仿佛一柄刀子样悬在头顶，惴惴不安。</p><p>写到这里就想起今年两个假期参加的托福补习班，之前叫 Nova，后来改名为 U.Know。并不是打广告，<strong>这里的老师是我见过最负责任最有热情的老师</strong>，上课环境也是超级棒（免费的饮料和咖啡，赞）。跟我上课的基本都是深圳的高中生，我在里面上课时深深感受到了年轻真好。同时感慨更多的是阶级与阶级之间的差距难以追赶，家境比你好的人比你还努力，同时人家还享受着相对来说更高级的资源。也许迈步中产可以去搞互联网或者金融，但是中产与中产以及往上的差距，是一代人难以弥补的。当我们还在挣扎于高考这泥潭中时，他们已经在考虑去哪个国家做志愿者。</p><p>我现在已经能理解，为什么在北京上海深圳这些超一线城市生活这么艰难，但是还是有很多人选择拼了命的往里钻。我从小学初中高中，没有一位老师系统的教过音标，我的音标都是自学的，甚至高考结束后都有些音标不认识。寒假初次去补课时，我的口语老师曾把我叫出去，一个音标一个音标的带我过，当时的我是大三，教室内的高中生在等我过完音标继续上课，当时心里真的很难受。但这也没办法，家乡是个四五线的小城市，我能有如今这种成就我已经很知足了。但相比于这些超一线城市，基础教育以及生活素养是难以望其项背的。我老舅家的小孩，今年才上小学一年级，但是已经上过一年多的英语外教课程，我是很羡慕的。社会资源是不平等的，如何争取到更多的资源只有凭借更高的工资更好的工作。</p><h2 id="十二月"><a href="#十二月" class="headerlink" title="十二月"></a>十二月</h2><p>说远了说远了，不管如何，十一月我结束了所有的TG考试，不管怎样就这么申请吧。最终到来了较为舒适的十二月。</p><p>十二月的我可谓是很小资了，在周围考研同学都在拼命冲刺的时段，我能在图书馆悠闲的看着自己喜欢的书籍，搞着自己喜欢的研究，唯一的担心就是醒来时看到学校发来的拒信。我的运气还不错，有些学校刚在十一月末提交完申请，十二月中旬就给我发了 offer，截止到本文，我已经收到了三个学校的 offer，一个比一个好，家里人也跟着我一起高兴。</p><p>但是到月末，感觉自己的十二月仿佛荒废了一样，就好比在之前在海底忍受着海压潜泳，然后突然间浮出水面，体内压力失衡。曾有连续的几天，几乎天天在家里宅着，刷着b站知乎。后期我是强迫自己起来去图书馆，才能正经看会书敲点代码刷刷题，准备明年的实习。虽是这么说，但是我个人认为那段时间成功的将我几近崩溃的精神状态调养了过来。</p><h2 id="二〇一八大事记"><a href="#二〇一八大事记" class="headerlink" title="二〇一八大事记"></a>二〇一八大事记</h2><p>时间线就这么捋完了，接下来我要记录下今年值得纪念的事件，或许其中有些事情不算什么，但是对于今年的我来说，这些就像担担面里的花生，虽不是主食但吃到时还是会惊艳一下。</p><p>清明前，终于决定动身前往武汉<strong>赏花</strong>。一直向往被武大捧起的樱花大道，而且武汉也是国内有头有脸的赏花圣地，作为樱花喜好者，一定要去的。清明时，跟爸妈一同前往了京都，去清水寺看那樱花遍野。然而运气不是很好，今年的气温升的有些快，清明并不是京都一带最好的赏花季，我们虽然错过了清水寺的樱花，但是来到了海拔较高的上野，如愿的看到了樱花满山。虽说错过了所谓最好的赏花季，但是我们去的时间节点正是上野樱花的末期。记得十分清楚，当天飘着小雨，风拂过树林，樱花瓣随着细密的雨丝落在身上，名副其实的樱花雨。</p><p><img src="/2018/12/31/我的2018，记忆名为伽勒底/5.JPG" alt=""></p><p>樱花可谓是我的本命花了，除了多年日漫的影响，除了它自身的美丽，我觉得樱花与人的一生特别相似，<font color="red"><strong>虽短，但是够绚烂</strong></font>。</p><p>今年我们软院亡了，学校政策改变，软件学院和计算机学院合并成计算机科学与技术学院，也算是曲线救国，成功跻身ESI前千分之一的计算机学院，嘿嘿。但是很担心的是SSSTA之后的存留问题，很怕明年就没有了这个组织，很怕明年302这个地方会被学院收回，很怕失去这个学校唯一值得我留恋的地方，很怕贵协这个寄托着我诸多宝贵记忆的组织成为历史。</p><p>今年我成功说服爸妈，有了自己的“肥宅快乐柜”，将大学四年省吃俭用（生活费）、辛苦工作（外包）、努力学习（奖学金）变现的手办放到了一起，摆在了透明的玻璃橱窗中，可能有些羞耻，但是有这么一个柜子摆着自己的心血，看到的时候内心还是超级激动的，这毕竟是每一个死宅的梦想。</p><p><img src="/2018/12/31/我的2018，记忆名为伽勒底/6.JPG" alt=""></p><p>今年参加了银临的线下，从九点多进场排队到下午三点，才和小伙伴买到银临的签售，而且能如此近距离接触到银临女神，是超级激动的。今年还参加了西安哈迷的线下活动，虽说加入了这个组织两年有余，但是还是第一次参加线下，和哈迷一同看的《神奇动物在哪里2》的点映，一堆人穿着长袍，带着本院的围巾，有些小伙伴带了魔杖甚至扫帚。感谢这次线下，给了我们一个环境，在这个环境中我们能肆意妄为的表达对魔法世界的向往与喜爱而不顾路人的眼光，漫展同理。</p><h2 id="工作"><a href="#工作" class="headerlink" title="工作"></a>工作</h2><p>是的，已经到了要面对之后独立生活的年纪了，尽管我还有一年半左右的研究生生涯，但是工作也必须提上日程了。说实话，硕士的作用，个人认为就是为本科和工作之间做铺垫，如果想研究，那就去读博吧。</p><p>我将来干什么？什么方向？起步的语言？这些问题我思考了一个暑假加大半个学期，毕竟学的是软件工程，细分方向的话能给你说出小一百种，开发？研发？还是运营？应聘时选择什么语言？Java？C++？还是Python？也许其他人没有这种苦恼，毕竟他们只会一种。说到这里，我要感谢SSSTA，贵协的学长教授了我们很多宝贵的学校学不到的经验，同时感谢自己上了一个比较水的大学，从而有很多时间去学自己感兴趣的东西而不被学业束缚。</p><p><strong>大学只是提供教学设备和资源的一个地方，不代表或者决定你是一个什么样子的人，关键在于你对自己的定位，还有怎么利用资源。</strong>如果将我的大学生活比作在海中航行的话，那SSSTA就是我航行路上的灯塔。在刚刚入学，对软件工程这个概念一知半解时，贵协的学长清楚的告诉我，我们这个专业是干什么，能干什么，各个方向是什么，哪些知识适合初学者掌握。可以说我的大学是很幸运的，其中一半来源于高中毕业时爸爸单位同事送给我的Mac，另一半来自于SSSTA的学长们的教导。</p><p>那台Mac使我专注于学习，SSSTA告诉我应该学什么。如果这两者缺失了一个，都不会有现在的我。感谢爸爸的那位同事，感谢SSSTA。在同届刚刚接触到C语言时，我已经将C的语法学完了；在同届学完C时，我已经会编写iOS程序了。并在大学的第一个寒假，接了个简单的iOS外包，用自己赚的钱买了垂涎已久的 PS4。在同届刚刚接触面向对象编程时，我已经在 Coursera 上听吴恩达的机器学习课程了；在同届刚刚掌握一门将来吃饭的手艺时，我已经接触过了自己感兴趣的所有方向，iOS（OC）、前端（VUE，JS）、后端（JS，Python）、机器学习（Python）、客户端（C#）、甚至接触了函数式编程（Scala）。正因为接触过的东西多，所以选择一个作为吃饭的手艺我思考了很久。</p><p>我是在2016，也就是大二的时候开始跟 Coursera 学的机器学习，当时行业上机器学习刚刚种下爆发的种子。一年不到，深度学习已经火到大江南北，势头可以跟当年的云计算拼一拼，几乎到了老少皆知的地步。一年的时间行业就基本完成了从0到1的建设，现在已经向从1到10前进。虽然现在机器学习岗还是很火，但是很显然已经处于退潮期（没能赶上黄金期）。很难想象硕士毕业，也就是两年后，这个行业的发展会如何，是仍蓬勃向外扩展，还是趋于饱和。</p><p>我当初刚刚上大一时，业界内正是iOS开发的黄金期，网上的iOS开发辅导班如过江之卿，但等到大二下学期国内的iOS待业人员已经过饱和，就连很多辅导班的老师都找不到工作。我现在就担心，对大数据的挖掘日渐加深，近年深度学习模型又没有太大的进展，就连比较新的GAN，也是2014年的算法了。顶会很少出现新算法，大都是对现有算法（CNN为首）的优化，不知道等我毕业那年机器学习就业情况。</p><p>现在正处于对将来方向的选择期，研究生不同于本科，所选的方向跟就业方向还是比较密切的，所以思考了很久，最终还是放弃了热门的机器学习。选机器学习领域的话，就必须选定一个方向钻研下去，而且估计要细到CV、NLP等具体领域（光说深度学习明显是不够的），因为不用说几年后，现在大厂的机器学习人才已接近饱和，待我毕业时，大厂的刚需是高端人才。什么是高端人才？是博士。我个人不是很喜欢读博，一是对研究的兴趣不是很大，而是怕掉头发。</p><p>曾经我有段时间是想继续搞机器学习的，觉得将传统的机器学习算法与神经网络结合，有着光明的发展前景。但是近几年的顶会论文几乎没有做这个方面的，而且学校内实验室的方向也是传统的神经网络实验室。这些使我感觉我这个想法很难实现，因为我知道自己的极限，相比于天才差了很大一截，那些发顶会的天才都没有将这个想法变现，我一个普通211的大学生又何德何能呢。</p><p>于是出于诸多原因，我最终选择了C++。一是C++这门语言是万金油，比Java还万金油，下到底层上到操作系统，C++都能干。二是C++这门语言比较难，它有着许多良好但复杂的语言特性，可以说从中诞生了Java和C#。如果我将C++吃透了，即使将来需要换门新的语言，起手也不会太难。三是距离我正式工作还有两年左右的时间，我有足够的时间将这门语言掌握以及积累项目经验。选择C++也算是一种豪赌吧，将放弃熟悉的python为赌注，去赌一个更广阔的未来。</p><h2 id="技术栈"><a href="#技术栈" class="headerlink" title="技术栈"></a>技术栈</h2><p>2018一年，我几乎没有学习什么新的知识，因为整一年的重心都是英语。</p><p>唯一能谈得上进步的地方就是上半年接触了前端，初步入门了VUE；然后复现了一些神经网络论文，到YouTube上看了一些较为新的公开课或者说是报告；跟中科院做了一个文本情感分析的项目。</p><p>今年在技术上的进步仅此而已，说来惭愧。</p><h2 id="新的习惯"><a href="#新的习惯" class="headerlink" title="新的习惯"></a>新的习惯</h2><ul><li>笔记习惯回归到纸张。</li></ul><p>之前学习新的内容，或者听新的课程，习惯于用电脑记录笔记。但是这有个弊端就是，当需要记录下一些公式时，你需要将手头的放下，花费相较长的时间用 Mathtype 整理公式。用笔记这个方式萌芽于上学期跟李磊一起上课，他当时在听斯坦福的CS224n，他当时就是边听边用笔记，我当时突然发现这样效率好高。</p><p>而且当我选择这种方式后，发现记录表格图表之类的内容时，效率也远比电脑输入快。纸张记录的弊端就是保存问题，我为了解决这个问题采取的是，每当完成一篇笔记时，就扫描成PDF存入电脑。</p><p><img src="/2018/12/31/我的2018，记忆名为伽勒底/7.png" alt=""></p><ul><li>习惯于用便签记录提醒生活的琐事</li></ul><p>岁月不饶人，老了老了，有些事情如果不记下来，会忘记的。</p><p>初始我选择纸质的便签，后来发现不能随身携带并且就算随身携带，也不能随时随地记录。于是就倾向于手机和电脑配合记录，感谢苹果，生态体系好，手机电脑同步超级方便。</p><ul><li>习惯于VSCode</li></ul><p>之前习惯于用专门的应用处理专门的事情。</p><p>比如写C/C++，我用CLion；写python，我用PyCharm；写前端，我用WebStorm；写java，我用IDEA；写Markdown，我用专门的编辑器。</p><p>现在我统统VSCode，甚至写txt</p><ul><li>锻炼</li></ul><p>这个习惯是后半年开始培养的，现在坚持的并不如前几者那么好。</p><p>起因主要是因为今年暑假，我去深圳除了补习英语外，还主要去检查心脏。那段时期，我如果前一天没有休息好，第二天起来时心脏跳的仿佛爆炸一样；日常也会出现一阵阵的针扎似的痛。严重到晚上睡觉我都不能向左侧着躺着睡觉，如果向左侧躺着，几乎就几分钟的时间，心脏就开始疼。甚至向右侧躺着时，左胳膊不能压在胸膛上，否则心脏也承受不了。</p><p>在深圳给心脏做了全套的检查，前前后后花了近三千，最后得出的结论就是心悸，原因是昼夜生活不规律、长时间熬夜以及长时间缺少运动。从那之后，我几乎天天都要保证每天的运动量，我胖，跑步难受，那我快走还不行吗？回学校后也找到了一起夜走的小伙伴。这几天因为天气过于寒冷，取消了夜走，但是白天我几乎还是坚持出去走的。原本是有打羽毛球的小伙伴的，但是他这学期去了日本，所以羽毛球这项运动就搁置了。</p><h2 id="至于今年的记忆，为什么叫迦勒底"><a href="#至于今年的记忆，为什么叫迦勒底" class="headerlink" title="至于今年的记忆，为什么叫迦勒底"></a>至于今年的记忆，为什么叫迦勒底</h2><p>因为我除了是个王厨外，FGO 这款游戏给了我很大的安慰。我几乎把它当做了避风港，在里面逃避一会现实的压力，在里面舒缓自己紧绷的神经。我还通过 FGO 这款游戏，认识了很多志同道合的小伙伴，和小伙伴在群里吹吹逼也是我很重要的解压方式。</p><p>正如 FGO 内所说的，“<strong>无数的邂逅在等待着你</strong>”，我选择出国，这意味着世界会以一种前所未有的速度扑面而来，我将面临更多的挑战，将接受更多的羁绊，将拥抱不确定的未来。</p><p>FGO 作为一款文字量达百万级的手游，对我而言它超脱了游戏这个范围，里面的故事使我感动，里面的故事使我成长。如今，FGO 已经陪伴我两年有余了，与迦勒底从者们的故事仍将继续。</p><p>2019年，借用 FGO 中的一句话，我要豪爽的说：“<strong>吾等乃是决定星辰未来，在星辰上刻下碑文的存在</strong>”。</p><h2 id="感谢"><a href="#感谢" class="headerlink" title="感谢"></a>感谢</h2><p>2018年对我来说，是非常重要的一年。</p><p>如果说2015年是因为高考，改变了我的人生轨迹；那今年就是出国，将彻底决定今后的命运。</p><p>我非常感谢这一年来帮助过、鼓励过我的人，特别是我的爸妈。我知道，他们是那种恨不得将我留在他们身边一辈子的父母，我选择来西安上学有部分原因就是这里离家远，他们不能常来。这种父母竟然能忍心将我送出国外，过着每年才能相见几月的日子，爸妈的付出我很感动，他们几乎放弃了今后的幸福生活，换得我一个广阔的未来，我很感谢，真的。将来我无论要面临即使登天般的困难，也要想办法把他们接到外面，在我的陪伴下，度过晚年。</p><p>其次我要感谢帮助过我的老师，鼓励过我的同学。</p><p>出国路是漫长的，漫长到几乎要从大一就要准备（我起步晚了，是从大二开始的，所以绩点并不是很好）；出国路是艰辛的，艰辛到甚至比高考比考研难上数倍。当时年轻的我觉得，出国比考研简单多了，考研还要学那么难的数学，背难以理解的政治。但是在出国这条路上走远后才发现，考研再难，也是学习。出国是不一样的。你需要在努力学习维持一个较高GPA的基础上，不断去丰富自己的软背景。这里丰富软背景包括但不限于参加各种大型比赛，去参加海外游学访学，去参加志愿者，你需要在你的软背景上呈现出：我不仅是一个学习好的学生，我还热爱生活，热爱新事物，关心社会时事，全面发展。这些可不仅仅是笔头说说的，是需要实际行动来证实的。</p><p>我很庆幸，自己特别热爱自己选择的这个专业，从而有很丰富的软背景；我很庆幸家里人支持我，给我提供资金帮助我完善这些软背景；我很庆幸，遇到了很多负责任的老师，从标化成绩到软背景提升，都给予了我莫大的支持与帮助；我很庆幸，遇到了很多可爱的小伙伴，给我加油打气，给我前行的动力。</p><font color="red"><strong>感谢今年帮助过我的所有人</strong></font>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;今年发生了太多值得写下来的事情，今年的感想也特别多。今年的种种最终交汇成这篇文章，以文字和图片的形式记录下来。尽管文笔有限，但文字会如咒语般，唤醒背后存在的情感与记忆
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;每年年末都要抽出一两天的时间来回忆今年我干了什么，还记得去年的总结写完后已经过完年了，就没有发。好长时间没有写这种文章了，文笔肯定略显平庸，反正是总结，凑活着写吧。&lt;/p&gt;
&lt;h2 id=&quot;时间线&quot;&gt;&lt;a href=&quot;#时间线&quot; class=&quot;headerlink&quot; title=&quot;时间线&quot;&gt;&lt;/a&gt;时间线&lt;/h2&gt;&lt;p&gt;先捋一遍时间线吧。&lt;/p&gt;
&lt;h2 id=&quot;一二月份&quot;&gt;&lt;a href=&quot;#一二月份&quot; class=&quot;headerlink&quot; title=&quot;一二月份&quot;&gt;&lt;/a&gt;一二月份&lt;/h2&gt;&lt;p&gt;今年的一、二月份，对于大多数 fate go 国服玩家来说都是一段不可明灭的记忆，从第七章的乌鲁克的救赎到终章的众志成城，蘑菇本人亲手执笔创造出来的剧本诚不欺我。&lt;/p&gt;
&lt;p&gt;下面这部分是对这部分的一个回忆。&lt;/p&gt;
    
    </summary>
    
      <category term="LIFE" scheme="https://saberda.github.io/categories/LIFE/"/>
    
    
  </entry>
  
  <entry>
    <title>简述迁移学习</title>
    <link href="https://saberda.github.io/2018/11/21/%E7%99%BD%E8%AF%9D%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/"/>
    <id>https://saberda.github.io/2018/11/21/白话迁移学习/</id>
    <published>2018-11-21T03:19:26.000Z</published>
    <updated>2018-11-21T13:59:14.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>本篇并不是对迁移学习的一个概述，只是简单说明什么情景应该使用迁移学习，以及迁移学习的一些基本算法思路</code></pre><p>首先介绍的是<strong>使用情景</strong></p><blockquote><p>Data not directly related to the task considered</p></blockquote><p>直译过来就是使用的数据与任务目标不是直接相关。举个例子来帮助大家明白这句话，我是在今年夏天时的一个比赛中了解到这个算法的，当时我的任务是通过分析 EMG （肌电信号）来识别以及预测手势。当时的问题是，我们小组内并没有足够的数据，这里的数据指的是使用我们小组研发的 EMG 采集器收集的数据，基本都是组内人员自己制作的。那么问题在于，我们花费了大量时间收集数据，但是数据量还是相对而言较少，如果直接将这些数据给神经网络训练的话，最后得到的结果可能无法避免的过拟合。</p><p>这种情况就可以采用迁移学习的思想，使用自己的少量数据与使用其他与当前任务相关不大的数据源一同训练。在上述例子中，我最后使用了国外的一个大学实验室收集的 EMG 信号当 Source Data。</p><a id="more"></a><p>这里插入一下迁移学习中的<strong>两个常用术语</strong></p><ul><li><strong>Target Data</strong>：与当前 Task 有关的数据</li><li><strong>Source Data</strong>：与当前 Task 无关的数据</li></ul><p>上诉两种数据都可以细分为 <strong>labeled</strong> 与 <strong>unlabeled</strong></p><p>在上面的例子中，我们小组内自己收集的数据就是 Target Data，国外实验室的数据就是 Source Data。</p><hr><p>下面就是针对数据的不同形式，从而对迁移学习常用算法进行一个简单介绍。我将从 Source Data 与 Target Data 的 labeled 与 unlabeled 两个状态分为四个较大的模块。具体可以见下图</p><p><img src="/2018/11/21/白话迁移学习/1.png" alt=""></p><h2 id="Model-Fine-Tuning"><a href="#Model-Fine-Tuning" class="headerlink" title="Model Fine-Tuning"></a>Model Fine-Tuning</h2><p><strong>使用数据情景</strong>：Target Data 与 Source Data 都是 labeled</p><p><strong>Task description</strong>： </p><ul><li>Target Data: (x<sup>t</sup>, y<sup>t</sup>) -&gt; (very little)</li><li>Source Data: (x<sup>s</sup>, y<sup>s</sup>) -&gt; (a large amount)</li></ul><blockquote><p>“One-shot learning: only a few examples in target domain”</p></blockquote><p><strong>具体思想</strong>：</p><p>Training a model by source data, then fine-tune the model by target data.</p><p>该方法主要面临<strong>问题</strong>：因为只有有限的 target data，所以要谨慎处理过拟合问题</p><h2 id="一些防止过拟合的训练方法"><a href="#一些防止过拟合的训练方法" class="headerlink" title="一些防止过拟合的训练方法"></a>一些防止过拟合的训练方法</h2><ol><li>Conservative training </li></ol><p><strong>方法主要思想</strong>：</p><p>加入限制，让 Source data 训练出的 model 与 Target data 训练出的 model 差距不要过大</p><p><img src="/2018/11/21/白话迁移学习/2.png" alt=""></p><ol start="2"><li>Layer Transfer</li></ol><p><strong>方法主要思想</strong>：</p><p>先用 Source Data 训练好一个模型，将该模型的大部分 Layer 复制到新模型，用 Target Data 训练该模型中非复制部分的 Layer。</p><p>该方法的<strong>优点</strong>：</p><p>训练 Target Data 时只用考虑非常少的参数</p><p>那么使用该方法时，我们要将哪些 layer 复制过去呢？</p><p>答案是需要分情况讨论。拿目前主要的两个应用方面，语音识别与图片识别，来举例。</p><ul><li>语音识别：通常复制最后几层的 layer，因为这最后几层的作用往往是分辨词汇，与语种的关系不是很大</li><li>图片识别：通常是复制前几层 layer，因为前几层往往是分辨简单的几何图形，与最后的图片识别结果关系不是很大</li></ul><h2 id="Multitask-Learning"><a href="#Multitask-Learning" class="headerlink" title="Multitask Learning"></a>Multitask Learning</h2><p><strong>使用数据情景</strong>：Target Data 与 Source Data 都是 labeled</p><p><strong>Task description</strong>： </p><ul><li>Target Data: (x<sup>t</sup>, y<sup>t</sup>) -&gt; (very little)</li><li>Source Data: (x<sup>s</sup>, y<sup>s</sup>) -&gt; (a large amount)</li></ul><blockquote><p>“Multitask Learning: The multi-layer structure makes NN suitable for multitask learning.”</p></blockquote><p>简单来说，通过训练一个神经网络，使得最后得到多种分类。一般该模型主要分为两类。</p><ul><li>第一类是输入的数据是相同的类型，最后得到多个不同的分类。比如输入都是图片，最后的结果是得到猫的图片与狗的图片。</li><li>第二类是输入的数据不是相同的类型，最后得到多个不同的分类。</li></ul><p><img src="/2018/11/21/白话迁移学习/3.png" alt=""></p><p>下面举一个使用多任务学习的例子，是一个语音识别模型。</p><p>输入数据为“声音（acoustic features）”，输出的类别分别为“法语，德语，西班牙语，意大利语和汉语”</p><p><img src="/2018/11/21/白话迁移学习/4.png" alt=""></p><h2 id="Domain-Adversarial-Training"><a href="#Domain-Adversarial-Training" class="headerlink" title="Domain-Adversarial Training "></a>Domain-Adversarial Training </h2><p><strong>使用数据情景</strong>：Target Data 是 unlabeled 的，Source Data 是 labeled 的</p><p><strong>Task description</strong>： </p><ul><li>Target Data: (x<sup>t</sup>, y<sup>t</sup>) -&gt; (testing data)</li><li>Source Data: (x<sup>s</sup>, y<sup>s</sup>) -&gt; (training data)</li></ul><blockquote><p>“Domain-Adversarial Training: Not only cheat the domain classifier, but satisfying label classifier at the same time”</p></blockquote><p><img src="/2018/11/21/白话迁移学习/6.PNG" alt=""></p><p>上图神经网络中，不同部分的具体分工见下：</p><ul><li>Feature extractor: Maximize the label classification accuracy + Minimize domain classification accuracy</li><li>Label predictor: Maximize the label classification accuracy</li><li>Domain predictor: Maximize the domain classification accuracy</li></ul><p>Ps：本来是想自己做的，发现ppt绘制3D图形太操蛋，就直接挪用了介绍该算法的论文里的图片。<a href="https://arxiv.org/abs/1505.07818" target="_blank" rel="noopener">论文地址</a></p><h2 id="Zero-shot-Learning"><a href="#Zero-shot-Learning" class="headerlink" title="Zero-shot Learning"></a>Zero-shot Learning</h2><p><strong>使用数据情景</strong>：Target Data 是 unlabeled 的，Source Data 是 labeled 的</p><p><strong>Task description</strong>： </p><ul><li>Target Data: (x<sup>t</sup>, y<sup>t</sup>) -&gt; (testing data)</li><li>Source Data: (x<sup>s</sup>, y<sup>s</sup>) -&gt; (training data)</li></ul><p>其中，Target data 与 Source data 是 in different tasks，即在 Source data 中从未出现过 Target data 中的数据。</p><blockquote><p>“Zero-shot Learning: Representing each class by its attributes”</p></blockquote><p>该模型适用于语言识别，因为语言 data 中不可能出现所有单词的发音，所以我们建立一个子表，让模型去识别发音，让识别的发音与子表中的数据去匹配。</p><p>下面为了方便理解，使用一个简单的图片识别做一个浅显易懂的例子，下图中分别对狗、鱼还有猩猩划分了一个简单的子表</p><p><img src="/2018/11/21/白话迁移学习/5.png" alt=""></p><p>其中，分别对其是否有毛发、腿的数量与是否有尾巴等进行判断，若有则标记为“1”，没有则为“0”。子表建立好之后，用输入的数据进行匹配，若该数据有皮毛有四条腿而且有尾巴，那么其大概率是一只狗。</p><p>在训练时，我们不直接辨识那张图片属于哪一类，而是去辨识每张图片它具备怎样的 attribute。</p><p>在测试时，尽管输入一张训练集中没有的事物，但只要找到图中的 attribute，然后查找表，看表中的那个分类最接近结果。</p><p>有时，你的 attribute 会很复杂，这时可以借鉴词向量的思想，对其进行 embedding；甚至当我们没有 dataSet 时，我们可以借用 word2vec，即同时使用 Attribute embedding + word embedding，此时需要计算 loss function。当然，还有一种更简单的 zero-shot learning 方法，即 Convex Combination of Semantic Embedding，读者有兴趣的话可以自行去网上搜索这些算法的具体实现方式，这里写的话会占用超级长的篇幅，并且与“简述”这个主题不符。</p><h2 id="Self-taught-Learning"><a href="#Self-taught-Learning" class="headerlink" title="Self-taught Learning"></a>Self-taught Learning</h2><p><strong>使用数据情景</strong>：Target Data 是 labeled 的，Source Data 是 unlabeled 的</p><p><strong>Task description</strong>： </p><ul><li>Target Data: (x<sup>t</sup>, y<sup>t</sup>) </li><li>Source Data: (x<sup>s</sup>, y<sup>s</sup>) </li></ul><p>该思想有两种主要解释方法，分别针对于非监督学习和监督学习</p><p>针对非监督学习：</p><blockquote><p>“Self-taught Learning: Learning to extract better representation from the source data”</p></blockquote><p>针对监督学习：</p><blockquote><p>“Self-taught Learning: Extracting better representation for target data”</p></blockquote><h2 id="Self-taught-Clustering"><a href="#Self-taught-Clustering" class="headerlink" title="Self-taught Clustering"></a>Self-taught Clustering</h2><p><strong>使用数据情景</strong>：Target Data 与 Source Data 都是 unlabeled 的</p><p><strong>Task description</strong>： </p><ul><li>Target Data: (x<sup>t</sup>, y<sup>t</sup>) </li><li>Source Data: (x<sup>s</sup>, y<sup>s</sup>) </li></ul><p>很少有人在这种情况下，即两个数据集都是 unlabeled 的，使用迁移学习，大家对这个思想了解下就好，<a href="https://www.cse.ust.hk/~qyang/Docs/2008/dwyakicml.pdf" target="_blank" rel="noopener">论文地址</a></p>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;本篇并不是对迁移学习的一个概述，只是简单说明什么情景应该使用迁移学习，以及迁移学习的一些基本算法思路
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;首先介绍的是&lt;strong&gt;使用情景&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Data not directly related to the task considered&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;直译过来就是使用的数据与任务目标不是直接相关。举个例子来帮助大家明白这句话，我是在今年夏天时的一个比赛中了解到这个算法的，当时我的任务是通过分析 EMG （肌电信号）来识别以及预测手势。当时的问题是，我们小组内并没有足够的数据，这里的数据指的是使用我们小组研发的 EMG 采集器收集的数据，基本都是组内人员自己制作的。那么问题在于，我们花费了大量时间收集数据，但是数据量还是相对而言较少，如果直接将这些数据给神经网络训练的话，最后得到的结果可能无法避免的过拟合。&lt;/p&gt;
&lt;p&gt;这种情况就可以采用迁移学习的思想，使用自己的少量数据与使用其他与当前任务相关不大的数据源一同训练。在上述例子中，我最后使用了国外的一个大学实验室收集的 EMG 信号当 Source Data。&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Neural Networks" scheme="https://saberda.github.io/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Reinforcement Learning &amp; Self-Play</title>
    <link href="https://saberda.github.io/2018/07/30/reinforcement-learning-and-self-play/"/>
    <id>https://saberda.github.io/2018/07/30/reinforcement-learning-and-self-play/</id>
    <published>2018-07-30T15:21:22.000Z</published>
    <updated>2018-11-21T05:53:27.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Meta-Learning-amp-Self-Play"><a href="#Meta-Learning-amp-Self-Play" class="headerlink" title="Meta Learning &amp; Self Play"></a>Meta Learning &amp; Self Play</h2><pre><code>This passage is a learning note about a paper talking about the reinforcement learning and self play.First of all, tell a joke.Title: How to perform as machine learning?Q: Do you know the result of 11 * 12?A: Yes. My answer is 233.Q: No, the answer is 132.A: Ok, my answer is 132.lol</code></pre><h2 id="The-reinforcement-Learning-Problem"><a href="#The-reinforcement-Learning-Problem" class="headerlink" title="The reinforcement Learning Problem"></a>The reinforcement Learning Problem</h2><p>The Reinforcement Learning framework just tell you that you have an agent in some environment and you want to find a policy for this agent that will maximize its reward.</p><a id="more"></a><p>It’s a super general framework because almost any problem you can think of can be describe as there is an agent that takes some actions and you want to take those actions which lead to the good rewards, the high rewards.</p><p>Now, the reason that reinforcement learning is interesting is because this reasonably good reinforcement learning algorithms. I should say reasonably good, I should say interesting reinforcement learning algorithms that can sometimes solve problems. So in the formulation, the environment gives the agents the observations and the rewards, but in the real world, the agent need to figure out its own rewards from the observation.</p><p>Humans and animals they are not being told by the world but something is good or bad, it’s on us to figure it out of for ourselves.</p><pre><code>Agent = neural work</code></pre><p>And this is how it looks like</p><p><img src="/2018/07/30/reinforcement-learning-and-self-play/1.png" alt=""></p><p>This is how it looks like now at least where the observation come in and a little network or helpfully a big neural network does some processing and produces an action.</p><p>And I’ll explain to you in this part the way in which the vast majority of reinforcement learning algorithms work.</p><ul><li>Add randomness to your actions</li><li>If the result was better than expected, do more of the same in the future</li></ul><p>So, this two both points it tries something random and if you eat better than expected, do it again.</p><p>And there is some math around it but that’s basically the core of it and then that everything else is like slightly clever ways of making better use of this randomness.</p><p>The reinforcement learning algorithms that we have new can solve some problems, but there is also a lot of things they can not solve.</p><p>If you had a super good reinforcement learning algorithms then you can build the system it could achieve super complicated goals really quickly and basically the technical portion of the field of AI would be complete and a really good algorithms would combine all the spectrum of ideas from machine learning, and reasoning and inference the best time and the training at the best time, all of those ideas would be put together in the right way to create a system which would figure out how the world works and then achieve its goals in this world and do it vey quickly.</p><p>But the algorithm we have today are still nowhere near at the level of what they can be in the future and will be.</p><h2 id="Hindsight-Experience-Replay"><a href="#Hindsight-Experience-Replay" class="headerlink" title="Hindsight Experience Replay"></a>Hindsight Experience Replay</h2><p>So now let’s discuss ways in which we can improve reinforcement learning algorithms and I’ll describe to you one very simple improvement.</p><p>The improvement boils down to this really simple idea so as discussed earlier, the very reinforcement learning algorithms is work is that you try something random and if you succeed, if you do better than expected then you should do it again.</p><p>But what will happen if you try lots of random things and nothing works, this is the case when exploration is hard when you rewards are infrequent you get a lot of failures, don’t have a lot of success. So the question is can we somehow find a way to learn from failure.</p><p>Next, I’ll explain to you the idea very briefly, the idea is the following. You try to do one thing, you aim to achieve one thing but you’ll probably fall unless you’re really good. So you will achieve something else.</p><p>So, why not use the failure ti achieve the one thing as training data to achieve the other thing.</p><ul><li>Setup: build a system that can reach any state</li><li>Goal: reach state A</li><li>Any trajectory ends up in some other state B</li><li>Use this as training data to each state B</li></ul><p>It’s really intuitive and it works.</p><h2 id="Learning-a-Hierarchy-of-Actions-With-Meta-Learning"><a href="#Learning-a-Hierarchy-of-Actions-With-Meta-Learning" class="headerlink" title="Learning a Hierarchy of Actions With Meta Learning"></a>Learning a Hierarchy of Actions With Meta Learning</h2><p>It’s a simple approach for learning hierarchy of actions, so one of the things that would be nice to do in reinforcement learning is to learn this hierarchy with the hierarchy of some kind.</p><p>But it’s never really been successful, truly successful, and I don’t want to claim that this is a success as well this is more of a demonstration which of how you could approach the problem learning a hierarchy if you had distribution over takes, then basically what you want is to train how level controllers such that they make it possible to solve the tasks quickly.</p><p>So you optimize the low level actions such that they make it possible to solve the tasks from your distribution tasks quickly.</p><h2 id="Evolved-Policy-Gradients"><a href="#Evolved-Policy-Gradients" class="headerlink" title="Evolved Policy Gradients"></a>Evolved Policy Gradients</h2><p>It will be kind of cool if we could evolve a cost function which would make it possible to solve reinforcement learning problems quickly, and as easy as you usually do in there kind of situations you have a distribution over take and you literally evolve the cost function. And the fitness of the cost function is the speed in which this cost function lets you solve problems from a distribution of problems.</p><pre><code>Goal: learn a cost function that leads to rapid learning.</code></pre><ul><li>Train a cost function such that reinforcement learning on this function learns very quickly.</li><li>Ingredients: a distribution over look</li><li>Use evolution strategies to learn the cost function</li></ul><p>So the learned cost function allows for extremely rapid learning but the learned cost function also has a lot of information about the distribution of tasks.</p><p>In this case, this result is not magic because you need your training task distribution to be equal to a test at distribution and now it’s improved some more.</p><h2 id="Self-Play"><a href="#Self-Play" class="headerlink" title="Self Play"></a>Self Play</h2><p>Self play is something which is really interesting. It’s an old idea that’s existed for many years back from the 60s.</p><p>The first really cool result in self play is from 1992 by Tesauro where he used a cluster of 386 computers to train a neural network using Q-learning to play backgammon with self play. And the neural network learned to the feed the world champion and it discover strategies but bag of and experts weren’t aware of and they decided and agreed those strategies were superior.</p><p><strong>Appealing properties of Self Play</strong></p><ul><li>Simple environment</li></ul><p>Self play has the property that you can have very simple environments. If you run self play in one simple environment, then you can potentially get behaviors with unbounded complexity self.</p><ul><li>Convert computer into data</li></ul><p>Self play gives you a way of converting computer into data which is great because data is really hard to get but computer is easier to get.</p><ul><li>Perfect curriculum</li></ul><p>Another very nice thing about self play is that it has an natural or perfect curriculum because if you are good then your opponent is good, the table is difficult. You always vain on between 50% of the tome. So it does not matter how good you are or how bad you are. It’s always challenging at the right level of challenge and so it means that you have a very smooth path of going from agents that don’t do much to agents that potentially do a lot of things.</p><h2 id="AI-Alignment-Learning-from-human-feedback"><a href="#AI-Alignment-Learning-from-human-feedback" class="headerlink" title="AI Alignment: Learning from human feedback"></a>AI Alignment: Learning from human feedback</h2><p>Here the question is that we are trying to address is really simple. You know as we train progressively more powerfully AI system it will be important to communicate to them goals of greater subtlety and intricacy and how can we do that.</p><p>Well, in this work, we investigate one approach which is having humans judge the behavior of an algorithms and some be route be really efficient.</p><p>The way it really works is that human judges provide feedback to the system. All of those bits of feedback are being cashed into a model of a reward using a triplet loss. It tries to come up with a single reward function that respects all the human feedback that was given to it.</p><p><img src="/2018/07/30/reinforcement-learning-and-self-play/2.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Meta-Learning-amp-Self-Play&quot;&gt;&lt;a href=&quot;#Meta-Learning-amp-Self-Play&quot; class=&quot;headerlink&quot; title=&quot;Meta Learning &amp;amp; Self Play&quot;&gt;&lt;/a&gt;Meta Learning &amp;amp; Self Play&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;This passage is a learning note about a paper talking about the reinforcement learning and self play.

First of all, tell a joke.
Title: How to perform as machine learning?
Q: Do you know the result of 11 * 12?
A: Yes. My answer is 233.
Q: No, the answer is 132.
A: Ok, my answer is 132.
lol
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;The-reinforcement-Learning-Problem&quot;&gt;&lt;a href=&quot;#The-reinforcement-Learning-Problem&quot; class=&quot;headerlink&quot; title=&quot;The reinforcement Learning Problem&quot;&gt;&lt;/a&gt;The reinforcement Learning Problem&lt;/h2&gt;&lt;p&gt;The Reinforcement Learning framework just tell you that you have an agent in some environment and you want to find a policy for this agent that will maximize its reward.&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Neural Networks" scheme="https://saberda.github.io/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>Extreme Multi-label Text Classification:Kim-CNN &amp; XML-CNN</title>
    <link href="https://saberda.github.io/2018/03/23/EMTC-md/"/>
    <id>https://saberda.github.io/2018/03/23/EMTC-md/</id>
    <published>2018-03-23T08:17:41.000Z</published>
    <updated>2018-03-23T08:58:49.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>This passage is a learning note about a paper talking about the extreme multi-label text classification.</code></pre><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>XMTC -&gt; Extreme Multi-label Text Classification</p><p>Finding each document its most relevant subset of labels from an extremely large space of categories.</p><p>Training data: {(x<sub>i</sub>, y<sub>i</sub>)}<sup>n</sup><sub>1</sub>, x<sub>i</sub> ∈ X, y<sub>i</sub> ∈ {0, 1}<sup>L</sup><br>X is the data, y is the label.</p><p><strong>Goal</strong>:<br>Learning a mapping g: X -&gt; {0, 1}<sup>L</sup><br>Our goal is finding a mapping from x to y.</p><a id="more"></a><p>Each document x<sub>i</sub> is associated with a set of relevant labels, denoted by label vector y<sub>i</sub>.</p><h2 id="Two-key-challenges-in-XMTC"><a href="#Two-key-challenges-in-XMTC" class="headerlink" title="Two key challenges in XMTC"></a>Two key challenges in XMTC</h2><p>When <em>n, L, D</em> are large:</p><ol><li>Scalability -&gt; both in training and testing (n -&gt; the number of document; D -&gt; the number of feature)</li><li>Data sparsity</li></ol><p>So, how to extract richer features representation and how to exploit label correlations are the key challenges.</p><h2 id="Related-works"><a href="#Related-works" class="headerlink" title="Related works"></a>Related works</h2><ul><li>Target-embedding methods </li></ul><p>Compress label vectors in target space down to low-dimensional embeddings.</p><ul><li>Tree-based ensemble methods</li></ul><p>Recursively partitions instance space to induce a tree structure.</p><ul><li>Deep learning for text classification</li></ul><p>Automatically extract features from raw text.<br>Has been remarkable success in multi-class classification.</p><h2 id="Kim-CNN"><a href="#Kim-CNN" class="headerlink" title="Kim-CNN"></a>Kim-CNN</h2><p><img src="/2018/03/23/EMTC-md/1.png" alt=""></p><p>It instead of using just a bag-of-word features, the roll text is fed into the model.</p><p>The resolution may not be that good, so here is a sequence of words, and each word is replaced by the word embedding. So you have a vector here. It’s a word embedding with the associated word.</p><p>So the input will form a <em>n</em> by <em>k</em> matrix, where <em>n</em> is number of words in a document, and <em>k</em> is dimension of the word embedding. You can think of this like image, and we are doing convolutional flitters on this image.</p><p>Then, what they do is they place one deconvolution flitter sliding through the time dimension.</p><p>So the first part is extracting a generalized version of the n-gram features, compared to the traditional bag-of-word or bigram or trigram features.</p><p>The second part, the red box there, they are using the filter size of two, but you can also use three or four and so on. That’s way, you are extracting more generalized bigram or trigram features. </p><p>After that, you have a feature map, and what they do, they just do max pooling to extract the most strongest signal in that feature map and stack them all together. And finally is the fully connected layer.</p><p>So this is a brief introduction of the current strongest method in multi-class classification. </p><p>There are several different parts, roughly three.</p><h2 id="XML-CNN-Extreme-Multi-label-CNN"><a href="#XML-CNN-Extreme-Multi-label-CNN" class="headerlink" title="XML-CNN (Extreme Multi-label CNN)"></a>XML-CNN (Extreme Multi-label CNN)</h2><p><img src="/2018/03/23/EMTC-md/2.png" alt=""></p><p>The first is the convolution, The red box is the convolution filter. We slide through the convolution filter in another dimension. Here we do it opposite direction that we swipe through the convolution filter through the word embedding dimension. You can think in an image is a spatial dimension. So the motivation is like each filters is capturing Moore’s global information, given the whole sequence.</p><p>So flying through different dimension of the word embedding is like capturing the most salient features of word among the entire document. So we also have filter of size 248 et cetera that capturing different spatial relations among the embedding matrix. </p><p>After that, we also have a feature map. What we do is not the traditional max pooling. What we do is like adaptive max pooling that extract, two to three most secure known features in the feature map. So if we are doing that, the final feature representation here will be even two to three times larger than the previous method. In the extreme multi-label setting, we know that the final output number of label here is very huge.</p><p>At last, we need to make a low-rank matrix factorization here. So there is a safe 512th hidden representation in the middle that first project this long feature map to a lower dimension and then we do fully connect.</p><h2 id="Memory-Consumption-A-case-study"><a href="#Memory-Consumption-A-case-study" class="headerlink" title="Memory Consumption: A case study"></a>Memory Consumption: A case study</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Vocabulary size: V(30k)</span><br><span class="line">Word embedding dimension: D(300) </span><br><span class="line">Document length: S(500) </span><br><span class="line">Number of labels: L(670K) </span><br><span class="line">Hidden = 512 </span><br><span class="line">Pooling units = 128Conv1D filters: 32</span><br><span class="line">filter sizes = [2, 4, 8]Total number of parameters: Embedding layer: V*D = 9MConv1D layer: S*32*(2+4+8) = 32K + 64K + 128K = 224KHidden layer: (128*32*3)*512 = 6.29MOutput layer: 512*670K = 343MTotal: 358.51M</span><br></pre></td></tr></table></figure><p>If using floating precision, the memory for medal parameters is around 1.33GB.</p><p>The analysis here is just the minimum of memory you use because when you are doing back propagation and the mini batches, that also depends on the mini batch size you use.</p><h2 id="Choice-of-Loss"><a href="#Choice-of-Loss" class="headerlink" title="Choice of Loss"></a>Choice of Loss</h2><p>We know that if in the Multi-class classification, if you use Softmax, the model will favor only one label and pushing other label to zero.</p><p><img src="/2018/03/23/EMTC-md/3.png" alt=""></p><p>So in the XMTC setting, you mostly put zero probability output on the other labels. But actually, in this kind of dataset, because each document is only tag with the most relevant, say five or ten labels, it doesn°Øt mean that other label is not relevant.</p><p>Using Softmax may not be that good choice, so we consider the most naive Binary Cross-entropy using the Sigmoid.</p><h2 id="So-how-do-we-learn-the-model-parameter-θ"><a href="#So-how-do-we-learn-the-model-parameter-θ" class="headerlink" title="So how do we learn the model parameter θ?"></a>So how do we learn the model parameter θ?</h2><p>We simply use the mean-square out less, the loss function here.</p><pre><code>Denotes the loss function L(~, ~). The energy function is parameterized by a neural network E(y; x, θ)</code></pre><p><img src="/2018/03/23/EMTC-md/4.png" alt=""></p><p>First, we need to find out the prediction y<sub>i</sub> hat, by solving the inference problem and of the energy network, given the fixed model parameter θ. So, exact solving the inference may not be feasible. Then what they do?</p><pre><code>Exact solving the inference may be infeasible, solved approximately by gradient descent with fix iterations.</code></pre><p><img src="/2018/03/23/EMTC-md/5.png" alt=""></p><p>They just do gradient descent with a fixed maximum iteration number, for instance, say just 5. So the do five times obtaining the prediction yi hat and fix that to calculate the gradient based on feature.</p><h2 id="Test-time-Optimization"><a href="#Test-time-Optimization" class="headerlink" title="Test-time Optimization"></a>Test-time Optimization</h2><ul><li>Computation Graph</li></ul><p>So actually calculating the gradient with respect to model parameter θ could be somehow tricky. So, let’s look at the computational graph here. This is the sketch on my note.</p><p><img src="/2018/03/23/EMTC-md/6.png" alt=""></p><p>The input is <em>x</em>, and you path this through some feature network. Use cache the feature and you will calculate, you first make a forward path based on this to calculate the prediction <em>y</em>. So you need to calculate the gradient with respect to <em>y</em> here in the 3rd box is a model part. And in the backward pass, you also want to calculate the gradient with respect to the model parameter θ.</p><p>Now you need to visit multiply times through the model box, for example, the first path it’s like this, the upper part. And the second part is like that, the middle part, because the y<sub>i</sub> is sort of like the final prediction of <em>y</em> is a dependency among the previous state and previous stat is an input function.</p><p>The last, you need to make the derivative multiple times path. But this is the detail and another work in the year is ICML.</p><h2 id="ICML-Input-Convex-Neural-Networks"><a href="#ICML-Input-Convex-Neural-Networks" class="headerlink" title="ICML(Input Convex Neural Networks)"></a>ICML(Input Convex Neural Networks)</h2><p>ICML, which is very similar to the precious structure Prediction Energy Network, except one thing, The design architecture of energy network.</p><p>In this work, they design the energy network to be convex with respect to the y. The benefit of that is when you do the test time optimization, solving inference problems put, this will have a global optimal solution.</p><p>Of course, to design the network to be convex, you have some assumption or can say constraint need to be made on with parameter W. W<sup>(z)</sup><sub>1:k-1</sub> are non-negative, activation functions are convex and non-decreasing.</p><p><img src="/2018/03/23/EMTC-md/7.png" alt=""></p><h2 id="Something-to-say"><a href="#Something-to-say" class="headerlink" title="Something to say"></a>Something to say</h2><p>In recent months，I have so much to do, such as TOEFL, GRE and final exam, that I don’t have time to update my blog. At the same time, I find that that writing notes by hand is more convenient and comfortable than using Word，which is another reason for reducing updates.</p><p>The following photo is two pages of this motes, and please don’t care about ugly handwritings.</p><p><img src="/2018/03/23/EMTC-md/8.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;This passage is a learning note about a paper talking about the extreme multi-label text classification.
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;XMTC -&amp;gt; Extreme Multi-label Text Classification&lt;/p&gt;
&lt;p&gt;Finding each document its most relevant subset of labels from an extremely large space of categories.&lt;/p&gt;
&lt;p&gt;Training data: {(x&lt;sub&gt;i&lt;/sub&gt;, y&lt;sub&gt;i&lt;/sub&gt;)}&lt;sup&gt;n&lt;/sup&gt;&lt;sub&gt;1&lt;/sub&gt;, x&lt;sub&gt;i&lt;/sub&gt; ∈ X, y&lt;sub&gt;i&lt;/sub&gt; ∈ {0, 1}&lt;sup&gt;L&lt;/sup&gt;&lt;br&gt;X is the data, y is the label.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;:&lt;br&gt;Learning a mapping g: X -&amp;gt; {0, 1}&lt;sup&gt;L&lt;/sup&gt;&lt;br&gt;Our goal is finding a mapping from x to y.&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Neural Networks" scheme="https://saberda.github.io/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>谈一谈 Fast R-CNN 和 Faster R-CNN</title>
    <link href="https://saberda.github.io/2017/12/17/FastRCNN-FasterRCNN/"/>
    <id>https://saberda.github.io/2017/12/17/FastRCNN-FasterRCNN/</id>
    <published>2017-12-17T08:10:42.000Z</published>
    <updated>2017-12-17T08:33:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文讨论内容涉及到之前整理的一篇文章，链接见下<a href="http://www.saberismywife.com/2017/10/31/CNN之定位检测/" target="_blank" rel="noopener">CNN之定位检测</a></p><h2 id="R-CNN-的一些问题"><a href="#R-CNN-的一些问题" class="headerlink" title="R-CNN 的一些问题"></a>R-CNN 的一些问题</h2><p>R-CNN并不是完美的，他也有一些问题。</p><p>在测试是它运行的很慢。我们可能有2000个区域，每个区域都要运行一下R-CNN，这就是很慢的原因。<br>我们还会面对一个比较有趣的问题。当我们使用SVM或者regression时是离线训练使用线性回归等方法训练，所以我们的R-CNN没有机会按照相应部分的网络目的升级。<br>同时R-CNN训练管道比较复杂时，他会有一些混乱。</p><p>Fast R-CNN</p><p>为了解决这些问题，有人提出了Fast R-CNN模型。Fast R-CNN的算法很简单：我们只需要交换提取出的区域然后在运行CNN。</p><a id="more"></a><p><img src="/2017/12/17/FastRCNN-FasterRCNN/1.png" alt=""></p><p>这个思想和那个overfeat的浮窗有些相似，所以这里的pipeline看起来有些相像。底层是我们的输入，我们把这个高分辨的图片输入，然后在卷积层运行，这样我们得到了高分辨率的卷积特征映射。之后我们的region proposals使用一个叫ROI pooling的东西从卷积特征映射里分离出这些区域的特征，这些区域的卷积特征将会进入全连接层，最后会有得到在之前文章里介绍过的和classification head和regression head。</p><p>这个算法解决了R-CNN有的一些问题。Fast R-CNN通过共享不同目标框的卷积特征的计算解决了测试过程中R-CNN很慢的问题。在Fast R-CNN中，我们同时训练所有部分，而不使用R-CNN中那个复杂的pipeline。</p><p>在Fast R-CNN中最有意思的部分就是region of interest pooling。</p><p><img src="/2017/12/17/FastRCNN-FasterRCNN/2.png" alt=""></p><p>现在我们有一个输入图片，很大可能是高分辨率的，并且我们有这个目标框。我们可以把这个高分辨率的图片输入到卷积层和池化层。但是现在有个问题是全连接层希望得到low-res conv的特征值，而整个图片的特征是high-res的。在Fast R-CNN中使用了很简单粗暴的方法解决了这个问题。</p><p><img src="/2017/12/17/FastRCNN-FasterRCNN/3.png" alt=""></p><p>对于给出的目标框，我们把它投影到那个conv feature的空间，再把那个conv feature volume切成小块，即切分成下层需要的h*w网络，在对每一小格都进行最大池化。</p><p><img src="/2017/12/17/FastRCNN-FasterRCNN/4.png" alt=""></p><p>所以我们现在使用这个简单的方法，使用目标框共享卷积特征值，最后提取出那个区域的输出。简单来讲，就是交换卷积的顺序，然后扭曲修改。同时这也是个比较简单的计算过程。因为基本上我们只使用了最大池化，而且我们知道如何对最大池化进行向后传播。你可以对这些region of interest pooling进行BP，使得我们可以把整个整体一起训练。</p><p>在实际训练中，Fast R-CNN相比于R-CNN会有更好的结果。这主要是因为其经过调整后良好的属性值。在Fast R-CNN中你可以看到在每个卷积的连接处，都有这样的调整，其保证了输出结果的准确性，使得结果有些提升。</p><p>但是当前模型也存在一个比较大的问题：之前的测试时间没有包含推荐区域的选取，故Fast R-CNN的瓶颈在于推荐区域的选择。如果你将推荐区域的选取所花费的时间也考虑进来，相比于不考虑的情况，速度明显慢了很多。但是解决方法是显而易见的。</p><p>Faster R-CNN</p><p>我们能使用卷积神经网络做回归和分类，那么我们也可以用其来做推荐区域的选取。这就是另一个模型采用的方法，这个模型的名字也是相当简单粗暴，叫 Faster R-CNN</p><p><img src="/2017/12/17/FastRCNN-FasterRCNN/5.png" alt=""></p><p>在Fast R-CNN中我们使用对整张输入图片进行卷积来取代对各个推荐区域进行卷积；在Faster R-CNN中使用区域推荐网络来替代额外的区域推荐算法来获得卷积层最末端的特征图谱，并从中获取推荐区域，完成这一步后剩下的操作和Fast R-CNN一样了。</p><p>Faster R-CNN的神奇之处就在于区域推荐网络，它使得我们可以在一个巨大的卷积神经网络中完成所有工作。</p><p>区域推荐网络的工作方式大致是：我们把卷积神经网络的最后一层的特征图谱当做输入，然后我们将区域推荐网络添加到卷机网络之上，然后对特征图谱进行滑窗操作，就是卷积。我们用3*3的卷积和对特征图谱进行卷积。在区域推荐网络中，我们有两种相似的顶层结构。</p><p><img src="/2017/12/17/FastRCNN-FasterRCNN/6.png" alt=""></p><p>在这边我们进行分类，在另一边我们判断图片中是否包含检测目标，并对位置进行回归。滑窗和特征图谱之间的位置关联表示我们查看的是图片的哪一部分，回归得出的结果给出特征图谱中具体位置。</p><p>但是实际上，它们所做的工作要稍微复杂一些。他们并不是直接对特征图谱中的位置进行回归，其中有几个形状固定的框。你可以想象这些不同形状和尺寸的框根据特征图谱点到原始图片点的关联，覆盖到原始图片上。在Fast R-CNN中，我们将这些框从原始图片映射到特征图谱；现在在Faster R-CNN中，我们将这些框从特征图谱映射到原始图片。</p><p><img src="/2017/12/17/FastRCNN-FasterRCNN/7.png" alt=""></p><p>现在这里有n个卷积框对特征图谱进行卷积，对于每个区域进行卷积它们会产出一个评分来判断这个框内是否有检测目标，它们还会输出4个回归坐标点从而得到正确的框。就像之前介绍的那样，训练区域推荐网络，获得预测未知类的检测器。</p><p>在最原始的Faster R-CNN论文中，他们训练网络的方式挺有意思的。他们首先训练区域推荐网络，然后训练Fast R-CNN，然后他们把两个网络融合起来，最终得到一个整体的网络。之后他们将训练方式整体化，好比他们有一个大网络，输入图片，网络中有一个内置的区域推荐网络，还有一个分类损失值用来表示每个推荐区域内是否包含目标物体。在区域推荐网络中，有一个回归边框，对卷积顶部的特征图谱进行回归，然后进行roi池化，之后做Fast R-CNN的操作，在网络的末端我们会得到分类的损失函数用来表示目标所属类以及回归损失，来获得正确的推荐区域。</p><p><img src="/2017/12/17/FastRCNN-FasterRCNN/8.png" alt=""></p><p>这项巨大的工作在这样一个巨大的网络中完成，最终输出4个损失值，然后对其训练，这样我们可以同时完成目标检测的多项工作</p><p>所以目前世界上最好的目标检测器是101层的ResNet加上Faster R-CNN在加上一些其他的内容。</p><p>因为可以看到在Fast R-CNN中，它是对你的区域建议做了一个修正，然后会将它反馈给你的网路，最后重新分类或重新获取一个新的预测值，这就进行了一次次边框的改进，性能也因为这些内容而得到了提高。除了对这些区域进行分类之外，他们还得到了一个可以体现整张图片所有的特征向量，它可以体现出更好的内容和更好的性能。</p><p>同时，它们还进行了多个尺度的测试，就像我们在Overfeat中看到的那样，测试时将在不同的尺寸上运行。</p><h2 id="YOLO：You-Only-Look-Once-Detection"><a href="#YOLO：You-Only-Look-Once-Detection" class="headerlink" title="YOLO：You Only Look Once Detection "></a>YOLO：You Only Look Once Detection </h2><p>我们之前说过关于定为回归的一个想法，这件事叫YOLO，事实上，它将检测问题直接当做回归问题来处理。</p><p><img src="/2017/12/17/FastRCNN-FasterRCNN/9.png" alt=""></p><p>它将我们输入的图像划分成许多个空间网格（S <em> S），通常是7</em>7的网格，对于每一个网格中的元素，我们得到一个关于边界检测的数（B），在大部分的实验中我们通常使用B=2。</p><p>在每个网络上，你要预测2个B边界框，也就是4个数，还要预测一个树来表示这一边界框的可信度，同时你还要对你数据集中的每一个类计算预测分类的评分，所以这个最终检测问题最终就变成了回归问题。</p><p>对于这个回归问题，往往需要用CNN来训练它。它和我们之前讲的区域建议之类的东西不同，当然他也存在一些问题，它的模型输出的数量是有上限的，所以如果你的测试数据和训练数据中有很多的ground-truth，就可能出现一些问题。</p><p><img src="/2017/12/17/FastRCNN-FasterRCNN/10.png" alt=""></p><p>实际上这个YOLO检测器的速度是很快的，它甚至比Faster R-CNN还要快。但是不幸的是它的工作效果不是很好，所以就有了Fast YOLO</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><p><strong>外部区域推荐的思想</strong>：你在做外部区域推荐的时候，你所做的是选取区域，然后进行卷积。如果能够同时做到这些，那会是很不错的。</p><p>卷积就像一个对图片进行处理的一般过程，你期待卷积能对分类和回归有利。其实卷积中所包含的信息对区域选取同样是十分有用的。这其实就是为了减少计算量。</p><p>在最后，对于所有划分，你使用的是同样的卷积特征图谱，不论是区域推荐，还是接下来的分类和回归。这就是为何能从这里获得运算速度的提升</p><p><strong>Rol池化</strong>：其工作过程是通过将原始图像分解成固定的网络，然后对其做最大池化，对它做旋转变换是比较困难的，但是有一个叫空间变换神经网络模型提出了很好解决该问题的方法。</p><p>它的中心思想是：不同于Rol池化，我们要做的是双线性插值，这有些像我们在处理纹理和图形中用到的。当你使用双线性插值时，也许你就能对这些区域进行操作。</p><p>我认为另一个关于旋转对象比较实际的问题是，我们的数据集中没有与之对应的ground-truth，对于大部分检测数据集，我们仅有的ground-truth信息都是沿着坐标轴的。</p><h2 id="本文中提到的模型链接"><a href="#本文中提到的模型链接" class="headerlink" title="本文中提到的模型链接"></a>本文中提到的模型链接</h2><p><a href="https://github.com/rbgirshick/rcnn" target="_blank" rel="noopener">R-CNN(Caffe + MATLAB)</a></p><p><a href="https://github.com/rbgirshick/fast-rcnn" target="_blank" rel="noopener">Fast R-CNN(Caffe + MATLAB)</a></p><p><a href="https://github.com/ShaoqingRen/faster_rcnn" target="_blank" rel="noopener">Faster R-CNN(Caffe + MATLAB)</a></p><p><a href="https://github.com/rbgirshick/py-faster-rcnn" target="_blank" rel="noopener">Faster R-CNN(Caffe + Python)</a></p><p><a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">YOLO</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文讨论内容涉及到之前整理的一篇文章，链接见下&lt;a href=&quot;http://www.saberismywife.com/2017/10/31/CNN之定位检测/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CNN之定位检测&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;R-CNN-的一些问题&quot;&gt;&lt;a href=&quot;#R-CNN-的一些问题&quot; class=&quot;headerlink&quot; title=&quot;R-CNN 的一些问题&quot;&gt;&lt;/a&gt;R-CNN 的一些问题&lt;/h2&gt;&lt;p&gt;R-CNN并不是完美的，他也有一些问题。&lt;/p&gt;
&lt;p&gt;在测试是它运行的很慢。我们可能有2000个区域，每个区域都要运行一下R-CNN，这就是很慢的原因。&lt;br&gt;我们还会面对一个比较有趣的问题。当我们使用SVM或者regression时是离线训练使用线性回归等方法训练，所以我们的R-CNN没有机会按照相应部分的网络目的升级。&lt;br&gt;同时R-CNN训练管道比较复杂时，他会有一些混乱。&lt;/p&gt;
&lt;p&gt;Fast R-CNN&lt;/p&gt;
&lt;p&gt;为了解决这些问题，有人提出了Fast R-CNN模型。Fast R-CNN的算法很简单：我们只需要交换提取出的区域然后在运行CNN。&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Neural Networks" scheme="https://saberda.github.io/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>如何在Xcode中添加&lt;bits/stdc++.h&gt;头文件</title>
    <link href="https://saberda.github.io/2017/11/17/%E5%A6%82%E4%BD%95%E5%9C%A8Xcode%E4%B8%AD%E6%B7%BB%E5%8A%A0-bits-stdc-h-%E5%A4%B4%E6%96%87%E4%BB%B6/"/>
    <id>https://saberda.github.io/2017/11/17/如何在Xcode中添加-bits-stdc-h-头文件/</id>
    <published>2017-11-17T08:44:23.000Z</published>
    <updated>2017-11-17T09:15:39.000Z</updated>
    
    <content type="html"><![CDATA[<p>在C++中的头文件 &lt;bits/stdc++.h&gt; 十分强大，能帮助你在实际中节省很多时间，它包含了很多头函数，这样你就可以只导入一次头文件就把所有基础头文件一次导入了。</p><p>但是 Xcode 它不自带这个函数，需要我们手动导入。</p><p>首先进入终端，直接进入 Xcode 目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1</span><br></pre></td></tr></table></figure><p>然后在其中创建目录 bits，接下来的所有步骤都需要管理员权限</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir bits</span><br></pre></td></tr></table></figure><p>然后在该目录下创建 stdc++.h 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo touch stdc++.h</span><br><span class="line">sudo vim stdc++.h</span><br></pre></td></tr></table></figure><p>然后进入编辑模式，stdc++.h 的具体代码在<a href="https://gist.github.com/frankchen0130/9ac562b55fa7e03689bca30d0e52b0e5" target="_blank" rel="noopener">这里</a></p><p>然后就可以在工程文件中导入该头文件了</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在C++中的头文件 &amp;lt;bits/stdc++.h&amp;gt; 十分强大，能帮助你在实际中节省很多时间，它包含了很多头函数，这样你就可以只导入一次头文件就把所有基础头文件一次导入了。&lt;/p&gt;
&lt;p&gt;但是 Xcode 它不自带这个函数，需要我们手动导入。&lt;/p&gt;
&lt;p&gt;首先进入终端，直接进入 Xcode 目录&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;cd /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;然后在其中创建目录 bits，接下来的所有步骤都需要管理员权限&lt;/p&gt;
    
    </summary>
    
      <category term="算法" scheme="https://saberda.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>RNN 与 LSTM</title>
    <link href="https://saberda.github.io/2017/11/02/LSTM/"/>
    <id>https://saberda.github.io/2017/11/02/LSTM/</id>
    <published>2017-11-01T17:32:49.000Z</published>
    <updated>2017-11-02T07:18:40.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="RNN-的简短复习"><a href="#RNN-的简短复习" class="headerlink" title="RNN 的简短复习"></a>RNN 的简短复习</h2><p>我们只在一开始输入了一次图片，使用这个模型，你的神经网络就可以回看输入的图片，然后根据图片的一步登生成描述的词汇。</p><p>每当你产生一个新词时，你使你的神经网络可以回看图片，然后找到它想要形容不同特征的下一个词，你可以通过一个可训练的方式实现这些，所以 RNN 不仅产生了形容词，还决定了下一步看向图片的哪里。所以这个模型它所做到的不仅仅是产生了 RNN 的输出结果，还要找到按顺序排列的下一个形容词的概率分布。</p><a id="more"></a><p><img src="/2017/11/02/LSTM/1.png" alt=""></p><p>但是在这个例子里，这个网络给出了一个值，我们运行神经网路，得到了一个 14 <em> 14 </em> 512 大小的激活容量，而且每一次我们得到的不仅仅是这个分布，还有一个 512 维的向量，它大概就像是一个用来查询的关键词，找出你下一步想看图片哪里，但是实际上这不是这片论文主要做的事情。这个 512维 的向量是由 RNN 生成的，用一些权值来预测，然后这个向量可以和这片区域里的每一个 14 * 14 做点乘，这样我们做完了全部点乘，得到了一个 14 * 14 的 compatibility map，然后我们放一个 softmax 在上面，基本上我们对他进行正则化，得到 attention over the image，所以这是一个 14 * 14 的映射。</p><p>基于像在图中的 RNN 最有意思的一点是，我们使用这个概率分布，奇招具有这些特征的权重和。这个 RNN 可以自己产生这些向量来得出最近最吸引自己的一点是什么，然后它回到那里，最后你做了一个关于不同特征加权和，找出这一个时间点上 RNN 最感兴趣的是什么。</p><p>举个例子，RNN 正在生成一些东西，现在我决定要查找一些东西，他生成一个有 512 个数字的向量关于对象之类的东西，然后和卷积神经网络的激活函数交互，然后可能对神经网络的一部分或是激活公式，最后你就集中你的注意力在那图片的那部分。所以基本上，通过这个交互，你可以查找图片，这就是一些和 soft attention 有关的东西了</p><pre><code>RNN 的输入可以有选择的 attention 作为对输入的处理。</code></pre><h2 id="RNN-的复杂化与-LSTM-的引出"><a href="#RNN-的复杂化与-LSTM-的引出" class="headerlink" title="RNN 的复杂化与 LSTM 的引出"></a>RNN 的复杂化与 LSTM 的引出</h2><p>如果你想把 RNN 变得更复杂，其中一种方法就是增加层数。一般情况下，层数越多工作效果越好。堆叠层数的方法有很多，其中一个就是堆叠 RNN 的层数，这部分也有很多方法。</p><p><img src="/2017/11/02/LSTM/2.png" alt=""></p><p>上图中是一种人们经常使用的方法，你可以看到，可以直接向RNN里加入新的层。下一个 RNN 的输入是前一个 RNN 的隐藏状态向量。在这副图里，横轴是时间，向上看使我们的不同的 RNN 。这在张图里，我们有三个 RNN，每个都有属于自己的权重集，而这些 RNN 就是从一个流向另一个，这些 RNN 都是一起训练的（即没有依次训练的说法），全部只有一个过程，只有一个向后传播。</p><p><img src="/2017/11/02/LSTM/3.png" alt=""></p><p>这个 RNN 公式，是对之前的公式一个重写，但是本质却没有变化，任然和之前做一样的处理。我们使用了一个向量形式的深度和一个向量形式的时间，我们把它们和W转移矩阵做处理，再把他们放到 tanh 的运算里。所以当你增加层数或者输入一个W转移矩阵时，这个公式可以写成一个新的公式，这就是我们可以怎样增加 RNN 的层数。这种方法使网络变得复杂，不仅仅局限于增加层数，就是使用一个更好的 RNN 公式。</p><h2 id="Long-Short-Time-Memory"><a href="#Long-Short-Time-Memory" class="headerlink" title="Long Short Time Memory"></a>Long Short Time Memory</h2><p>我们现在已经了解到了这个非常简单的 RNN 公式，而在实际试验中，你基本上用不了这么简单的公式，基本上之前展示的那个网络很少使用，取而代之的是 LSTM（long short time memory），基本现在所有的论文都用这个。所以如果你要使用 RNN，这里就是你要使用的公式。在这里我要强调的是，他的一切都和之前的 Vanilla 网络是一样的，唯一不同的是RNN公式变得复杂了。我们还是从下一层接受 hidden vector，就像你的输入一样，在较早的时间点对应的是前一个状态。</p><p><img src="/2017/11/02/LSTM/4.png" alt=""></p><p>一般情况下，RNN 每一步只有一个向量 h，而LSTM每次有两个向量，hidden cector h 和 cell state vector c。所以每一次我们有 h 和 c 两个并行，然后向量 c 用黄色表示（下面那一层），所以这个空间里每个点有两个向量。</p><p><img src="/2017/11/02/LSTM/5.png" alt=""></p><p>我们把它串联起来，然后放入一个W转移矩阵，之后通过一个更复杂的方式得到新的 hidden state。我们现在只是找到了一个更复杂的办法来处理来自下层和过去的输入来得到一个 hidden state 的更新，接下来我们讲讲这个公式的细节。</p><p>上面展示的 LSTM 的方程我们先讲上面这部分，我们从下面和之前一个状态接受这两个向量，h 是前一个状态，x 是输入，我们用 W 映射到 x 和 h，x 和 h的大小都是 n，他们都由 n 个数字组成，最后我们会产生 4n 个数字，通过 W 矩阵产生了 4n * 2n。<strong>这里有 4 个向量，i, f, o, g，分别是输入 input，遗忘 forget，输出 output，然后i, f, o经过 sigmoid gate，g经过 tanh gate。</strong></p><p><img src="/2017/11/02/LSTM/6.png" alt=""></p><p>所以<strong>这个网络的工作方式</strong>就是，对 c 进行操作，根据你之前的状态和你下层传上来的值，使用 i, f, g, o 实现了对于 c 的操作。这里来解释一下过程，要把 i, f 和 o 想象成二进制的，不是 0 就是 1，我们想用他们来表示门（gate），之后我们对他们进行 sigmoid，因为我们想要使他们可以微分，然后队可以对所有点进行 BP。基于环境就把所有的都当做是二进制的就行了。</p><p>所以这个公式的作用是：根据这些门和 g，我们最终将完成对 c 值得更新，f 被称作忘记门，通常用来把一些细胞（cells）状态置 0，这些细胞可以理解为计数器，他们可以通过 f 运算重新置 0。这里发生的是数组运算乘法，如果 f 为 0，你会发现 c 通过与 f 相乘输出为 0，这样我们就<strong>实现了计数器重置</strong>。</p><p>我们也可以将其与 i 和 g 的乘积相加，由于 i 取值在 0 到 1 之间，g 取值 -1 到 1 之间，因此我们对每一个细胞加上一个介于 -1 到 1 的数值，在每一个时间步内都进行这些运算，包括通过忘记门把它重置为 0 或者加上一个 -1 到 1 之间的数值，这就是我们<strong>细胞状态的更新</strong>。</p><p>隐藏层函数 h 更新是以挤压细胞的形式进行的，tanh(c)，挤压程度由输出门进行调整，所以经过 o 参数调整后只有一部分细胞进入隐含状态，我们通过学习选择一部分细胞状态进入隐藏状态。</p><p>这有一些东西需要强调一下，<strong>我们用 -1 到 1 之间的数值 i 来乘 g</strong>，但是如果用 g 替换，g 取值已经是 -1 到 1 之间，那为什么还要乘上 i 呢？在我们只想要为 c 增加 -1 到 1 情况下，这一步有什么意思吗？这是 LSTM 中 cell 参数的一部分，我认为一个原因是如果你认为 g 是前文的线性函数 tanh 得到的，如果我们直接加上 g 而不是加上 i * g，那将会得到一个非常简单的函数，通过加上 i 乘法操作，我们使函数更加复杂，这样我们更能表达我们加到细胞状态（cell state）里的东西；另一种思考方式就是把这两个概念分开来看，g 表示我们要在细胞状态里增加多少，i 表示我们是否要进行增加这个状态，所以 i 就像表示是否要进行这个操作，g 表示我们要增加的东西，通过分离这两个参数 LSTM 的训练效果会更好。</p><p>好的接下来来看下面部分的公式，我们把它看做细胞流过，<strong>第一个操作是 f 乘以 c</strong>，f 是 sigmoid 函数的输出，若 f 为 0，你将会关闭细胞，然后对计数器重置。如下图所示</p><p><img src="/2017/11/02/LSTM/7.png" alt=""></p><p>这部分是 g，他基本是加到细胞状态（cell state）里的，然后细胞状态一部分进入隐藏状态（hidden state），但是他要先经过 tanh 函数，然后经过 o 参数调整，由此可见，o 向量可以决定细胞状态那一部分进入隐藏状态，你会发现隐藏状态不仅进入了 LSTM 的下一次迭代运算，还会上浮到更高层，因为这个隐藏状态向量要接通上面更远的 LSTM 网络或者进行一次预测。</p><p><img src="/2017/11/02/LSTM/8.png" alt=""></p><p>所以当你展开的时候他是这个样子，从下面得到输入，从前面得到隐藏状态，x 和 h 决定了你的运算门 f, i, g和o，他们都是 n 维的向量，最后对细胞状态进行操作，你可以对它进行一次重置操作，一次加上一个 -1 到 1 之间的值，一部分细胞状态会以学习的方式释放，他可以向上去进行预测，也可以向前进入 LSTM 的下一次迭代。</p><p><img src="/2017/11/02/LSTM/9.png" alt=""></p><p>下面展示的就是 LSTM 正常的工作流程</p><p><img src="/2017/11/02/LSTM/10.png" alt=""></p><h2 id="LSTM-为什么效果比-RNN-好？"><a href="#LSTM-为什么效果比-RNN-好？" class="headerlink" title="LSTM 为什么效果比 RNN 好？"></a>LSTM 为什么效果比 RNN 好？</h2><p>RNN 也有一些状态向量，你要对他进行操作，通过递归公式进行转换，最后随着时间不断更新你的隐藏状态向量，你会发现在 LSTM 中，这些细胞状态进入网络，并且部分细胞进入隐藏状态。</p><p>我们根据隐藏状态决定如何对细胞进行操作。如果<strong>忽略忘记门</strong>，那我们只是对细胞进行加法迭代，所以这有一些操作可以看做是细胞状态的函数，但不管他们是什么，我们最后都是我们都是间接的改变细胞状态，而不是直接对他进行变换，同时这意味着这是一种依赖于加法的不断的计算。</p><p><img src="/2017/11/02/LSTM/11.png" alt=""></p><p>事实上他和 ResNet 有些相似，一般来说用卷积神经网络进行转换词义，ResNet 加入了这些跳跃的链接，你会发现残差有加法操作，即 x，我们要基于 x 进行这些计算，然后再加上 x，这些是ResNet 中的基本模块，事实上这也是 LSTM 中的运算方式，我们有这些加法操作，其中的 x 取决于你的细胞，我们进行了一些函数运算，然后选择加到你的细胞状态中但是 LSTM 与 ResNet 不同，还有这些忘记门可以选择关闭一部分信号。</p><pre><code>看起来拥有加法操作会使网络变得更好，这能使反向传播更加高效。</code></pre><h2 id="为何-RNN-存在梯度消失的问题？"><a href="#为何-RNN-存在梯度消失的问题？" class="headerlink" title="为何 RNN 存在梯度消失的问题？"></a>为何 RNN 存在梯度消失的问题？</h2><p>来考虑一下 RNN 和 LSTM 中反向传播的动态特性，尤其是在 LSTM 中。如果在某些时间中诸如一些梯度信号将会变得非常清楚。如果我在上张图片的最后注入梯度函数，然后这些加法操作就像是梯度的高速公路，这些梯度会经过这些加法门。因为加法相等的分配梯度，所以如果我在这接入梯度，他会沿着网络传递。当然梯度也会通过这些 f，这些有助于梯度回到梯度流，这样使你永远不会遇到RNN中梯度消失问题，因为在 RNN 里这些梯度会在反向传播中死亡变成 0。在 LSTM 中，由于加法高速公路的存在，在任何时间内，我们从上面注入 LSTM 的梯度，都会流过细胞，不会消失</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">H = 5# dimensionality of hidden state</span><br><span class="line">T = 50# number of time steps</span><br><span class="line">Whh = np.random.randn(H, H)</span><br><span class="line"></span><br><span class="line"># forward pass of an RNN (ignoring inputs x)</span><br><span class="line">hs = &#123;&#125;</span><br><span class="line">ss = &#123;&#125;</span><br><span class="line">hs[-1] = np.random.randn(H)</span><br><span class="line">for t in xrange(T):</span><br><span class="line">ss[t] = np.dot(Whh, hs[t - 1])</span><br><span class="line">hs[t] = np.maxinum(0, ss[t])</span><br><span class="line"></span><br><span class="line"># backward pass of the RNN</span><br><span class="line">dhs = &#123;&#125;</span><br><span class="line">dss = &#123;&#125;</span><br><span class="line">dhs[T-1] = np.random.randn(H) # start off the chain with random gradient</span><br><span class="line">for t in reversed(xrange(T)):</span><br><span class="line">dss[T] = (hs[t] &gt; 0) * dhs[t]# back through the nonlinearitydhs[t-1] = np.dot(Whh.T, dss[t])# backprop into previous hidden state</span><br></pre></td></tr></table></figure><p>这是一个关于循环神经网络的例子，在这个循环神经网络中我们不管这些输入，只看隐藏状态的更新部分。所以要最小化权值 Whh，这是一个隐藏状态。现在要对这个 vanilla RNN 进行向前传播，这里我们的时间长度 T 设为 50，我们要用 Whh 乘前一时间长度，然后再用 ReLU 函数来计算。这就是忽略所有输入向量的前向传播，即用 Whh 乘以 h 然后和 0 比大小，再乘以 h 再比大小，循环往复。</p><p>接着要进行反向传播，在最后一步时在这里插入了一个随机的梯度，也就是说在第 50 时间长度插入一个随机的梯度，然后再做反向传播。在进行反向传播时要用到 ReLU 函数，先用 Whh 进行乘法，然后通过 ReLU 函数，再进行乘法再通过 ReLU。有件事需要注意一下，这里我再做反向传播的时候用到了 ReLU，对所有输入值去阈值，丢掉所有小于 0 的数，这里我对 Whh 乘以 h 运算符进行反向传播，事实上我们就是在进行非线性变换之前乘上 Whh 矩阵。</p><p>我们在运行过程中，一遍又一遍的乘以 Whh 这个矩阵，因为在向前传播中我们每一次循环中都乘了 Whh，并且对所有的隐藏状态进行反向传播，最后一公式 Whh 乘以 hs 结束，得到的结果就是你的梯度和 Whh 矩阵相乘，然后对他们使用 ReLU，再乘以 Whh 再运行 RelU，这样我们就乘了这个矩阵 50 次。</p><p>那么他可能会产生两个问题，首先如果你使用的是标量，而不是矩阵，例如使用了一个随机数，然后我得到了第二个数，我不停的用第二个数去乘以第一个数，那么这个序列会变成什么样？有两种情况，我不停的用同一个数去乘他，不管他是 0 还是无限，当然如果第二个数是 1，这是唯一一种不会出现爆炸的情况，否则不管是消失还是爆炸，都将是很糟糕的情况。</p><p>虽然这里是矩阵而不是数，但是实际上他的泛化也会发生这样的事情。如果 Whh 矩阵的谱半径，也就是这个矩阵的最大特征值要比1大很多，那么就会爆炸；如果小于 1，那么就会消失。<strong>因为 RNN 的这个循环的计算，使得它出现了这些很糟糕的问题，它非常不稳定有时候甚至会导致消失或者爆炸。</strong></p><p>所以在实践中，我们可以采取一点小技巧来控制梯度爆炸，在实践中，他就像一个不完整的解决方案，比如你懂得梯度是大于 5 的，那就将元素全部裁剪为 5，这个方法叫做<strong>梯度裁剪</strong>，它可以用来解决梯度爆炸问题，这样你的循环就不会在发生爆炸了。</p><p>但是在循环神经网络中仍然可能会出现梯度消失的问题，而 LSTM 能很好的抑制梯度消失，因为这些高速公路的细胞状态只改变了加法运算，他的梯度会被一直传下去，不会消失。</p><p>这大致解释了为什么他的工作效果会更好。通常我们会使用 LSTM，也会使用梯度裁剪，因为 LSTM 也可能会出现梯度爆炸，但它一般不会出现梯度消失。</p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>首先 RNN 是很不错的，但是只用 RNN 事实上工作的不是很好，所以通常使用 LSTM 来代替，他们最好的地方是他们的加法计算，可以使梯度传播工作的更好，并且也不会出现梯度消失的问题.</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;RNN-的简短复习&quot;&gt;&lt;a href=&quot;#RNN-的简短复习&quot; class=&quot;headerlink&quot; title=&quot;RNN 的简短复习&quot;&gt;&lt;/a&gt;RNN 的简短复习&lt;/h2&gt;&lt;p&gt;我们只在一开始输入了一次图片，使用这个模型，你的神经网络就可以回看输入的图片，然后根据图片的一步登生成描述的词汇。&lt;/p&gt;
&lt;p&gt;每当你产生一个新词时，你使你的神经网络可以回看图片，然后找到它想要形容不同特征的下一个词，你可以通过一个可训练的方式实现这些，所以 RNN 不仅产生了形容词，还决定了下一步看向图片的哪里。所以这个模型它所做到的不仅仅是产生了 RNN 的输出结果，还要找到按顺序排列的下一个形容词的概率分布。&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Neural Networks" scheme="https://saberda.github.io/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络_RNN</title>
    <link href="https://saberda.github.io/2017/11/01/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C-RNN/"/>
    <id>https://saberda.github.io/2017/11/01/循环神经网络-RNN/</id>
    <published>2017-10-31T17:47:43.000Z</published>
    <updated>2017-10-31T18:41:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>循环神经网络，英文名是Recurrent Neural Network，随着对它的不断深入了解，你会发现这个神经网络模型是多么的有趣。你可以喂给它莎士比亚的作品，经过有效的训练后它会给你输出带有莎士比亚风格的句子；你喂给它 Linux 源码，它会装模作样的给你生成一段它自己写的代码，尽管会有语意错误，但是不会有语法错误。</p><p>那么废话不多说，直接进入正文吧。</p><h2 id="Recurrent-Neural-Network"><a href="#Recurrent-Neural-Network" class="headerlink" title="Recurrent Neural Network"></a>Recurrent Neural Network</h2><pre><code>循环神经网络的好处就是他们会在你建立神经网络架构时给予你很高的灵活性</code></pre><p>咱们先来看一下最左边这个例子，所以一般你在处理神经网络的时候，你会得到一个固定大小的向量（即图中的红色框），然后用隐藏层 – 这个绿色的框来处理它，你就会得到一个固定大小的向量，就是蓝色。所以会有一个固定大小的图像进入网络，然后要输出一个固定大小的向量，他是一个 class score，在循环神经网络中，我们可以采用不同的顺序实现，比如从输入开始或输出开始，或者两者同时开始。</p><a id="more"></a><p><img src="/2017/11/01/循环神经网络-RNN/1.png" alt=""></p><p>再举一个图像字幕的例子。比方说你得到了一个固定大小的推向，通过循环神经网络，我们会生成一些按顺序排列的描述图像内容的词，那么这些词会连成一句话，这就是这幅图的描述。</p><p><img src="/2017/11/01/循环神经网络-RNN/2.png" alt=""></p><p>循环神经网络也可以用在情感分类中，我们来举游说的例子。我们会处理一定数量按照顺序排列的词，然后试着去把这个句子里的词按正面情感和负面情感来分类。</p><p><img src="/2017/11/01/循环神经网络-RNN/3.png" alt=""></p><p>在用机器进行语言翻译时，我们也可以用到循环神经网络我们需要让这个网络把这些单词，比如说是英文单词翻译成法语单词，所以我们把这些词放在循环神经网络中，我们把这称之为从一个序列翻译至另一个序列（seq to seq）。所以我们通过这个网络把英文句子翻译成了法语句子。</p><p><img src="/2017/11/01/循环神经网络-RNN/4.png" alt=""></p><p>最后一个例子就是视频分类，提到这个，也许你会想到把视频里的每一帧图像都按照一定数量的类来分类，但关键是你实际上不希望这个预测仅仅是当前时间所对应的当前脱氨的函数，你更希望他是当前时间之前所有图片的函数。那么循环神经网络就可以让你构建一个架构，这个架构可以让你得到一个预测得到某个时间点前所有图片的函数。即使你没有输入或输出的序列，你也可以用到循环神经网络，甚至你在最开始的那个例子中用到他，因为你可以对你的固定尺寸的输入或者输出按顺序的进行处理。</p><p><img src="/2017/11/01/循环神经网络-RNN/5.png" alt=""></p><p>那么循环神经网络实质上就是这个绿色的框。他自己有一个状态，并定期的接受数据，所以每一个在每一个时间点中它有内在的状态。然后它可以通过每个时间点所接受内容的根据函数来修改自己的状态，当然，他会在 RNN 里等待着，RNN 会根据他在接受输入时状态的参与程度来改变它的行为。</p><p><img src="/2017/11/01/循环神经网络-RNN/6.png" alt=""></p><p>我们还要关注基于 RNN 状态所生成的输出，我们可以在 RNN 上方生成这些向量，你会看到这样的图，但我要说的是 RNN 就只是在中间的一块，他有一个状态，可以随时间变化接受向量。我们可以在一些应用中根据他上方的状态进行假设</p><p><img src="/2017/11/01/循环神经网络-RNN/7.png" alt=""></p><h2 id="循环的过程"><a href="#循环的过程" class="headerlink" title="循环的过程"></a>循环的过程</h2><p>那么整个过程看起来是这样的：RNN 有某种状态，这里我们记为向量 H 。因为这也可以是许多向量的集合，所以这是一个更加综合的状态。我们现在要根据之前的隐藏状态 h 前的时间 t-1 以及现在输入向量 X 列一个方程，其中还要有一个函数，我把它称之为递归函数（recurrence function），这个函数有一个参数 W ，那么当我们对 W 进行改变的时候，我们就会发现RNN有了不同的表现。当然，我们想要的是RNN的某个特定的表现，所以我们要训练这些数据中的权重。</p><p>现在我要着重说明的是，在每个时间不长中我们都要有同一个函数，同一个固定大小的f<sub>w</sub>，在每一个时间步长中我们都要用这个函数。这样就既可以使我们按顺序使用循环回归网络，又不用去管这个序列的大小。无论输入或输出的序列有多长，在每一个时间不长中我们用的都是同一个函数。</p><p><img src="/2017/11/01/循环神经网络-RNN/8.png" alt=""></p><p>所以在一个特定的循环神经网络的情况中，在可用的最简单的循环中建立这个函数最简便的方法就是 vanilla RNN。在这个例子中，循环神经网络的状态就是这个隐藏状态（hidden state）h，我们还会得到一个循环方程式。这个方程式可以告诉你怎么来更新你的隐藏状态。这需要用到之前的隐藏状态还有现在输入的 X<sub>t</sub> 。在这个最特殊也是最简单的例子中，我们要用到这些权矩阵 W<sub>hh</sub> 和 W<sub>xh</sub>。这两个矩阵分别对之前的隐藏状态和现在的输入做投影，然后把这两者相加，并求出这个和的双曲正切值。这就是我们更新时间 t 下隐藏状态的方法。这个循环所做的就是告诉我们h是怎样随时间和目前时间步长的输入的变化而变化的。那么现在可以对h进行检测。比如用另一个矩阵对隐藏状态进行投影。</p><p><img src="/2017/11/01/循环神经网络-RNN/9.png" alt=""></p><p>那么，这就是一个完整的简单的例子，你可以把它用于你自己的神经网络中。为了讲述这到底怎么用的，我现在要讲 X<sub>t</sub> 和 y 在向量中的抽象形式，我们可以用语义学来分析这些向量。我们可以用到循环神经网络的其中一个方面，也就是字符级语言模型（character-level language model）。这是我认为理解RNN最简单的方法之一，因为它又直观又有趣。</p><h2 id="Char-RNN"><a href="#Char-RNN" class="headerlink" title="Char-RNN"></a>Char-RNN</h2><p>那么现在我们有了一个使用RNN的字符级语言模型，他的工作原理是：把一系列的字符输入到循环神经网络中。在每一个时间步长里，我们都会要求循环神经网络来预测下一个字符是什么，所以他就会根据他所看到的字符来预测下一个字符是什么。</p><p>举一个简单的例子，我们有一个训练序列 hello，字符词汇，即 [h, e, l, o]，我们试着在这组训练数据中使用循环神经网络来学习预测序列中的下一个字符方法开始运作，将每一个字符按照先后不同时间点转化为一个循环神经网路。第一步完成的是 h 字符，然后是 e，我们就这样完成了 H-E-L-L。我们来使用一个词向量 – one hot向量，代表了字符的顺序和词汇。</p><p><img src="/2017/11/01/循环神经网络-RNN/10.png" alt=""></p><p>我们来看一下递推公式。</p><p><img src="/2017/11/01/循环神经网络-RNN/11.png" alt=""></p><p>在每一个测试中我们从h开始，然后我们要求计算隐藏层，每一个时间步骤使用我们的地推公式。假设这一层中只有三个数字，那么我们用一个三维向量，基本在时间点上总结了所有的字符，直到最后一个。每一个时间步骤上都有隐藏层，现在我们可以预测每个时间步骤所连接的序列中的下一个字符是什么。</p><p><img src="/2017/11/01/循环神经网络-RNN/12.png" alt=""></p><p>例如，因为这个单词中有四个字符，我们预测在每个时间点有四个数字。例如在第一个位置，我们对应了字母 H ，同时 RNN 在这时候的权重已经计算到了他现在的字母以及下一个位置和字符。那么推断，H 对应的下一位字符的权重是 H 的话是 1.0，E 是 2.2，L 是 -3.0，O 是 4.1，除了其他可能性，当然，我们也知道，在这个训练序列中，E 是在 H 后面的那个字符，所以事实上，变为绿色的那个 2.2 是正确的答案，我们希望这个值是高的，所以我门要让其他值比较低，这就使我们基本上有一个目标，谁会是序列中的下一个字符。</p><p><img src="/2017/11/01/循环神经网络-RNN/13.png" alt=""></p><p>我们仅仅是想让这一类的值是高的，其他的值是低的，包括绿色的信号、损失函数，并且通过这些链接向后传播。这种用来思考的方法是每个时间步骤，我们基本上有一个大的 softmax 分类器，这些大的 softmax 中的每一个结束后会接着下一个字符，在每个点上我们知道下一个字符是什么。所以我们只是得到了从上到下的损失，并且通过这张图流通，向后至所有的箭头，我们要得到所有权重矩阵的梯度，这样我们将会知道如何转移矩阵。</p><p>那么当前的问题来自于 RNN，所以我们要会塑造这些权重，这才是正确的方式来形成字符，并且你可以想象的出来你是如何训练的。</p><h2 id="训练-RNN"><a href="#训练-RNN" class="headerlink" title="训练 RNN"></a>训练 RNN</h2><p>正如我提到的每个递归场景都有自己的功能，我们每一个时间步骤都有一个 W<sub>xh</sub> ，也有一个 W<sub>hy</sub> 。我们在这个图解中用了四次 W<sub>xh</sub> 和 W<sub>hy</sub>，向后传播时，我们通过这些给他们计数，因为我们会把所有的添加到相同的权重矩阵，也因为他已经被用在多个时间步骤，这使我们能够处理不同输入的大小，因为即使我们每次做相同的事情，功能的数量也不会相同。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># data I/O</span><br><span class="line">data = open(&apos;input.txt&apos;, &apos;r&apos;).read() # should be simple plain text file</span><br><span class="line">chars = list(set(data))</span><br><span class="line">data_size, vocab_size = len(data), len(chars)</span><br><span class="line">print &apos;data has %d characters, %d unique.&apos; % (data_size, vocab_size)</span><br><span class="line">char_to_ix = &#123; ch:i for i,ch in enumerate(chars) &#125;</span><br><span class="line">ix_to_char = &#123; i:ch for i,ch in enumerate(chars) &#125;</span><br></pre></td></tr></table></figure><p>在最开始，正如你所看到的，只有 numpy，一些文本数据正在加载，所以我们在这里只是收集了大量的字符序列，在这种情况下输入 txt 文件，然后我们会得到文件中的所有字符，我们还会找到所有独一无二的字符。然后我们创建映射字典，映射字符索引，从索引能找到字符，我们最基本的还是为了字符。从表面上看是一大堆的数据，我们有数百个的字符或者类似的东西并且在序列里排序，所以我们把索引关联到每个字符上，然后我们在进行初始化。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># hyperparameters</span><br><span class="line">hidden_size = 100 # size of hidden layer of neurons</span><br><span class="line">seq_length = 25 # number of steps to unroll the RNN for</span><br><span class="line">learning_rate = 1e-1</span><br><span class="line"></span><br><span class="line"># model parameters</span><br><span class="line">Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden</span><br><span class="line">Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden</span><br><span class="line">Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output</span><br><span class="line">bh = np.zeros((hidden_size, 1)) # hidden bias</span><br><span class="line">by = np.zeros((vocab_size, 1)) # output bias</span><br></pre></td></tr></table></figure><p>首先是隐藏大小的初始值，因为你会用到 RNN，所以你不能让他成为 100，我们有学习率，序列长度在这里达到了 25，这是一个你需要意识到的参数。此外，需要注意的是，如果我们的输入数据很大，比如说有数百万次，你就没有办法把它放在所有的上面，因为我们需要保持所有的数据和内存，这样我们就可以开始向后传播。但是事实上，我们没办法把他们所有都存在内存中，并且向后传播所有的输入数据块，在这种情况下，我们可以在一段时间内通过一个 25 字符的序列，我会在下文讲到。我们有整个数据集，但是现在要让他变成在某一时间只有 25 个字符的数据块，并且每次都是按时通过 25 个字符，因为我们负担不起太长时间的向后传播，因此我们必须记录所有的数据。所以我们让数据块包含 2 5个字符。然后我们例举了所有的 W 矩阵，分析了一些随机的方框所以 W<sub>xh</sub> 和 W<sub>hy</sub> 都是我们的参数，这样我们才能训练向后传播。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">n, p = 0, 0</span><br><span class="line">mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</span><br><span class="line">mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad</span><br><span class="line">smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0</span><br><span class="line">while True:</span><br><span class="line">  # prepare inputs (we&apos;re sweeping from left to right in steps seq_length long)</span><br><span class="line">  if p+seq_length+1 &gt;= len(data) or n == 0: </span><br><span class="line">    hprev = np.zeros((hidden_size,1)) # reset RNN memory</span><br><span class="line">    p = 0 # go from start of data</span><br><span class="line">  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]</span><br><span class="line">  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]</span><br><span class="line"></span><br><span class="line">  # sample from the model now and then</span><br><span class="line">  if n % 100 == 0:</span><br><span class="line">    sample_ix = sample(hprev, inputs[0], 200)</span><br><span class="line">    txt = &apos;&apos;.join(ix_to_char[ix] for ix in sample_ix)</span><br><span class="line">    print &apos;----\n %s \n----&apos; % (txt, )</span><br><span class="line"></span><br><span class="line">  # forward seq_length characters through the net and fetch gradient</span><br><span class="line">  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)</span><br><span class="line">  smooth_loss = smooth_loss * 0.999 + loss * 0.001</span><br><span class="line">  if n % 100 == 0: print &apos;iter %d, loss: %f&apos; % (n, smooth_loss) # print progress</span><br><span class="line">  </span><br><span class="line">  # perform parameter update with Adagrad</span><br><span class="line">  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], </span><br><span class="line">                                [dWxh, dWhh, dWhy, dbh, dby], </span><br><span class="line">                                [mWxh, mWhh, mWhy, mbh, mby]):</span><br><span class="line">    mem += dparam * dparam</span><br><span class="line">    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update</span><br><span class="line"></span><br><span class="line">  p += seq_length # move data pointer</span><br><span class="line">  n += 1 # iteration counter </span><br></pre></td></tr></table></figure><p>然后我们先看最后一部分。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># prepare inputs (we&apos;re sweeping from left to right in steps seq_length long)</span><br><span class="line">  if p+seq_length+1 &gt;= len(data) or n == 0: </span><br><span class="line">    hprev = np.zeros((hidden_size,1)) # reset RNN memory</span><br><span class="line">    p = 0 # go from start of data</span><br><span class="line">  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]</span><br><span class="line">  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]</span><br></pre></td></tr></table></figure><p>在这里，我们有主函数，我们把这里的一些初始值设置为20，然后我们继续对一批数据进行采样，所以这也是我们在这个数据集处批处理 25 个字符的地方。这些就是输入列表，输入列表基本上只有对应的 25 个字符。你所看到的目标是所有的相同字符，除去移除的那个，因为这些都是我们试图在每一层预测的检索。重要的目标是那 25 个字符的输入列表，还有将会被移除的目标，这就是我们基本的数据采样。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># sample from the model now and then</span><br><span class="line">  if n % 100 == 0:</span><br><span class="line">    sample_ix = sample(hprev, inputs[0], 200)</span><br><span class="line">    txt = &apos;&apos;.join(ix_to_char[ix] for ix in sample_ix)</span><br><span class="line">    print &apos;----\n %s \n----&apos; % (txt, )</span><br></pre></td></tr></table></figure><p>我们使用低层次字符和测试时间的方式，就是我们可以看到一些字符，然而他们并不是那些在这个序列中下一个字符的分布，所以你可以想象从他的采样到他在分布中形成下一个字符。我们需要不断的采样，并且持续下去，就可以生成任意的文本数据，这就是我们要做的代码，并且生成了样本函数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># forward seq_length characters through the net and fetch gradient</span><br><span class="line">  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)</span><br><span class="line">  smooth_loss = smooth_loss * 0.999 + loss * 0.001</span><br><span class="line">  if n % 100 == 0: print &apos;iter %d, loss: %f&apos; % (n, smooth_loss) # print progress</span><br></pre></td></tr></table></figure><p>现在我们来说一下 loss function（损失函数），损失函数接受输入的目标，它也接受H prep，H prep 的缺点就是他的形状向量来自于前一个数据块，所以我们要分批进行25个字符的区块，并且我们要跟踪在 25 个字符结尾的是什么情景，以至于在向后传播相遇时，我们可以看到 H 最初的形态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># perform parameter update with Adagrad</span><br><span class="line">  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], </span><br><span class="line">                                [dWxh, dWhh, dWhy, dbh, dby], </span><br><span class="line">                                [mWxh, mWhh, mWhy, mbh, mby]):</span><br><span class="line">    mem += dparam * dparam</span><br><span class="line">    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update</span><br></pre></td></tr></table></figure><p>因此我们确保隐藏层通过这个方式在区块之间是基本上正确的传播，但是我们只向后传播这些 25 次，为此我们添加了损失函数和梯度，还有所有的权重矩阵和方框，可以输出损失，然后会有一个参数的更新，告诉我们要更新比较老的部分。注意：这里共有 25 个 softmax 分类器，我们对这 25 个端同时进行反向传播，最后将所求梯度加起来。</p><p>损失函数是这一块代码，它包含了向前传播和向后传播两部分方法。我们可以比较一下向前传播和向后传播。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def lossFun(inputs, targets, hprev):</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  inputs,targets are both list of integers.</span><br><span class="line">  hprev is Hx1 array of initial hidden state</span><br><span class="line">  returns the loss, gradients on model parameters, and last hidden state</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  xs, hs, ys, ps = &#123;&#125;, &#123;&#125;, &#123;&#125;, &#123;&#125;</span><br><span class="line">  hs[-1] = np.copy(hprev)</span><br><span class="line">  loss = 0</span><br><span class="line">  # forward pass</span><br><span class="line">  for t in xrange(len(inputs)):</span><br><span class="line">    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation</span><br><span class="line">    xs[t][inputs[t]] = 1</span><br><span class="line">    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state</span><br><span class="line">    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars</span><br><span class="line">    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars</span><br><span class="line">    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)</span><br></pre></td></tr></table></figure><p>在向前传播，你应该基本认识到，我们得到的亏损目标和我们被等待接受的这 25 个索引并不是我们通过他们的传递从 1 到 25，我们创建了文本的输入向量，虽然只是一些 0，并且我们设置了一个 one-hot 编码，无论他的指数是什么，我们把它集成一个编码。在计算中，循环公式用的就是这个方程。hs[t]，在这里 h 就是跟踪不同步骤中结果的量，我们使用循环公式计算隐含层的向量以及输出向量。接着使用 softmax 公式，得到归一化的概率，损失值则等于 -log（正确类的概率）。</p><p><img src="/2017/11/01/循环神经网络-RNN/16.png" alt=""></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># backward pass: compute gradients going backwards</span><br><span class="line">  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)</span><br><span class="line">  dbh, dby = np.zeros_like(bh), np.zeros_like(by)</span><br><span class="line">  dhnext = np.zeros_like(hs[0])</span><br><span class="line">  for t in reversed(xrange(len(inputs))):</span><br><span class="line">    dy = np.copy(ps[t])</span><br><span class="line">    dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here</span><br><span class="line">    dWhy += np.dot(dy, hs[t].T)</span><br><span class="line">    dby += dy</span><br><span class="line">    dh = np.dot(Why.T, dy) + dhnext # backprop into h</span><br><span class="line">    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity</span><br><span class="line">    dbh += dhraw</span><br><span class="line">    dWxh += np.dot(dhraw, xs[t].T)</span><br><span class="line">    dWhh += np.dot(dhraw, hs[t-1].T)</span><br><span class="line">    dhnext = np.dot(Whh.T, dhraw)</span><br><span class="line">  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:</span><br><span class="line">    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients</span><br><span class="line">  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]</span><br></pre></td></tr></table></figure><p>在向后传播，我们从第 25 层穿过隐藏层，直到第一层，你或许已经注意到，这里我并不知道我需要处理多少细节，但是这里反向通过了一个 softmax 函数，反向通过了激活函数，我对所有的梯度和参数进行加和。值得一提的是，梯度是与权值同尺寸的的矩阵，在代码中使用了 “+=”，因为在反向传播过程中权值矩阵会求得多个梯度，我们需要将这些梯度叠加起来，因为我们向前的每一步都用到了权值矩阵，所以在反向求导是也要不断的叠加梯度。这样我们就求出了梯度，现在就可以利用损失函数对初值进行更新了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">def sample(h, seed_ix, n):</span><br><span class="line">  &quot;&quot;&quot; </span><br><span class="line">  sample a sequence of integers from the model </span><br><span class="line">  h is memory state, seed_ix is seed letter for first time step</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  x = np.zeros((vocab_size, 1))</span><br><span class="line">  x[seed_ix] = 1</span><br><span class="line">  ixes = []</span><br><span class="line">  for t in xrange(n):</span><br><span class="line">    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)</span><br><span class="line">    y = np.dot(Why, h) + by</span><br><span class="line">    p = np.exp(y) / np.sum(np.exp(y))</span><br><span class="line">    ix = np.random.choice(range(vocab_size), p=p.ravel())</span><br><span class="line">    x = np.zeros((vocab_size, 1))</span><br><span class="line">    x[ix] = 1</span><br><span class="line">    ixes.append(ix)</span><br><span class="line">  return ixes</span><br></pre></td></tr></table></figure><p>最后，在这里我们有个采样方法。我们使用这个方法，基于我们之前训练出来的模型（如字符串的衔接）来生成新的文本。我们随机获得一些字符，并用训练好的模型对这个字符进行扩展，我们使用循环公式，获得字符的概率分布，然后从中取样，取出最有可能出现的字符。然后我们开始取下一个字符，依次迭代，直到我们获得足够长的文本。</p><p>共有 25 个 softmax 分类器，我们对这 25 个端同时进行反向传播，然后将所求梯度加起来</p><p><a href="https://gist.github.com/karpathy/d4dee566867f8291f086" target="_blank" rel="noopener">源码地址</a></p><h2 id="训练成果"><a href="#训练成果" class="headerlink" title="训练成果"></a>训练成果</h2><p>下面是 Andrej Karpathy 和 Justin Johnson 做的一些训练。</p><p>他们用 Char-RNN 去学习一些文本，RNN 去读这些文本、小段代码，我们注意到某些特定的单元以及 RNN 隐藏层的状态，我们用颜色来标注这些单元，来表示这些单元是否“兴奋”。可以看出，有很多隐藏层的状态很难去理解，他们时而兴奋时而沉默，显得很奇怪。这是因为他们关注的是字符级的变化，例如在 ah 之后接 e 的情况有多少等。</p><p><img src="/2017/11/01/循环神经网络-RNN/14.png" alt=""></p><p>但是有些单元表达的信息是可以理解的。对于像引号检测这样的单元而言，当前一个引号出现，它们即处于“开启状态”，一直到后一个引号出现。跟踪这样的单元比较可靠，这是从反向传播中得到的信息。很明显可以看出，对于这样的一个字符级模型，引导内外信号强度的差别很大，这是值得学习的有用特征，于是 RNN 用一部分隐藏层来对引号进行跟踪，以分辨目前是在引号中还是引号外。</p><p><img src="/2017/11/01/循环神经网络-RNN/15.png" alt=""></p><p>注意，这里的 RNN 使用了包含 100 个字符的序列进行学习，即我们只在 100 层上进行反向传播，这 100 层才是这个单元的学习区间，因为他不知道长于 100 字符的情况，但是这里的引号之间的长度明显大于 100，这个情况表明你可以对小于 100 长度的数据进行训练，然后将情况合理的推广到更长的序列，所以对于长度长于 100 字符的序列，模型依然有效</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><ul><li>在 RNN 中为什么不使用正则化？</li></ul><p>因为在 RNN 中使用正则化并不常见，甚至有时候正则化反而会得出更差的结果，所以我有时候不考虑他，他属于一种超参数</p><p>===</p><ul><li>我们是否要去学习这些输入的单词本身含义？</li></ul><p>对于这 25 个连续的单词，我们并不关心一个词是否存在，我们关心的是字符所处的位置，这个模型中我们不需要了解字符，也不需要了解语言，模型所学习的是字符的序列</p><p>===</p><p>经过训练后的 RNN 很少会犯语法错误，比如括号的匹配一类的，但是所生成的文章的意思会随着训练的进行而逐渐明朗。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;循环神经网络，英文名是Recurrent Neural Network，随着对它的不断深入了解，你会发现这个神经网络模型是多么的有趣。你可以喂给它莎士比亚的作品，经过有效的训练后它会给你输出带有莎士比亚风格的句子；你喂给它 Linux 源码，它会装模作样的给你生成一段它自己写的代码，尽管会有语意错误，但是不会有语法错误。&lt;/p&gt;
&lt;p&gt;那么废话不多说，直接进入正文吧。&lt;/p&gt;
&lt;h2 id=&quot;Recurrent-Neural-Network&quot;&gt;&lt;a href=&quot;#Recurrent-Neural-Network&quot; class=&quot;headerlink&quot; title=&quot;Recurrent Neural Network&quot;&gt;&lt;/a&gt;Recurrent Neural Network&lt;/h2&gt;&lt;pre&gt;&lt;code&gt;循环神经网络的好处就是他们会在你建立神经网络架构时给予你很高的灵活性
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;咱们先来看一下最左边这个例子，所以一般你在处理神经网络的时候，你会得到一个固定大小的向量（即图中的红色框），然后用隐藏层 – 这个绿色的框来处理它，你就会得到一个固定大小的向量，就是蓝色。所以会有一个固定大小的图像进入网络，然后要输出一个固定大小的向量，他是一个 class score，在循环神经网络中，我们可以采用不同的顺序实现，比如从输入开始或输出开始，或者两者同时开始。&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Neural Networks" scheme="https://saberda.github.io/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络之定位检测</title>
    <link href="https://saberda.github.io/2017/10/31/CNN%E4%B9%8B%E5%AE%9A%E4%BD%8D%E6%A3%80%E6%B5%8B/"/>
    <id>https://saberda.github.io/2017/10/31/CNN之定位检测/</id>
    <published>2017-10-30T16:41:05.000Z</published>
    <updated>2017-10-30T17:38:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>简单的来复习一下卷积神经网络，我们分解了那些层次并花费了大量的时间来理解卷积算子的工作原理，并且学习如何把一个特征图像转换到另一个，这是通过滑动特征图上的窗口来计算内积，然后把这些表现通过许多层的处理来转化。如果你还记得这些较低的学习边界和颜色的卷积层，而高层则学习更加复杂的物体部分。我们还讲到了池化，池化用于抽样，并且缩小网路内的特征表现，这是我们看到的一个很普通的层级结构。我们还对特定的网络架构进行分析，这样你就能看到这些事物是如何在实践中被连接到了一起。</p><p>再简单复习一下经典模型，LeNet，他是一个很小的5层网络（98年）用于数字识别。AlexNet，是他拉开了深度学习的火爆序幕，之后是ZFNet，是一个图像分类网络。我们现在明白在分类中，更深一般会更好，例如表现良好的VGG和GoogLeNet。接下来我们讲到的是ResNet的新的神奇的网络，它深达152层，没有强大GPU的还是不要去轻易尝试使用这个。正如你们所见，这些网络都在变得更深，也随之表现的更好。</p><a id="more"></a><p><img src="/2017/10/31/CNN之定位检测/1.png" alt=""></p><h2 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h2><p>但是这些都仅仅为了分类，所以我现在介绍下定位，这是在计算机视觉另一个很大很重要的问题。正如我刚才所说的，网络越深结果可能越好这个结论，在这两个任务中一样成立，所以到目前为止我们一直在讲分类。其实就是对于一张图片，我们要对不同数量上的物体类别进行分类。这是一个在计算机视觉中很好也很基础的问题，我们用它来理解CNN。</p><p><img src="/2017/10/31/CNN之定位检测/2.png" alt=""></p><p>但人们在研究过程中也遇到了许多其他的问题。这其中就有分类和定位。现在，除了仅仅把图像和类标签进行分类，我们还希望把框框放在图像中，表明这就是分类发生的区域。还有一个人们正在解决的问题就是检测。所以这里还是一些对象类别的数量，但实际上我们希望选中在图像内所有该类别的目标以及它们周围的选框。在一个最近人们才开始着手研究的项目中有一个令人着魔的东西，他叫做实时图像分割，其原理就是你有一定数量的类别，你想在图像中找到所有类别对应的目标，但是相比于用框框选，实际上更希望圈出围绕该目标的轮廓，并识别所有的属于每一个目标的像素。图像分割是有一些疯狂，所以我在这里不去讨论他，但是你们也要了解他。</p><p>我们今天则要着重关注定位和检测的内容。这两者的区别就是我们所找到的目标的数量。在定位中，一般是有一个或一种对象；但在检测中，会有很多个或多种对象。这看起来是个很小的区别，但是他对最终的架构起很重要的作用。所以我们会先讲分类和定位。因为这两个是最简单的。</p><p>概括一下我刚才所讲的，分类是一张图片中只有一种给定类别标签的对象，定位则是在图像中给对象圈上框框，而分类定位就是指我们要同时符合这两者的要求。</p><p><img src="/2017/10/31/CNN之定位检测/3.png" alt=""></p><p>介绍一下人们目前用此达成的任务会给你们一些启发。我们讲到了ImageNet分类的挑战，ImageNet还有分类+定位的挑战。相似与分类，这里有1000个类，在这些类中，每一个训练目标都有一个类和许多的在图像内部对应该分类的位置选框。那么现在有一个用于测试的更小的算法，这个算法会得出五个猜想，与其说这些猜想仅是类标签，不如说他们是与位置框框在一起的类标签。为了得到正确的猜想，你要得到正确的类标签和正确的位置框框。如果得到了正确位置，那就代表你的结果很接近分割的准确率（loU： intersection of union）了，不过你现在不必去考虑loU的事情。</p><p>所以重复一遍就是，只要你五个猜想中的一个是正确的，你就会得到正确的imageNet。所以这就是人们一直研究的分类+定位的最主要数据集。</p><h2 id="回归的使用"><a href="#回归的使用" class="headerlink" title="回归的使用"></a>回归的使用</h2><p>当研究定位时，有一个特别有用的很基础的范式：回归。我不知道在回想机器学习的课程当中，你能不能想起分类算法，比如SVM，还有回归算法；当我们讨论定位的时候，我们可以把它视作回归问题。</p><p>比如说我们有一张图片，这张图片要经过一系列的处理过程，并最终生成四个代表框框大小的实数。有许多不同的参数来描述框框，人们常用的是用XY坐标定位框框的左上角、宽度和高度，不过也有其他的变量，但对于框框来说这些变量都只能是四个数。还有一些真实准确（ground truth）的选框，也是用四个变量描述的。</p><p><img src="/2017/10/31/CNN之定位检测/4.png" alt=""></p><p>我们还可以计算损失，比如欧式损失，欧式损失是一个很标准的选择，是我们所生成的数字与正确的数字之间的差额。那么现在我们就可以同样训练他就像我们训练分类网络那样：我们用ground truth边框对许多批数据进行抽样，并向前传播，我们所做的预测与正确的预测只差所带来的计算机损失则往回传播，一次为方法来升级网络。</p><p>这个范式很简单，也使得定位的实行也变得简单。这里有一个简单的秘诀，这个秘诀会告诉你怎样来实时分类+定位。首先你要下载现有的前训练模型。如果你对自己的技术感到自行的话，你可以自己进行训练，训练模型可以使AlexNet，VGG，GoogLeNet等。那么我们就可以得到训练后生成class scores的全连接层.</p><p><img src="/2017/10/31/CNN之定位检测/5.png" alt=""></p><p>但我们先暂且不去管他，现在我们在这个网络里再接上一些新的FC。我们把这称之为回归网络（regression head）。但我认为这实际上跟FC是一个意思，只不过输出一些实数。</p><p><img src="/2017/10/31/CNN之定位检测/6.png" alt=""></p><p>现在我们训练它就像训练我们自己的分类网络，唯一区别就是我们把class cores和classes替换成L2 loss和ground truth框框。除了以上的区别，我们使用训练原来网络的方法来同样训练这个新的网络。当测试的时候，我们就用这斜来实现分类和定位。</p><p><img src="/2017/10/31/CNN之定位检测/7.png" alt=""></p><p>我们找了一些照片，并且训练了分类网络和定位网络，我们将图片进行处理就能得到class scores和框框，就这样任务完成了，按照步骤做就能在你们的项目中进行分类和定位。</p><p>在这个方法中还有个细节要注意，在进行回归时一共有两种主要方式，<strong>不定类回归</strong>（class-agnostic regression）和<strong>特定类回归</strong>（class-specific regression）。</p><p><img src="/2017/10/31/CNN之定位检测/8.png" alt=""></p><p>不定位回归是无论我使用什么类别，全连接层中都使用相同的结构和权值来得到边界昂狂框（bounding box），得到的结果总是4个数字，无论是什么类别他们表示的都是那个框。另外一种选择是特定类回归，输出的结果是C承上4个数字，相当于每种类别有一个边界框，很多人发现这种方法在很多情况下效果会更好。直观地说这种方法确实是有意义的，试着思考一下，对一只猫猫确定其边界和对火车确定边界总是有一些不同的，所以你需要在网络中有不同的处理来应对这个问题。他稍微改变了计算损失的方式，不仅仅是使用ground truth class来计算损失，除此之外在其他方面这两种基本属于同一思想。</p><p><strong>另一个需要选择的地方是在网络的那个位置进行回归</strong>。这也不是很重要，不同的人有不同的方式。这个比较常见的选择是将回归层（regression head）放在最后一次卷积层后，这就像是你对全连接层重新做了初始化，像Overfeat、VGG网络就是这种方式。另一种选择是将回归层放在FC之后，像deepPose和RCNN就是这种方式工作的。这两种方式效果都不错，你可以按照自己的喜好去设置。</p><p><img src="/2017/10/31/CNN之定位检测/9.png" alt=""></p><p>顺便提一下，我们可以用这个框架对不止一种物体来划定其边界框。通过这个分类和定位，我们输入一张图片，然后关注他所产生的这个图片的边界框。但是在一些情况下，你提前知道要对固定数量的物体进行划分边界框，在这里总结起来很简单，回归层输出了每个物体对应的边界框，然后你用同样的方式对网络再次进行训练。</p><p>同时对多个物体进行定位是非常通用而且功能强大的。例如，这项技术在人类姿势判定中得到应用。当你输入一个人的特写镜头时，人们想知道这个人的姿势是什么。人体都有着固定数量的身体关节，像手腕。我们需要去找到所有的关节，所以当我们输入这张图片时通过一个卷积网络，然后我们能够在XY轴上找到每个关节的位置，从而能够让我们对这个人的姿势进行预测。</p><p><img src="/2017/10/31/CNN之定位检测/10.png" alt=""></p><p>总的来说，定位的思想和通过回归定位固定数量的物体是非常简单的。我知道有时你想在项目中进行检测，而不是定位和分类，如果你们向更深入的理解图像，并在进行项目时能够考虑到这些方面的话，我希望你们能够考虑一下这个定位的框架：如果在每个图像中都有固定数量的物体需要定位的话，你可以将它看成是定位问题，这样会使问题更简单一点。</p><p>所以这个通过回归定位的方法确实很简单，他确实有用，但是如果你想在imageNet这样的比赛占有一席之地的话，你需要一些新的东西。所以人们定位是考虑到的方法还包括了滑动窗口（sliding window）。在这个方法中你依然要在网络中进行分类和定位的操作，但是你不只是运行了一次，而是在不同的位置多次进行，再讲不同位置进行聚合，事实上有一种高效的方法来做这个事情。</p><h2 id="Overfeat"><a href="#Overfeat" class="headerlink" title="Overfeat"></a>Overfeat</h2><p>为了更具体的了解滑动窗定位方法的工作过程，我们先了解一下Overfeat的结构。</p><p><img src="/2017/10/31/CNN之定位检测/11.png" alt=""></p><p>Overfeat是在imageNet定位挑战赛中的优胜者，这个结构看上去和我们之前讲过的很相似，开始是一个AlexNet，然后是分类层和回归层。分类层输出产生类的分数，因为这是一个ALexNet类型的结构，期望的输入图片大小是 221 * 221，但是实际上我们可以输入更大的图片，此时滑动窗的方法能够起到作用。</p><p>比如说 257 * 257 的图片，然后在图片左上方进行分类和定位，从而得到类的分数和相应的边界框。重复这个操作，使用相同的分类和定位网络，在这个图片四个角落都运行了一次，最终我们会得到四个边界框，对于每个位置都有一个边界框和类的分数。但是事实上我们只需要一个边界框，所以使用了一些方法对这些边界框和分数进行了合并，这部分很复杂，我目前也不是很懂。反正通过组合和聚集不同位置的边界框可以帮助这个模型进行错误修正，他的工作效果很好。</p><p>（当你进行回归时，输出的是（表示边界框的）四个数字，这个数字理论上可能出现在任何地方，他甚至不一定在图片内部，当你再用滑动窗方法进行训练的时候，你对不同位置进行操作时坐标轴也会进行一些改变，这也是需要注意的一个地方）</p><p>但是在实际操作中要使用远多于四个的边界框，使用的输入图片的大小也有多种，你可以看到这是从论文中摘取的图像，在左边的图像中，你可以看到在不同的位置对这个网络进行评估，在中间的图像中你可以看到的是对每个位置输出响应的边界框，在图像底部你可以看见是对于这些位置分数的映射。这看上去有点乱，但是采用这个聚合方法，他们最终能得到那个熊的最终边界框，然后确定这个图片是一只熊。</p><p><img src="/2017/10/31/CNN之定位检测/12.png" alt=""></p><h2 id="全连接层与卷积层的转换"><a href="#全连接层与卷积层的转换" class="headerlink" title="全连接层与卷积层的转换"></a>全连接层与卷积层的转换</h2><p>一个你能够预料的问题是对每个类别都运行这个网络的花费是非常昂贵的，事实上有个高效的方法。我们通常将网络看成由卷积网络和链接网络构成的，但是换一种角度看，一个全连接网络是有4096个数字构成是一个向量，如果不把他看成一个向量，而是将它看成另一个卷积的特征映射。（下图为原结构）</p><p><img src="/2017/10/31/CNN之定位检测/13.png" alt=""></p><p>我们将它进行转换成 1 <em> 1 维的，所以这个方法是将全连接层转换成卷积层，我们得到了这个卷积特征映射，通过这个特征映射我们产生4096维向量的每一个元素，我们考虑通过一个 5 </em> 5 的卷积层，而不是对特征映射进行直接的变换。这看上去有一些怪异，但是思考以后你会发现这很有意义。所以我们将这个全连接层进行一个 5 <em> 5 的卷积运算，然后我们通过另一个全连接层，从4096维到4096维，这是一个 1 </em> 1 的卷积运算。如果你认真思考用数学公式演算，你就能理解了。（下图为转换后结构）</p><p><img src="/2017/10/31/CNN之定位检测/14.png" alt=""></p><p>现在我们要做的是使用卷积层来组合原来的网络的全连接层，这样效果很好因为我们的网络就完全只由卷积层与池化层和元素操作了。我们实际上可使用不同尺寸的图片来运行网络，这种方法可以通过一个非常节约计算资源的方法来给我们一个等价的、与位置无关的结果。</p><p>下面看看他的原理。</p><p>在训练环节你用的是一个 14 <em> 14 的输入图片进入卷积层，然后是这个全连接层，我们现在把它视为卷积层，在这里我们有个 5 </em> 5 的卷积核，通过这个卷积核把之前的结果变成一个 1 <em> 1 的元素。在这里我们没有展现深度，这里的 1 </em> 1 其实是 1 <em> 1 </em> 4096，这样我们就把这个层转化成了额卷积层。</p><p>下面这一行，我们的卷积层运行在了一张更大尺寸的图片上，你们可以看到我们加了一些额外的像素，然后我们在运行一次同样的卷积池化过程，最后得到了一个2*2的输出。这样最大的好处就是我们现在可以在不同大小的图片上使用相同的计算过程，这样能够大大提高我们的效率。</p><p><img src="/2017/10/31/CNN之定位检测/15.png" alt=""></p><p>现在我的输出是原来的四倍大，但是我们计算消耗的时间远远小于四倍，因为如果你考虑两次计算中不同的地方，其实额外的计算量只发生在黄色的这一部分，所以现在我们十分高效的计算了神经网络在不同位置上的结果，在这同时并没有带来特别大的额外计算量。</p><h2 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h2><ul><li>你可以使用同一个网络进行分类和定位，也可以使用不同的网络分别进行分类和定位</li><li>尽量不要选择没有使用反向传播的网络</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单的来复习一下卷积神经网络，我们分解了那些层次并花费了大量的时间来理解卷积算子的工作原理，并且学习如何把一个特征图像转换到另一个，这是通过滑动特征图上的窗口来计算内积，然后把这些表现通过许多层的处理来转化。如果你还记得这些较低的学习边界和颜色的卷积层，而高层则学习更加复杂的物体部分。我们还讲到了池化，池化用于抽样，并且缩小网路内的特征表现，这是我们看到的一个很普通的层级结构。我们还对特定的网络架构进行分析，这样你就能看到这些事物是如何在实践中被连接到了一起。&lt;/p&gt;
&lt;p&gt;再简单复习一下经典模型，LeNet，他是一个很小的5层网络（98年）用于数字识别。AlexNet，是他拉开了深度学习的火爆序幕，之后是ZFNet，是一个图像分类网络。我们现在明白在分类中，更深一般会更好，例如表现良好的VGG和GoogLeNet。接下来我们讲到的是ResNet的新的神奇的网络，它深达152层，没有强大GPU的还是不要去轻易尝试使用这个。正如你们所见，这些网络都在变得更深，也随之表现的更好。&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Neural Networks" scheme="https://saberda.github.io/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>CNN_Structure_2</title>
    <link href="https://saberda.github.io/2017/10/27/CNN-Structure-2/"/>
    <id>https://saberda.github.io/2017/10/27/CNN-Structure-2/</id>
    <published>2017-10-26T18:06:59.000Z</published>
    <updated>2017-10-26T18:12:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章还是一篇在学习中整理的笔记，不想做优化，就是这么任性</p><h2 id="CNN如何做到的可视化？"><a href="#CNN如何做到的可视化？" class="headerlink" title="CNN如何做到的可视化？"></a>CNN如何做到的可视化？</h2><p>在第一层，将权重可视化，因为他们直接和图像相关。那我们如何将后面的卷积层可视化？<strong>这些可视化方法都是对神经元的反应做出的可视化，而不是对滤波器本身</strong>，实际上目前并没有好的方法对滤波器本身做可视化，所以在<a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html" target="_blank" rel="noopener">ConventnetJS</a>中你可以找到权重，但是并不能直接对他们做出很好的解释。因为权重自己没有很好的解释意义</p><h2 id="Pooling-Layer-的真正目的以及这么处理的原因"><a href="#Pooling-Layer-的真正目的以及这么处理的原因" class="headerlink" title="Pooling Layer 的真正目的以及这么处理的原因"></a>Pooling Layer 的真正目的以及这么处理的原因</h2><a id="more"></a><p>当你做池化时，实际做的事情是扔掉一些空间上的信息，所以在进行池化时，我知道某个2*2的小方块的某个地方有个咖喱棒，或者是可以投影回图像中的某个地方，但是我不知道咖喱棒的具体位置，因为我扔掉了部分空间信息，我就不知道剩下的数字是来自原来的哪个位置。所以在空间上，我们隔绝了部分信息，但我们仍然知道某个地方有咖喱棒。这样使得我们在每次池化时都扔掉了一小部分信息。</p><p>这里我想说的是，虽然我们扔掉了一部分信息，但是我们的目的是最终把分类算出来，比如我们分析出这个图片中有咖喱棒，那这个图片一定是saber；即你不必知道那些东西的具体位置，所以pooling layer就扔掉了一些相关的信息。</p><h2 id="为什么要在滤波器中做点乘？"><a href="#为什么要在滤波器中做点乘？" class="headerlink" title="为什么要在滤波器中做点乘？"></a>为什么要在滤波器中做点乘？</h2><p>我们喜欢做点乘是因为我们知道怎么高效的做这个运算，而且他的输入域是局域的，我们希望共享参数，这样就不会使得权重的数量多到爆炸（所以没有一个简单的数学公式可以代替）</p><h2 id="图片数据的存储形式"><a href="#图片数据的存储形式" class="headerlink" title="图片数据的存储形式"></a>图片数据的存储形式</h2><p>数据都是三维数组的形式，但是如果你想要用批处理的方式来处理数据，那它就是四维数组，总之你处理的每一层都是数组</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章还是一篇在学习中整理的笔记，不想做优化，就是这么任性&lt;/p&gt;
&lt;h2 id=&quot;CNN如何做到的可视化？&quot;&gt;&lt;a href=&quot;#CNN如何做到的可视化？&quot; class=&quot;headerlink&quot; title=&quot;CNN如何做到的可视化？&quot;&gt;&lt;/a&gt;CNN如何做到的可视化？&lt;/h2&gt;&lt;p&gt;在第一层，将权重可视化，因为他们直接和图像相关。那我们如何将后面的卷积层可视化？&lt;strong&gt;这些可视化方法都是对神经元的反应做出的可视化，而不是对滤波器本身&lt;/strong&gt;，实际上目前并没有好的方法对滤波器本身做可视化，所以在&lt;a href=&quot;http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ConventnetJS&lt;/a&gt;中你可以找到权重，但是并不能直接对他们做出很好的解释。因为权重自己没有很好的解释意义&lt;/p&gt;
&lt;h2 id=&quot;Pooling-Layer-的真正目的以及这么处理的原因&quot;&gt;&lt;a href=&quot;#Pooling-Layer-的真正目的以及这么处理的原因&quot; class=&quot;headerlink&quot; title=&quot;Pooling Layer 的真正目的以及这么处理的原因&quot;&gt;&lt;/a&gt;Pooling Layer 的真正目的以及这么处理的原因&lt;/h2&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Neural Networks" scheme="https://saberda.github.io/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>CNN Structure -- 1</title>
    <link href="https://saberda.github.io/2017/10/25/CNN-Structure/"/>
    <id>https://saberda.github.io/2017/10/25/CNN-Structure/</id>
    <published>2017-10-24T17:24:03.000Z</published>
    <updated>2017-10-24T18:03:52.000Z</updated>
    
    <content type="html"><![CDATA[<p>一个简单的加强记忆的笔记，没做优化</p><h2 id="ConV层："><a href="#ConV层：" class="headerlink" title="ConV层："></a>ConV层：</h2><ul><li><p>Output size: </p><p>  <strong>(N - F) / stride + 1</strong></p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">N: 图像像素</span><br><span class="line">F: 滤波器像素</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>pad</li></ul><p>填补(pad)，在图像周围加上一圈（或多圈）像素0 -&gt; 卷积层中的超参数</p><p><u>填补的用处</u>：加上一圈（或多圈）再做卷积，输出的尺寸会和输入的尺寸一样大</p><p>eg：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">已知条件：</span><br><span class="line">输入尺寸：32 * 32 * 3</span><br><span class="line">用 10 层 5*5 的滤波器</span><br><span class="line">stride是 1，pad 是 2</span><br><span class="line"></span><br><span class="line">可以推出：</span><br><span class="line">输出尺寸：</span><br><span class="line">(32 + 2 * 2 - 5) / 1 + 1 = 32</span><br><span class="line">so: 32 * 32 * 10</span><br><span class="line">此网络的参数个数：</span><br><span class="line">每层有 5 * 5 * 3 -&gt; 75 + 1 -&gt; 76 个（ +1 for bias）</span><br><span class="line">so: 共 76 * 10 -&gt; 760 个</span><br></pre></td></tr></table></figure><h2 id="ConV-Layer-Summary："><a href="#ConV-Layer-Summary：" class="headerlink" title="ConV Layer Summary："></a>ConV Layer Summary：</h2><ul><li>每次接收尺寸：W<sub>1</sub> · H<sub>1</sub> · D<sub>1</sub></li><li>每次输出尺寸：W<sub>2</sub> · H<sub>2</sub> · D<sub>2</sub></li><li>卷积层的个数：K</li><li>滤波器的长度：F</li><li>步长：S</li><li>pad的长度：P</li><li>W<sub>2</sub> = (W<sub>1</sub> - F + 2 · P) / S + 1</li><li>H<sub>2</sub> = (H<sub>1</sub> - F + 2 · P) / S + 1</li><li>D<sub>2</sub> = K</li><li>每个滤波器权重的数量：F · F · D</li><li>该卷积层权重的数量：F · F · D · K + K</li></ul><p>说明：</p><ol><li>其中 K 总是 2 的指数；</li><li>K, F, S, P 是该卷积的四个超参数</li><li>一般地，我们假设 W<sub>2</sub> = H<sub>2</sub></li><li>为什么 pad 一般选择填充的像素是 0 ：滤波器只考虑了输入的数据，因为做点乘的时候，0 不会对输出有影响</li><li>1 * 1 的滤波器做卷积看似没有意义，实则不是（因为练习中卷积一般是在二维层次上的，但是数据是三维的）。有时我们称这种滤波器为“线缆”或者“有深度的列”</li></ol><h2 id="Pooling"><a href="#Pooling" class="headerlink" title="Pooling"></a>Pooling</h2><p>在做卷积运算时，我们不会在空间上缩小数据的尺寸，而是会保持空间的尺寸。空间尺寸的减小通常是在下采样层（Pooling Layer）进行的。</p><p>下采样层就是把输入数据拿过来，然后通过下采样达到空间上压缩的目的。这个下采样在每个激活映射中独立的进行。在深度方向的每一层的激活映射本质上都是下采样之后再放回。</p><p>常见的两种下采样方式：</p><ul><li>Max Pooling : 取每个分区中的最大数字</li><li>Average Pooling : 取每个分区中的平均数字</li></ul><h2 id="Pooling-Layer-Summary："><a href="#Pooling-Layer-Summary：" class="headerlink" title="Pooling Layer Summary："></a>Pooling Layer Summary：</h2><ul><li>接收数据尺寸：W<sub>1</sub> · H<sub>1</sub> · D<sub>1</sub></li><li>输出数据尺寸：W<sub>2</sub> · H<sub>2</sub> · D<sub>2</sub></li><li>滤波器的尺寸：F （即下采样取样的范围）</li><li>步长：S</li><li>W<sub>2</sub> = (W<sub>1</sub> - F) / S + 1</li><li>H<sub>2</sub> = (H<sub>1</sub> - F) / S + 1</li><li>D<sub>2</sub> = D<sub>1</sub></li></ul><p>说明：</p><p>F, S 是它的超参数</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;一个简单的加强记忆的笔记，没做优化&lt;/p&gt;
&lt;h2 id=&quot;ConV层：&quot;&gt;&lt;a href=&quot;#ConV层：&quot; class=&quot;headerlink&quot; title=&quot;ConV层：&quot;&gt;&lt;/a&gt;ConV层：&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Output size: &lt;/p&gt;
&lt;p&gt;  &lt;strong&gt;(N - F) / stride + 1&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;N: 图像像素&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;F: 滤波器像素&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Neural Networks" scheme="https://saberda.github.io/tags/Neural-Networks/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce VS RDBMs</title>
    <link href="https://saberda.github.io/2017/10/16/MapReduce-RDBMs/"/>
    <id>https://saberda.github.io/2017/10/16/MapReduce-RDBMs/</id>
    <published>2017-10-15T17:58:35.000Z</published>
    <updated>2017-10-15T18:24:32.000Z</updated>
    
    <content type="html"><![CDATA[<p>Now, <strong>the RDBMs – Rational Database Management Systems</strong>, is good for updating a small portion of a big database. RDBMs uses a traditional <strong>B-Tree</strong>, which is highly dependent on the time required to perform seek operations. </p><p>Compared to this, MapReduce is good for updating all, or a majority of a big database. MapReduce uses <strong>sort and merge</strong> to rebuild the database which depends more on transfer operations. </p><p>RDBMs is good for applications that required the data sets if the database to be very frequently updated such as in point queries or small dataset updates. MapReduce is better for <strong>WORM, which is a write once and read many times</strong> based data applications. </p><a id="more"></a><pre><code>Now, MapReduce is a complementary system to the traditional RDBMs system.</code></pre><p>Here are some of the characteristics compared. </p><p><img src="/2017/10/16/MapReduce-RDBMs/1.png" alt=""></p><h2 id="Data-types"><a href="#Data-types" class="headerlink" title="Data types"></a>Data types</h2><p>In order to understand the data types, we need to understand what types of data are actually existing and what their characteristics are. </p><ul><li><p><strong>Structured data</strong> is that has a formal defined structure, such as XML documents or database tables. Semi-structure is data that has looser format, where the data structure is used as a guide, and may be ignored. </p></li><li><p><strong>Unstructured data</strong> is data that dose not have and formal structure, plain text, or image data. These are the things that we may use in our social network, sending images and also text massages, or what we will use in Google(Baidu) to search out various things.</p></li></ul><p><u>MapReduce is very effective on unstructured and semi-structured data</u>. MapReduce interprets that data during the data processing session. It dose not use intrinsic properties of data as input keys or input values. The parameters used are selected by the person analyzing the data. And MapReduce has a programming model that is linearly scalable. </p><p>MapReduce uses two very important functions. One is the <strong>map function</strong>, the other is the <strong>reduce function</strong>. And both of these functions define a key-value pair mapping relationship such like key-value_1 -&gt; key-value_2.</p><p>Here are some of the Hadoop release series characteristics. </p><p><img src="/2017/10/16/MapReduce-RDBMs/2.png" alt=""></p><p>In 1.X, you can have secure authentication, old configuration names, and also old MapReduce APIs. In addition, MapReduce 1 runtime, the classic version is included. In the 0.22 version, it includes new configuration names, and the old MapReduce APIs. And the new MapReduce APIs are also included.</p><p>And it includes MapReduce 1 runtime, the classic version. In the 2.X version, which are the more recent ones, secure authentication, new configuration names, old MapReduce APIs, and new MapReduce APIs, they are all included. Some of the more advanced characteristics include MapReduce 2 runtime <strong>YARN, HDFS</strong> federation, and HDFS high-availability. The Hadoop release series 2.X, includes several major new features, </p><p>MapReduce 2 is the new MapReduce runtime implemented on a new system called YARN. <strong>YARN stands for yet another resource negotiator</strong>. YRAN is a great resource management system for running distributed applications. </p><p>HDFS federation partitions the HDFS namespace across multiple <strong>namenodes</strong>. It enables improved support for clusters with very large number of files. The HDFS high-availability feature uses standby namenodes for backup, and therefore the namenode is no longer a potential SPOF, which is a single point of failure.</p><h2 id="Epilogue"><a href="#Epilogue" class="headerlink" title="Epilogue"></a>Epilogue</h2><p>The next note is about the HDFS’s details, and I will turn it into passage and push the blog if I have enough time :).</p><p>And the last note is a short introduction about the Hadoop, I’m too busy now to update it into passage because I have to finish the FGO activity. </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Now, &lt;strong&gt;the RDBMs – Rational Database Management Systems&lt;/strong&gt;, is good for updating a small portion of a big database. RDBMs uses a traditional &lt;strong&gt;B-Tree&lt;/strong&gt;, which is highly dependent on the time required to perform seek operations. &lt;/p&gt;
&lt;p&gt;Compared to this, MapReduce is good for updating all, or a majority of a big database. MapReduce uses &lt;strong&gt;sort and merge&lt;/strong&gt; to rebuild the database which depends more on transfer operations. &lt;/p&gt;
&lt;p&gt;RDBMs is good for applications that required the data sets if the database to be very frequently updated such as in point queries or small dataset updates. MapReduce is better for &lt;strong&gt;WORM, which is a write once and read many times&lt;/strong&gt; based data applications. &lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Cloud Computing" scheme="https://saberda.github.io/tags/Cloud-Computing/"/>
    
  </entry>
  
  <entry>
    <title>Cloud Service Models</title>
    <link href="https://saberda.github.io/2017/09/28/Cloud-Service-Models/"/>
    <id>https://saberda.github.io/2017/09/28/Cloud-Service-Models/</id>
    <published>2017-09-27T17:26:20.000Z</published>
    <updated>2017-09-27T17:42:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>We are going to look into further details of the cloud service models.</p><p>First of all, look at this picture. At the bottom which is the infrastructure, the IaaS. And in here, virtual machines, server storage, network, is basically what is provides in the infrastructure. Above that is PaaS, where this is a platform as a service, and it includes database, web server and deployment tools. The next one on the top is SaaS and this is software as a Service. Here, CRM, email, games, virtual desktop and what is actually in the software application domain.</p><a id="more"></a><p><img src="/2017/09/28/Cloud-Service-Models/1.png" alt=""></p><p>As mentioned in the <a href="http://www.saberismywife.com/2017/09/18/Cloud-Introduction/" target="_blank" rel="noopener">last passage</a>, the lower service model supports the upper one, so in other words, the software as a service needs platform as a service underneath it, and platform as a service needs infrastructure as a service underneath it. These together are a solid structure service model. The cloud clients will use web browsers, mobile apps and other client access applications to log in and receive these types of cloud services.</p><h2 id="Infrastructure-as-a-service"><a href="#Infrastructure-as-a-service" class="headerlink" title="Infrastructure as a service"></a>Infrastructure as a service</h2><p>First, let’s take a look at infrastructure as a service. Infrastructure supports over the Internet and this is basically providing cloud computing and storage resources such as computing power, storage services, software packages and bundles, virtual local area networks, VLAN and also virtual machine features. This enables basically virtual machine administration, and this is providing control of computing resources through administrative access to virtual machines.</p><p>Server virtualization features are provided through this technology. Access to computing resources are enabled by administrative access to these virtual machines. A virtual machine administrative command example would save data on the cloud service, start web server, install a new application and these types of things are these types of administrative commands.</p><p>IaaS has <strong>these types of procedures</strong>. First,the software owner will create a virtual machine and upload it into the storage and the storage area network, the SAN.</p><p>Now, once it’s in the cloud servers, a virtual switch, a VSwitch can be used to pull the requested apps and OS and the virtual machine unit. From the VSwitch, the virtual machine is used by the users and their requests where the application and the OS, that operating package is delivered to the users so that they can use it. This is how infrastructure as a service works.</p><p><img src="/2017/09/28/Cloud-Service-Models/2.png" alt=""></p><h2 id="The-benefits-of-IaaS"><a href="#The-benefits-of-IaaS" class="headerlink" title="The benefits of IaaS"></a>The benefits of IaaS</h2><p>Well, flexible and efficient renting of computer server hardware. Here comes a question, what are the resources that you can rent? Well, virtual machine, storage, bandwidth, IP addresses, monitoring services, firewalls for security. In addition, the rent payment basis is focused on what type of resources am I requesting for. How much of time did I rent it? Are there any service packages that provide a combination? This will determine how much money I need to pay.</p><p>Therefore, the benefits of IaaS are portability and interoperability with legacy applications. In other words, it enables a method portability based on infrastructure resources that are used through Internet connections. In addition, it enables a method to maintain interoperability with legacy applications and workload between IaaS clouds.</p><h2 id="Platform-as-a-service"><a href="#Platform-as-a-service" class="headerlink" title="Platform as a service"></a>Platform as a service</h2><p>Platform as a service, provides development and deployment tools for application development. It provides runtime environment for apps. And as you can see in this figure, on the cloud server, the developers will put the IDE, the integrated development environment, such that components like data security, backup and recovery, application hosting and scalable infrastructure are inside the cloud servers so that users can access them whenever they need to.</p><p><img src="/2017/09/28/Cloud-Service-Models/3.png" alt=""></p><h2 id="The-PaaS’s-types"><a href="#The-PaaS’s-types" class="headerlink" title="The PaaS’s types"></a>The PaaS’s types</h2><p><img src="/2017/09/28/Cloud-Service-Models/4.png" alt=""></p><p>Application delivery-only environment, stand alone development environment, add-on development facilities, and open platform as a service. PaaS types include these type of further definitions. First, application delivery-only environment, this provides on-demand scaling and application security. Stand-alone development environment, this provides an independent platform for a specific function. The open platform as a service provides open-source software to run applications for PaaS providers. The add-on development facilities enables customization to the existing SaaS platforms.</p><h2 id="The-PaaS’s-benefits"><a href="#The-PaaS’s-benefits" class="headerlink" title="The PaaS’s benefits"></a>The PaaS’s benefits</h2><p><img src="/2017/09/28/Cloud-Service-Models/5.png" alt=""></p><p>The PaaS’s benefits can be summarized into these four. Taking a closer look, which means lower administrative overhead. That means the user does not need to be involved in any administrative of the platform. Next, lower total cost of ownership. The user does not need to purchase any hardware, memory, or server, because the hardware, memory and the server, that’s basically what is already prepared in the cloud. All I need to do is log into it and received its services. The other two benefits? Scalable solutions, application resource demand based automatic resource scale control is provided through PaaS cloud services. More current system software, a cloud provider needs to maintain software upgrades and patch installations so the user does not need to. It’s all provided and everything should be upgraded and all the patches should be installed and ready for a user to come in and just use it.</p><h2 id="Software-as-a-service"><a href="#Software-as-a-service" class="headerlink" title="Software as a service"></a>Software as a service</h2><p>This provides software applications as a service to the end user. Software that is deployed on a cloud service which is accessible through the Internet. </p><h2 id="SaaS’s-characteristics"><a href="#SaaS’s-characteristics" class="headerlink" title="SaaS’s characteristics"></a>SaaS’s characteristics</h2><p>The characteristics include on-demand availability. Cloud software is available anywhere that the cloud is reachable through the Internet. as maintenance, no user software upgrade or maintenance is needed. All of this should be supported by the cloud and the cloud administrators.  </p><p>Flexible scale up or scale down which means that even though I start off with something small, if I need to grow on the overall structure of what is supported for a certain application, I can easily scale up, if I feel that I don’t need all of this, I can scale down, and that is provided because I’m using the cloud resources. In addition, centralized management and centralized data is one of the other features of SaaS.</p><p>In addition, SaaS enables a shared data model. Multiple users can share a single data model and a single database. Cost effectiveness, pay based on what you use. There is no risk in buying the wrong software. Basically you’re using, you’re renting, you are borrowing the software that’s already in the cloud. So, basically, if you buy the wrong software that’ in a package, you install it on your Pc, maybe you can feel that you did something wrong and may regard it, but actually, that’s not going to happen when you are using a SaaS cloud support. Multitenant programming solutions. Now, this is something cool, especially when you have multiple programmers trying to program something which later on, you need to mix and match and combined into one coordinated program to work on something like a big project together.</p><p>In some cases, if everybody bught their software separately, installed in on their PC and did their own program development, your programs may not combine well together because some of the versions and patches and upgrades may be different based upon the user. In this case, since everybody’s logging into the cloud and using the SaaS platform which is one unique software package that they bring to their computer as they access and program, there is no possible situation where is a version mismatch because everybody’s logging in and receiving the same version. And they do the development so basically it should combine together if you programmed everything correctly.     </p><p>In SaaS, the open SaaS application looks like this. On the cloud servers, the components such as SaaS directory web service, SaaS messaging web service, SaaS collaboration web services and SaaS content web services are all installed.</p><p><img src="/2017/09/28/Cloud-Service-Models/6.png" alt=""></p><p>In order for a user to get access to these SaaS features and functions, the user will have to use the client user interface, the client UI. And that will trigger operation through the SaaS orchestration unit which will enable the users to access these SaaS services.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;We are going to look into further details of the cloud service models.&lt;/p&gt;
&lt;p&gt;First of all, look at this picture. At the bottom which is the infrastructure, the IaaS. And in here, virtual machines, server storage, network, is basically what is provides in the infrastructure. Above that is PaaS, where this is a platform as a service, and it includes database, web server and deployment tools. The next one on the top is SaaS and this is software as a Service. Here, CRM, email, games, virtual desktop and what is actually in the software application domain.&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Cloud Computing" scheme="https://saberda.github.io/tags/Cloud-Computing/"/>
    
  </entry>
  
  <entry>
    <title>Cloud Introduction</title>
    <link href="https://saberda.github.io/2017/09/18/Cloud-Introduction/"/>
    <id>https://saberda.github.io/2017/09/18/Cloud-Introduction/</id>
    <published>2017-09-18T10:33:17.000Z</published>
    <updated>2017-09-18T11:13:13.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="At-first-what-does-cloud-computing-do"><a href="#At-first-what-does-cloud-computing-do" class="headerlink" title="At first, what does cloud computing do?"></a>At first, what does cloud computing do?</h2><p>Well. It provides online data storage. It enables configuration and accessing of online applications. Apart from that, it provides a variety of software usage. And also it provides computing platform and computing infrastructure. </p><p>Now, that may not have lightened up what the meaning of cloud computing is, so here comes an example.</p><a id="more"></a><p>Here, using Gmail on my iPhone to check my emails. This is something that very commonly I would do. Well, one day I receive an email, and it has an attachment file that’s a Microsoft PowerPoint. Well, I think about it, and on my iPhone, I don’t have Microsoft Office or PowerPoint installed. Not even that, I don’t even have Windows OS installed on it. How am I going to use my phone to look at this PowerPoint attachment file? Well Google Drive services includes Google Docs, Sheets, and Slides. And this software packages are all inside the Google cloud, the Google Gmail server. And I can use these, the Google Docs, the Google Sheets and Slides to open up the attachment file and look at in. Even though my phone dose not have the Microsoft Windows OS, it dose not have the Microsoft Office and PowerPoint installed on it. But still, because I’m use the platform inside to look at these attachment files. </p><p>That’s what cloud computing is about. You can use the resources that you may not have on your device, but because you’re connected to the cloud that has these capabilities of software, platform, and computing infrastructure. So therefore, I just need connectivity, and it will do all the things that I need it to do, and send over the image so that I can use it as if everything is equipped on my mobile device.</p><p>Now just by reading to this, you can see that for smart devices, smartphones, in addition for new technology like internet of things, where these devices may not be fully equipped with all the software platform and infrastructure hardware that is needed. Well, cloud computing can provide magical power to it. And that’s why we’re going to study about the cloud computing.</p><h2 id="What’s-a-cloud"><a href="#What’s-a-cloud" class="headerlink" title="What’s a cloud?"></a>What’s a cloud?</h2><p>Well, a cloud can provide services through a public or private network, or the internet, which is the most common connection, where the services posting system is at a remote location. Cloud can support various applications such as email, web conferencing, games, database management, CRM which stands for Customer Relationship Management, and so many more.</p><p>Cloud models include <strong>public cloud, private cloud, hybrid cloud and a community cloud</strong>.</p><p>First let’s look more into the definitions of what these cloud are. The public cloud, this enables public systems and service access. Open architecture, such as email, are provided. This cloud be less secure due to its openness.</p><p>The next one is a private cloud, and it enables service access within an organization. Now, due to its limited in-organization characteristics, it may be more private in nature and therefore more secure. </p><p>Then, there is a community cloud, where a cloud accessible by a group of organizations that are formed, and that would be a community cloud. </p><p>Then we gave a hybrid cloud, this is a combination of a public cloud and a private cloud. The private cloud part would support critical activities. The public cloud will support non-critical activities to be accessed by various users flexibly.</p><h2 id="Cloud-service-models"><a href="#Cloud-service-models" class="headerlink" title="Cloud service models"></a>Cloud service models</h2><p><img src="/2017/09/18/Cloud-Introduction/1.png" alt=""></p><p>Now, first what I need to say is that as you can see on this picture there is SaaS, underneath it there is PaaS, and underneath it there is IaaS.</p><p>And these stand for SaaS, this stands for Software as a Service. The next one, PaaS, P-A-A-S, stands for Platform as a Service. The one at the bottom is IaaS, and this stands for Infrastructure as a Service.</p><p>The lower Service model supports the management, computing power, security of the upper service model. What this means is that IaaS supports PaaS and SaaS. Next, PaaS supports SaaS. And from SaaS, well, it is supported by both PaaS and IaaS.</p><p>And Those are the combination representative clouds models. A cloud client would use a web browser, mobile app, or some type of other client access feature to access one of these, or a combination of a couple of these.</p><p>Now let’s look into some more details.</p><p>Software as a Service provides a variety of software applications as a service to the end user.</p><p>The Platform as a Service provides a program executable platform for applications and development tools. </p><p>Infrastructure as a Service provides fundamental computing and security resources for the entire cloud. Backup storage, computing power, virtual machines these are the Infrastructure as a Service main domain.</p><p>Now, if you look at other papers and other definitions about cloud computing, you will see so many other type of combinations. For example, there are many other definitions that cloud be used. And to represent them all together, we could put just an X right here, where X stands for anything, anything as a service. And you can put in X your represented letter. For example, if you want to do a Network as a Service, then you use the N of network and you can call it a NaaS., N-A-A-S. Or, if you are talking about Database as a Service then you can use the D, the first letter of the database. Well, you can put it in here as DaaS, and that would be D-A-A-S, Database as a Service. You can do that for Business as a Service. </p><p>Now, however, when you go into the details of it, and based upon that three structure model which I had IaaS, PaaS, and SaaS on top. Well you can see that Network as a Service, NaaS, this one right here, this would fall into the gigger category of IaaS, Infrastructure as a Service, because networking is a part of the infrastructure. In addition, Database as a Service, DaaS right here. Database is considered a part of the platform services, so therefore DaaS can be seen as a part of PaaS that is provided, the Platform as a Service. So therefore you can use these terms or you can go with a general larger term, which was IaaS, PaaS and SaaS.</p><h2 id="Cloud-benefits"><a href="#Cloud-benefits" class="headerlink" title="Cloud benefits"></a>Cloud benefits</h2><p><img src="/2017/09/18/Cloud-Introduction/2.png" alt=""></p><p>As you can see here, high efficiency, reliability, flexibility. In addition, applications as utilities over the Internet. You can manipulate and configure apps online. In addition, it is very cost-effective, and no software required, because all the software should be in the cloud.</p><p>Online development and deployment tools should be provided to you. In addition, on demand self service, meaning that when you need it, you go and request for this service. And when you request it, it should be provided to you on demand.</p><p>In addition, resource available on the network, this is the good part. But it does make sure that you have network connectivity, because you have to reach your cloud to receive the resources available on the cloud. So therefore, network dependent, cloud connectivity, and there you will be able to receive your resources.</p><p>Going into some of the characteristics, well, the essential characteristic is on demand self service. The common characteristics include broad networks access (广泛的网络访问), rapid elasticity (快速弹性), resource pooling (资源池), and measured services (测量服务). In addition, normally you can obtain massive scaling characteristics (大规模扩展), resilient computing support (弹性计算支持), and homogeneity (同质性). In addition, geographic distribution (地理分布), virtualization (虚拟化), service orientation (服务定位), low cost software (低成本的软件), and an advanced security (先进的安全特性) are the common characteristics.</p><p>The reading list below is recommended.</p><p><a href="https://github.com/SaberDa/EBOOK-cloud_computing_tutorial" target="_blank" rel="noopener">cloud_computing_tutorial</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;At-first-what-does-cloud-computing-do&quot;&gt;&lt;a href=&quot;#At-first-what-does-cloud-computing-do&quot; class=&quot;headerlink&quot; title=&quot;At first, what does cloud computing do?&quot;&gt;&lt;/a&gt;At first, what does cloud computing do?&lt;/h2&gt;&lt;p&gt;Well. It provides online data storage. It enables configuration and accessing of online applications. Apart from that, it provides a variety of software usage. And also it provides computing platform and computing infrastructure. &lt;/p&gt;
&lt;p&gt;Now, that may not have lightened up what the meaning of cloud computing is, so here comes an example.&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Cloud Computing" scheme="https://saberda.github.io/tags/Cloud-Computing/"/>
    
  </entry>
  
  <entry>
    <title>中断以及中断处理</title>
    <link href="https://saberda.github.io/2017/09/07/%E4%B8%AD%E6%96%AD%E4%BB%A5%E5%8F%8A%E4%B8%AD%E6%96%AD%E5%A4%84%E7%90%86/"/>
    <id>https://saberda.github.io/2017/09/07/中断以及中断处理/</id>
    <published>2017-09-07T05:34:43.000Z</published>
    <updated>2017-09-07T05:40:31.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="中断"><a href="#中断" class="headerlink" title="中断"></a>中断</h2><p>事实上所有计算机都提供了允许其他模块（I/O、存储器）中断处理器正常处理过程的机制。分类如下：</p><ul><li>程序中断：在某些条件下由指令执行的结果产生，例如算术溢出、除数为0、试图执行一条非法的机器指令以及访问到用户不允许的存储器位置。</li><li>时钟中断：由处理器内部的计时器产生，允许操作系统以一定规律执行函数</li><li>I/O中断：由I/O控制器产生，用于发信号通知一个操作的正常完成或各种错误条件</li><li>硬件故障中断：由诸如掉电或存储器奇偶错误之类的故障产生</li></ul><a id="more"></a><p>中断最初是用于提高处理器效率的一种手段。例如：大多数I/O设备比处理器慢得多，假设处理器使用正常的指令周期方案来给一台打印机传送数据，在每一次读写后，处理器暂停并保持空闲，直到打印机完成工作。暂停的时间长度可能相当于成百上千个不涉及存储器的指令周期。显然，这对于车里器的使用来说是非常浪费的。</p><p>这里给出一个实例，假设有一个 1GHz CPU 的PC机，大约每秒执行109条指令。一个典型的硬盘速度是7200转/分，这样大约旋转半周的时间是4ms，处理器比这快4百万倍。</p><p>下图显示了这种事情状态。</p><p><img src="/2017/09/07/中断以及中断处理/1.png" alt=""></p><p>用户程序在处理过程中交织着执行一系列WRITE调用。竖实线表示程序中的代码段，代码段1、2和3表示不涉及I/O的指令序列。WRITE调用要执行一个I/O程序，此I/O程序是一个系统工具程序，由它执行真正的I/O操作。此I/O程序由三部分组成：</p><ol><li>图中标记为4的指令序列用于实际的I/O操作做准备。这包括复制将要输出到特定缓冲区的数据，为设备命令准备参数</li><li>实际的I/O命令。如果不使用中断，当执行此命令时，程序必须等待I/O设备执行请求的函数（或周期额检测I/O设备的状态或轮询I/O设备）。程序可能通过简单的重复执行一个测试操作的方式进行等待，以确定I/O操作的完成</li><li>图中标记5的指令序列，用于完成操作。包括设置一个表示操作成功或失败的标记。</li></ol><p>虚线代表处理器执行的路径；也就是说，这条线显示了指令执行的顺序。当遇到第一条WRITE指令之后，用户程序被中断，I/O程序开始执行。在I/O程序执行完成后，WRITE指令之后的用户程序立即恢复执行。</p><p>由于完成I/O操作可能花费较长的时间，I/O程序需要挂起等待操作完成，因此客户程序会在WRITE永初停留相当长的一段时间。</p><h2 id="中断和指令周期"><a href="#中断和指令周期" class="headerlink" title="中断和指令周期"></a>中断和指令周期</h2><p>利用中断功能，处理器可以在I/O操作的执行中执行其他指令。考虑下图所示的控制流，和前面一样，用户程序到达系统调用WRITE处，但涉及的I/O程序仅包括准备代码和真正的I/O命令。在这些为数不多的几条指令执行后，控制返回用户程序。在这期间，外部设备忙于从计算机存储器接收数据并打印。这种I/O操作和用户程序中指令的执行是并发的。</p><p>当外部设备做好服务的准备，也就是说，当他准备好从处理器接收更多的数据时，该外部设备的I/O模块给处理器发送一个中断请求信号。这时处理器会做出响应，暂停当前程序的处理，转去处理服务于特定I/O设备的程序，这个程序称作中断处理程序（interrupt handler）。在对该设备的服务响应完成后，处理器恢复原先的执行。图中用X表示发生中断的点。注意：中断可以在主程序中的任何位置发生，而不是在一条指定的指令处。</p><p><img src="/2017/09/07/中断以及中断处理/2.png" alt=""></p><p>从用户程序的角度看，中断打断了正常执行的序列。当中断处理完成后，再恢复执行。因此，用户程序并不需要为中断添加任何特殊的代码，处理器和操作系统负责挂起用户程序，然后在同一个地方恢复执行。</p><p><img src="/2017/09/07/中断以及中断处理/3.png" alt=""></p><p>为适应中断产生的情况，在指令周期中要增加一个中断阶段，如上图所示。在中断阶段中，处理器检查是否有中断产生，即检查是否出现中断信号。如果没有中断，处理器继续执行，并在取指周期去当前程序的下一条指令；如果有中断，处理器挂起当前程序的执行，并执行一个中断处理程序。这个中断处理程序通常是操作系统的一部分，他确定中断的性质，并执行所需要的操作。例如，在前面的例子中，处理程序决定哪一个I/O模块产生中断，并转到往该I/O模块中写更多数据的程序。当中断处理程序完成后，处理器在中断点回复对用户程序的执行。</p><p><img src="/2017/09/07/中断以及中断处理/4.png" alt=""></p><p>简单解释一下上面例子，就是当主程序执行到WRITE指令时，跳转至I/O程序并执行，其中主程序被挂起；当I/O程序执行到I/O命令时，控制器调回主程序，与I/O命令并发执行；当I/O程序执行完I/O命令时，即运行到中断处理程序，控制器进行中断处理，跳回I/O程序，主程序停止并发；待I/O程序结束后，回到主程序中断出继续执行。</p><p>很显然在这个处理中有一定的开销，在中断处理程序中必须执行额外的指令以确定中断的性质，并决定采用适当的操作。然而，如果简单的等待I/O操作的完成将花费更多的时间，因此使用中断能够更有效的使用处理器。</p><p>为进一步理解在效率上的提高,更典型的情况是，特别是对比较慢的设备比如打印机来说，I/O操作比执行一些列用户指令的时间要长得多。在这种情况下，用户程序在由第一次调用产生的I/O操作完成之前，就达到了第二次WRITE调用。结果是用户程序在这一点挂起，当前面的I/O操作完成后，才能继续新的WRITE调用，也才能开始一次新的I/O操作。</p><h2 id="中断处理"><a href="#中断处理" class="headerlink" title="中断处理"></a>中断处理</h2><p>中断激活了很多事件，包括处理器硬件中的事件以及软件中的事件。下图显示了一个典型的序列，当I/O设备完成一次I/O操作时，发生下列硬件事件：</p><p><img src="/2017/09/07/中断以及中断处理/5.png" alt=""></p><ol><li>设备给处理器发出一个中断信号。</li><li>处理器在相应中断前结束当前指令的执行。</li><li>处理器对中断进行测定，确定存在未响应的中断，并提交给中断的设备发送确认信号，确认信号允许该设备取消他的中断信号。</li><li>处理器需要为把控制权转移到中断程序中去做准备。首先，需要保存从中断点恢复当前程序所需要的信息，要求最少信息包括程序状态字（PSW）和保存在程序计数器中的下一条指令要执行的指令地址，他们被压入系统控制栈中。</li><li>处理器把响应此中断的中断处理程序入口地址装入程序计数器中。可以针对每类中断有一个中断处理程序，也可以针对每个设备和每类中断各有一个中断处理程序，这取决于计算机系统结构和操作系统的设计，如果有多个中断处理程序，处理器就必须决定调用哪一个，这个信息可能已经包含在最初的信号中，否则处理器必须给发中断的设备发送请求，以获取含有所需要信息的响应。</li></ol><p>一旦完成对程序计数器的装入，处理器则继续到下一个指令周期，该指令周期也是从取值开始。由于取值是由程序计数器内容决定，因此控制被转移到中断处理程序，该程序的执行引起下列操作：</p><ol start="6"><li>在这一点，与被中断程序相关的程序计数器和PSW被保存在系统栈中，此外，还有一些其他信息被当做正在执行程序的状态的一部分。特别需要保存处理器寄存器的内容，因为中断处理程序可能会用到这些寄存器，因此所有这些值和任何其他的状态信息都需要保存。在典型情况下，中断处理程序一开始就在栈中保存所有的寄存器内容，其他必须保存的状态信息还有很多，这里不是本篇文章的重点，就不详细介绍了。下图给出了一个简单的例子。在这个例子中，用户程序在执行地址为N的存储单元中的指令之后被中断，所有寄存器的内容和下一条指令的地址（N+1），一共M个字，被压入控制栈中。栈指针被更新指向新的栈顶，程序计数器被更新指向中断服务程序的开始。</li><li>中断处理程序现在可以处理中断，其中包括检查与I/O操作相关的状态信息或其他引起中断的事件，还可能包括给I/O设备发送附加命令或者应答。</li><li>当中断处理结束后，被保存的寄存器值从栈中释放并恢复到寄存器中</li><li>最后的操作是从栈中恢复PSW和程序计数器的值，其结果是下一条要执行的指令来自被前面被中断的程序</li></ol><p>保存被中断程序的所有状态信息并在以后恢复这些信息，这是十分重要的，这是由于中断并不是程序调用的一个历程，他可以在任何时候发生，因而可以在用户程序执行过程中的任何一点上发生，他的发生是不可预测的。</p><h2 id="多个中断"><a href="#多个中断" class="headerlink" title="多个中断"></a>多个中断</h2><p>至此，我们已经讨论了发生一个中断的情况。假设一下，当正在处理一个中断时，可以发生一个或者多个中断，例如，一个程序可能从一条通信线中接收数据并打印结果。每完成一个打印操作，打印机就会产生一个中断；每当一个数据单元到达，通信线控制器也会产生一个中断。数据单元可能是一个字符，也可能是连续的一块字符串，这取决于通信规则本身。在任何情况下，都有可能在处理打印机中断过程中发生一个通信中断。</p><p>处理多个中断有两种方法。第一种是当处理一个中断时，禁止再发生中断。禁止中断的意识是处理器将对任何新的中断请求信号不予理睬。如果在这期间发生了中断，通常中断保持挂起，当处理器再次允许中断时，再有处理器检查。因此，当用户程序正在执行并且有一个中断发生是，立即禁止中断；当中断处理程序完成后，再恢复用户程序之前再允许中断，并且由处理器检查是否还有中断发生。这个方法简单，因为所有中断都严格按照顺序处理。</p><p>上诉方法的缺点是没有考虑相对优先级和时间限制的要求。例如，当来自通信线的输入到达时，可能需要快速接收，以便为更多的输入让出空间。如果在第二批输入到达时第一批还没有处理完，就有可能由于I/O设备的缓冲区装满或者溢出而丢失数据。</p><p><img src="/2017/09/07/中断以及中断处理/6.png" alt=""></p><p>第二种方法是定义中断优先级，允许高优先级的中断打断低优先级的中断处理程序的运行。如图。</p><p><img src="/2017/09/07/中断以及中断处理/7.png" alt=""></p><h2 id="多道程序设计"><a href="#多道程序设计" class="headerlink" title="多道程序设计"></a>多道程序设计</h2><p>即使使用了中断，处理器仍有可能未得到有效的利用，例如处理器在长I/O等待下的使用率，但如果完成I/O操作的时间远远大于I/O调用期间用户代码的执行时间（通常情况下），则在大部分时间处理器是空闲的。解决这个问题的方法是允许多道用户程序同时处于活动状态。</p><p>假设处理器执行两道程序。一道程序从存储器中读取数据并放入外部设备中，另一道是包括大量计算的应用程序。处理器开始执行输出程序，给外部设备发送一个写命令，接着开始执行其他应用程序。当处理器处理很多程序时，执行顺序取决于他们的优先级以及它们是否正在等待I/O。当一个程序被中断时，控制权转移给中断处理程序，一旦中断处理程序完成，控制权可能并不立即放回到这个用户程序，而可能转移到其他待运行的具有更高优先级的程序。最终，当原先被中断的用户程序变为最高的优先级是，它将被重新恢复执行。这种多道程序轮流执行的概念称作多道程序设计。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;中断&quot;&gt;&lt;a href=&quot;#中断&quot; class=&quot;headerlink&quot; title=&quot;中断&quot;&gt;&lt;/a&gt;中断&lt;/h2&gt;&lt;p&gt;事实上所有计算机都提供了允许其他模块（I/O、存储器）中断处理器正常处理过程的机制。分类如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;程序中断：在某些条件下由指令执行的结果产生，例如算术溢出、除数为0、试图执行一条非法的机器指令以及访问到用户不允许的存储器位置。&lt;/li&gt;
&lt;li&gt;时钟中断：由处理器内部的计时器产生，允许操作系统以一定规律执行函数&lt;/li&gt;
&lt;li&gt;I/O中断：由I/O控制器产生，用于发信号通知一个操作的正常完成或各种错误条件&lt;/li&gt;
&lt;li&gt;硬件故障中断：由诸如掉电或存储器奇偶错误之类的故障产生&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="编程" scheme="https://saberda.github.io/categories/%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="操作系统" scheme="https://saberda.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>MapReduce &amp; Hadoop</title>
    <link href="https://saberda.github.io/2017/07/19/MapReduce-and-Hadoop/"/>
    <id>https://saberda.github.io/2017/07/19/MapReduce-and-Hadoop/</id>
    <published>2017-07-19T01:43:12.000Z</published>
    <updated>2017-07-19T14:31:47.000Z</updated>
    
    <content type="html"><![CDATA[<p>Hadoop 是什么？Hadoop 又与 MapReduce 有什么关系？本篇文章会简单介绍两者之间的关系，算是自己在学习过程中做的笔记吧。</p><p>我们在练习过程中可能就使用几M的数据量，但是真正在实践中我们接触的数据可比这大得多，这是如果还使用一台电脑进行运算的话就显得十分笨拙，并且实际上远远超出了我们的计算能力。幸运的是一些开源的软件项目提供了处理海量数据的方案，其中一个项目就是 Hadoop，它采用JAVA语言编写，支持在大量机器上分布处理数据。</p><p>Hadoop 是 MapReduce 框架的一个开源实现。本篇笔记简单介绍 MapReduce 和 Hadoop 项目。</p><a id="more"></a><h2 id="MapReduce-分布式计算的框架"><a href="#MapReduce-分布式计算的框架" class="headerlink" title="MapReduce: 分布式计算的框架"></a>MapReduce: 分布式计算的框架</h2><blockquote><p>优点：可在短时间内完成大量工作<br>缺点：算法必须经过重写需要对系统工程有一定的理解<br>适用数据类型：数值型和标称型数据</p></blockquote><p>MapReduce 是一个软件框架，可以将单个计算作业分配给多台计算机进行执行。它假定这些作业在单机上运行需要很长时间，因此使用多台机器缩短运行时间。</p><p>MapReduce 在大量节点组成的集群上运行。它的工作流程是：单个作业被分成很多小份，输入数据也被切片分发到每个节点，各个节点只在本地数据上做运算，对应的运算代码称为 mapper，这个过程被称作 map 阶段。每个 mapper 的输出通过某种方式组合（一般会有排序），排序后的结果再被分为小份分发到各个节点进行下一步处理。第二步的初级称为 reduce 阶段，对应的运行代码称为 reducer。reducer 的输出就是程序执行的最终结果。</p><p>MapReduce 的优势在于它使得程序以并行方式进行。</p><pre><code>注意：在任何时候，每个 mapper 或者 reducer 之间都不进行通信。每个节点只处理自己的事务，并且在本地分配的数据集上进行运算。</code></pre><p>不同类型的作业可能需要不同数目的 reducer。此外，在 MapReduce 的框架中还有其他一些灵活的配置选项。MapReduce 的整个编配工作由主节点（master node）控制。这些主节点控制整个 MapReduce 的作业编配，包括每份数据存放的节点位置，以及 map，sort 和 reduce 等阶段的时序控制等。此外，主节点还要包含容错机制。一般的，每份 mapper 的输入数据会同时分配到多个节点形成多分副本，用于事物的失效处理。</p><p>一个 MapReduce 集群的简单示意图</p><p><img src="/2017/07/19/MapReduce-and-Hadoop/1.png" alt=""></p><p>上图中的每台机器都有两个处理器，可以同时处理两个 map 或者 reduce 任务。如果机器0在 map 阶段宕机，主节点将会发现这一点，并在发现该问题后将机器0移出集群，并在剩余的节点上继续作业。在一些 MapReduce 的实现中，在多个机器上都保存有数据的多个备份。同时每个节点都必须与主节点通信，表明自己工作正常。若节点失效或工作异常，主节点将重启该节点或将该节点移出可用机器池。</p><p>小小的总结一下：</p><ol><li>主节点控制 MapReduce 的作业流程</li><li>MapReduce 的作业可分为 map 任务和 reduce 任务</li><li>map 任务之间不做任何数据交流，reduce 任务也是一样</li><li>在 map 和 reduce 任务中间有一个 sort或者combine 阶段</li><li>数据被重复存放在不同的机器上，以防止某个机器失效</li><li>mapper 和 reducer 之间的数据传递形式是键值对</li></ol><p>Apache 的 Hadoop 项目是 MapReduce 框架的一个实现，下面我们讨论 Hadoop 项目，并介绍如何用 python 使用它。</p><h2 id="Hadoop流"><a href="#Hadoop流" class="headerlink" title="Hadoop流"></a>Hadoop流</h2><p>Hadoop 是一个开源的 JAVA 项目，为运行 MapReduce 提供了大量所需的功能。除了分布式计算之外，Hadoop 自带分布式文件系统。</p><p>这里只是初探 Hadoop，关于更深层的使用会在日后更新。</p><p>下面将使用 python 编写 MapReduce 代码，并在 Hadoop流 中运行。Hadoop流 很想 Linux 系统中的管道。如果用 mapper.py 调用 mapper，那么Hadoop流 可以像 Linux 命令一样执行。例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat test.txt | python mapper.py | sort | python reducer.py &gt;</span><br><span class="line">output.txt</span><br></pre></td></tr></table></figure><p>这样，类似的 Hadoop流 就可以在多台机器上分布式执行，用户可以通过 Linux 命令来测试python编写的 MapReduce 脚本</p><p>基础概念就先介绍到这里，之后系列的文章将介绍具体实现，可能会用到亚马逊提供的服务器。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Hadoop 是什么？Hadoop 又与 MapReduce 有什么关系？本篇文章会简单介绍两者之间的关系，算是自己在学习过程中做的笔记吧。&lt;/p&gt;
&lt;p&gt;我们在练习过程中可能就使用几M的数据量，但是真正在实践中我们接触的数据可比这大得多，这是如果还使用一台电脑进行运算的话就显得十分笨拙，并且实际上远远超出了我们的计算能力。幸运的是一些开源的软件项目提供了处理海量数据的方案，其中一个项目就是 Hadoop，它采用JAVA语言编写，支持在大量机器上分布处理数据。&lt;/p&gt;
&lt;p&gt;Hadoop 是 MapReduce 框架的一个开源实现。本篇笔记简单介绍 MapReduce 和 Hadoop 项目。&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Machine Learning" scheme="https://saberda.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>详细谈谈二维码生成原理与编码（一）</title>
    <link href="https://saberda.github.io/2017/05/15/%E4%BA%8C%E7%BB%B4%E7%A0%81/"/>
    <id>https://saberda.github.io/2017/05/15/二维码/</id>
    <published>2017-05-15T11:38:03.000Z</published>
    <updated>2017-05-15T16:00:05.000Z</updated>
    
    <content type="html"><![CDATA[<pre><code>本想一篇文章就解决的，后来发现需要介绍的有点多，一个晚上绝对写不完，看来又要开新坑了。</code></pre><h2 id="一、什么是二维码？"><a href="#一、什么是二维码？" class="headerlink" title="一、什么是二维码？"></a>一、什么是二维码？</h2><p>二维码有很多种，其中占据大半江山的主要有两种，一是以 PDF417 为代表的<a href="https://en.wikipedia.org/wiki/PDF417" target="_blank" rel="noopener">堆叠式条形二维码</a>，另一个是以 QR Code 为代表的<a href="https://en.wikipedia.org/wiki/QR_code" target="_blank" rel="noopener">矩阵式二维码</a>。</p><p>PDF417 是“便携式数据文件”的缩写。它最少分为三层，最多 90 层。包括左右空白区、起始符、终止符、左右层指示符号字符等。它的具体架构如下图：</p><a id="more"></a><p><img src="/2017/05/15/二维码/图一.png" alt=""></p><p>QR Code 分为编码区与功能区。编码区主要负责版本格式、数据和纠错编码；功能区主要负责位置探测图形、校正图像以及位置探测，其中位置探测图形是快速识别的关键。它的具体架构如下：</p><p><img src="/2017/05/15/二维码/图二.png" alt=""></p><p>并且 QR Code 有有多种版本，版本与版本间的主要差别在于图形的尺寸，尺寸越大所能存储的信息越多。二维码一共有40个尺寸。官方叫版本Version。Version 1是21 x 21的矩阵，Version 2是 25 x 25的矩阵，Version 3是29的尺寸，每增加一个version，就会增加4的尺寸，公式是：(V-1)*4 + 21（V是版本号） 最高Version 40，(40-1)*4+21 = 177，所以最高是177 x 177 的正方形。</p><p><img src="/2017/05/15/二维码/图三.png" alt=""></p><p>目前国内甚至世界范围内使用最为广泛的就是 QR Code，所以本篇文章具体介绍该二维码，并且下面的文章为方便大家理解，所有的二维码均指 QR Code。</p><h2 id="二、怎样处理一个二维码？"><a href="#二、怎样处理一个二维码？" class="headerlink" title="二、怎样处理一个二维码？"></a>二、怎样处理一个二维码？</h2><p>这个部分我将分为两大部分来进行具体解释。第一部分是图像处理，第二部分是具体解码。</p><p>图像处理又可以细分为三个层次：</p><ol><li>初级图像处理：图像去噪与图像锐化</li><li>中级图像处理：图像分割与边缘提取</li><li>高级图像处理：识别图像所包含的信息</li></ol><p>解码过程也主要由三部分组成，这些部分的具体操作会在下文详细介绍：</p><ol><li>二维码码字提取</li><li>纠错译码</li><li>信息译码</li></ol><p>其中纠错译码是为了求解伴随因子，即判断正确码字的个数 [正确码字 = Y<sub>ji</sub><sup>错误码字</sup>]</p><p>信息译码 = 模式指示 + 字符段 + 数据位流。该功能一是为判断编码模式与字符个数，二是按照对应的编码进行解码</p><h2 id="三、图像处理"><a href="#三、图像处理" class="headerlink" title="三、图像处理"></a>三、图像处理</h2><p>先放一张图来大致表示一下图像处理的主要步骤</p><p><img src="/2017/05/15/二维码/二维码进行图像处理.png" alt=""></p><p>将上图文字化具象一下，可以得到一下具体步骤：</p><ul><li>拍摄图像输入</li><li>预处理</li><li>图像定位</li><li>图像校正</li><li>解码或译码</li></ul><p>关于图像输入的渠道目前主要分为四种，该四种包含了几乎所有的信息码。</p><ol><li>光管输入：主要采集一维码</li><li>激光输入：采集一维码与QR Code</li><li>线阵输入：即由LED组成的矩阵二维码</li><li>图像输入：即照片等形式的扫描</li></ol><p>图像的采集与数字图像的质量由二维矩阵的形式存储与计算机中。其中该图像的对比度、清晰度等等都是图像质量的主要影响因素。清晰度是指所采集图像的亮度、对比度、尺寸大小、颜色饱和度（颜色饱和度越小，清晰度越高）。</p><p>用物理的方式表示图像采集可以抽象为下图所表示的模型：</p><p><img src="/2017/05/15/二维码/数字图像的采集过程.png" alt=""></p><p>接下来详细介绍一下预处理模块。这个模块是相当重要的，其主要功能就是将二维码从众多复杂信息中提取出来。可能你会问，二维码不就在那里嘛，一眼就可以看见，怎么个复杂法？也许对于人眼来说二维码很好辨认，但是对于计算机来说可不是这样。计算器需要一些列复杂的判断才能分辨出二维码的位置。</p><p><img src="/2017/05/15/二维码/预处理.png" alt=""></p><p>上图就是一个预处理所进行的步骤。咱们来一个个的分析。</p><p><strong>图像的灰度化处理</strong></p><p>摄像头采集到的二维条码图片是RGB格式的彩色图像，由红(Red)、绿(Green)、蓝(Blue)三种基本颜色按照一定的比例混合得到，每一种颜色分量有256个灰度级，三种颜色组合可以表示出多种颜色，几乎可以表示人类能够感知到的所有颜色。彩色图像包含了大量识别过程中不需要的色彩信息，这些信息都需要占用存储空间。在计算机中，R、G、B三个分量分别占用一个字节的内存，一个像素至少需要占用三个字节的内存。灰度图像只表示亮度信息，只需要占用一个字节内存。所以，在图像处理过程中，通常都先将彩色图像转换成灰度图像，这样不但可以减少存储开销，而且可以减少后续图像处理的计算量，加快二维条码识别速度。</p><p>假设灰度图像中灰度值用 Y 表示，彩色图像各分量的灰度值分别为 R、 G、 B，那么标准的灰度值Y的计算公式为：</p><center>Y = 0.30R+ 0.59G+0.11B</center><p><strong>图像的中值滤波处理</strong></p><p>普通CMOS摄像头采集到的二维条码图像中经常会含有一些噪声点，导致图像退化，对后续的条码识别产生严重的干扰，降低条码识别率。针对图像中普遍存在的高斯噪声、椒盐噪声采取了中值滤波降噪处理，能够有效的去除上述几种噪声，保护图像的边缘信息，不会对后续的边缘提取产生很大的影响。</p><p>中值滤波具有良好的噪声抑制能力，是一种非线性平滑技术。对于像素点，它统计其领域窗口内所有像素点的灰度值并进行排序，将这个像素点的灰度值设置为排序后的中间值。一般选取窗口像素点个数都为奇数，因此中值滤波的数学表达式如下：</p><center>P<sub>M</sub> = Median{ P<sub>1</sub>, P<sub>2</sub>, P<sub>3</sub> … P<sub>n</sub> } = P<sub>i((n+1) / 2)</sub></center><p>其中， P<sub>1</sub>, P<sub>2</sub>, P<sub>3</sub> … P<sub>n</sub> 是邻域内的像素灰度值，P<sub>M</sub> 就是邻域内的中指。</p><p>窗口选取对图像中值滤波的效果也将产生很大的影响。根据不同的应用场合需要采用不同的采样窗口，常见的采样窗口有十字形、米字型、棱形、矩形等，其中矩形窗口最为常用。本文主要是去除二维条码图像中含有的椒盐噪声，结合二维条码的特性，选用 3*3 的矩形窗口作为滤波窗口。</p><p>中值滤波是通过对邻域内像素的灰度值进行排序，能后选取排序结果的中间值作为该像素点的灰度值来实现的。图像中存在的椒盐噪声一般是随机分散分布的，在较小的领域内通常不会存在多个椒盐噪声，经过排序后，噪声点通常是排在靠前或者靠后的位置，因此，中值点的值比较能代表这个像素点的实际灰度值。中值滤波的具体操作过程如下：</p><ol><li>遍历这个模板中所有像素点的灰度值</li><li>将这些灰度值从小到大进行排序</li><li>选取排序结果的中间值，并将其作为这个模板中心像素点的灰度值</li></ol><p><strong>图像的二值化处理</strong></p><p>基于门限化的二值化方法。大津算法(Otsu)就是其中的代表，对光照均匀的图片具有良好的分割效果。但拍摄时，由于光源的不确定性经常会导致拍摄到的图像存在光照不均的现象，图像中靠近光源那边比较明亮，另一边则比较暗，对后续的图像二值化处理产生严重的影响。这里就采用了一种自适应亮度均衡化算法。</p><p>日本学者大津展之首先提出了最大类间方差理论，也叫做大津算法，简称 OTSU 算法。它的基本思想如下：设定一个阈值 t，它把一幅灰度图像分割成两组，一组灰度对应目标图像，另一组对应背景图像。假设灰度图像的灰度值为 0~k 级， t 从 0 开始取值，一直到 k，当 t=T 使得这两组灰度值的类间方差最大，类内方差最小时，目标图像和背景图像两部分的差别达到了最大化，若将此时的阈值 T 作为二值化的门限值，将获得最佳的二值化效果。</p><p>比如:一幅大小为 M*N 的 QR 条码图像，设阈值 t 将图像分割成前景色和背景色两组，n<sub>i</sub> 为图像中灰度值的像素个数， G 为灰度图的最大灰度级，8位灰度图的最大灰度级为255，则：</p><p>前景/背景色像素点占整个图像的比例 ω1 和 ω2 分别为：</p><p><img src="/2017/05/15/二维码/公式一.png" alt=""></p><p>前景/背景色像素点的平均灰度值 μ1 和 μ2 分别为：</p><p><img src="/2017/05/15/二维码/公式二.png" alt=""></p><p>整幅图像的平均灰度值 μ 为：</p><p><img src="/2017/05/15/二维码/公式三.png" alt=""></p><p>则两组图像的类间方差 σ<sup>2</sup>(t) 为：</p><center>σ<sup>2</sup>(t) = ω1μ1 - μ<sup>2</sup> + ω2μ2 - μ<sup>2</sup></center><p>当 0~k 之间改变 t，带入到上式中，当 t=T 使得 σ<sup>2</sup>(t) 取得最大值时，有由大津定律我们得知，T 即为最佳阙值，T = max[σ<sup>2</sup>(t)]。根据阙值 T 将图像数据分为大于阙值与小于阙值两部分。</p><p>由于大津算法是全局阈值法，图像分割阈值的选取是建立在整幅图片前景色和背景色的区分基础上的，当图像中存在比较严重的光照不均时，容易错误的将图像中较暗的部分认为是前景色，较亮的部分背景色，而不是把条码本身作为前景色，得到错误的门限阈值，得到的效果并不理想。</p><p>所以使用自适应亮度调节！</p><p><strong>二维码预处理之自适应亮度均衡算法的介绍：</strong></p><p>第一步：将条形码图像的第一行像素均等的分成 N 块，假设每一块含有 C 个像素，块内每个像素点分别标记为 p<sub>N1</sub>, p<sub>N2</sub>, p<sub>N3</sub>, …, p<sub>NC</sub>。则图像一行的像素点总数个数 K = N * C。p<sub>N1</sub> 和 p<sub>NC</sub> 分别为这个快中第一个和最后一个像素，计算出每块的灰度等级记为 SL<sub>N</sub> 为：</p><center>SL<sub>N</sub> = MAX_M(p<sub>N1</sub>~p<sub>NC</sub>) / M</center><p>其中，MAX_M(p<sub>N1</sub>~p<sub>NC</sub>) = ∑ H<sub>pi</sub>。H<sub>pi</sub> 为这个块中最大的 M 个像素点的灰度值，利用求平均值的方法来减小噪声点的干扰。</p><p>第二步：计算前后两块的插值为：</p><center>D<sub>i</sub>ff<sub>N</sub> = | SL<sub>N</sub> - SL<sub>N+1</sub> |</center><p>同时，不断将块的大小进行对等分裂，直至差值 D<sub>i</sub>ff<sub>N</sub> 小于阙值或者块的大小不大于 M。</p><p>第三步：在相邻两个块之间使用线性插值计算，可以计算出块内每个点的像素灰度值，用 S<sub>Ni</sub> 表示如下：</p><center>S<sub>Ni</sub> = ( SL<sub>N</sub> <em> ( C-1 ) + SL<sub>N</sub> </em> i ) / C</center><p>其中，S<sub>Ni</sub> 表示第 N 块中的第 i 个像素的灰度值。</p><p>第四步：基于上诉计算出的光照分布，就可以自适应的调整给各个像素点的增益。如果 S<sub>Ni</sub> 的值较小，这个像素点的增益就可以适当加大，反之则减小。这样就可以将背景较暗的部分加量，背景明亮的部分则基本保持不变，达到光照补偿期望的效果。具体的增益公式如下：</p><center>AGC<sub>Ni</sub> = BC / S<sub>Ni</sub></center><p>其中，AGC<sub>Ni</sub> 为第 N 块第 i 个像素的增益。BC是经验值，用来控制图像亮度的系数值，通过反复试验，得出当 BC = 240 时能获得良好的效果。</p><p>处理后图像每个像素点的灰度值为：</p><center>p<sub>Ni</sub>‘ = p<sub>Ni</sub> * AGC<sub>Ni</sub></center><p>这里的 p<sub>Ni</sub> 为原始灰度值，p<sub>Ni</sub>‘ 为原图像中的像素点在经过自适应增益变换后的灰度值。同时，注意到，当 SL<sub>N</sub> 的灰度级比较大时，已经不需要对其进行增益处理，此时我们可以设定一个阙值 T，当 SL<sub>N</sub> 超过这个阙值时，不对该像素点进行增益处理。</p><p>第五步：采用大津算法对自适应亮度均衡后的图形进行二值化处理，效果如下图：</p><p><img src="/2017/05/15/二维码/效果图.png" alt=""></p><p>算法部分暂时到此结束。接下来将介绍二维码的定位方式与解码过程。</p>]]></content>
    
    <summary type="html">
    
      &lt;pre&gt;&lt;code&gt;本想一篇文章就解决的，后来发现需要介绍的有点多，一个晚上绝对写不完，看来又要开新坑了。
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&quot;一、什么是二维码？&quot;&gt;&lt;a href=&quot;#一、什么是二维码？&quot; class=&quot;headerlink&quot; title=&quot;一、什么是二维码？&quot;&gt;&lt;/a&gt;一、什么是二维码？&lt;/h2&gt;&lt;p&gt;二维码有很多种，其中占据大半江山的主要有两种，一是以 PDF417 为代表的&lt;a href=&quot;https://en.wikipedia.org/wiki/PDF417&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;堆叠式条形二维码&lt;/a&gt;，另一个是以 QR Code 为代表的&lt;a href=&quot;https://en.wikipedia.org/wiki/QR_code&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;矩阵式二维码&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;PDF417 是“便携式数据文件”的缩写。它最少分为三层，最多 90 层。包括左右空白区、起始符、终止符、左右层指示符号字符等。它的具体架构如下图：&lt;/p&gt;
    
    </summary>
    
      <category term="编程" scheme="https://saberda.github.io/categories/%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="二维码" scheme="https://saberda.github.io/tags/%E4%BA%8C%E7%BB%B4%E7%A0%81/"/>
    
  </entry>
  
  <entry>
    <title>模拟退火</title>
    <link href="https://saberda.github.io/2017/04/21/%E6%A8%A1%E6%8B%9F%E9%80%80%E7%81%AB/"/>
    <id>https://saberda.github.io/2017/04/21/模拟退火/</id>
    <published>2017-04-21T13:48:33.000Z</published>
    <updated>2017-04-21T15:22:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>模拟退火是一种通用概率算法，用来在固定时间内寻求一个大的寻找空间内找到的最优解。</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>模拟退火来自冶金学的专有名词退火。退火是将材料加热后再经特定速率冷却，目的是增大晶粒体积，并且减少晶格中的缺陷。材料中的原子原来会停留在使内能有局部最小值的位置，而随机在其他位置中移动。退火冷却时速度较慢，使得原子有较多可能可以找到内能比原来更低的位置。</p><p>模拟退火的原理也和金属退火的原理近似：我们将热力学的理论套用到统计学上，将搜寻空间内每一点想像成空气内的分子；分子的能量，就是它本身的动能；而搜寻空间内的每一点，也像空气分子一样带有“能量”，以表示该点对命题的合适程度。算法先以搜寻空间内一个任意点作起始：每一步先选择一个“邻居”，然后再计算从现有位置到达“邻居”的概率。</p><a id="more"></a><p>模拟退火问题大方向是属于贪心算法一类，关于贪心算法的介绍，详见我的这篇文章</p><p><a href="http://www.saberismywife.com/2016/11/08/详探贪心算法/" target="_blank" rel="noopener">详探贪心算法</a></p><h2 id="模拟退火"><a href="#模拟退火" class="headerlink" title="模拟退火"></a>模拟退火</h2><p>考虑寻找一个低能量系统的问题，其状态由一个马尔科夫链排序。</p><center>F = &lt; E &gt; - TH</center><p>其中 F 表示的是物理系统中的自由能量。</p><p>温度 T 可视为一种伪温度，它控制表示神经元“突触噪声”的热波动。它的精确标度因而无关紧要。相应的，我们可以定义概率 p<sub>i</sub> 和抛分函数(partition函数) Z 如下：</p><center>p<sub>i</sub> = Z<sup>-1</sup>(-E<sub>i</sub> / T)</center><p>和</p><center>Z = ∑ exp(-E<sub>i</sub> / T)</center><p>由F = &lt; E &gt; - TH观察到当温度 T 趋近于零，系统的自由能量F趋近平均能量 <e>。由 F -&gt; &lt; E &gt;，我们观察到由自由能量最小化原则，该马尔科夫链的平稳分布(即Gibbs分布)，当 T -&gt; 0 时塌陷到平均能量 &lt; E &gt; 的全局最小点。换句话说，序列中的低能状态在低温时受到更强的支持。这些观察促使我们提出问题：为什么不简单应用 Metropolis 算法产生大量的代表该随机系统在低温下的构成(Configuration)？我们不提倡使用使用这种策略是因为在很低温度下马尔科夫链到热平衡的收敛速度特别慢。而提高计算效率更好的方法是在较高温度运行随机系统，这是达到平衡状态的收敛相当快，接着随温度的精细下降保持系统的平衡态。也就是我们使用两个相关成分的组合：</e></p><ul><li>一个决定温度下降速度的调度表</li><li>一个算法迭代求解每个温度表给出的新的温度下的平衡分布，这是利用前面温度时的最终状态作为新温度时的起始点</li></ul><p>我们刚才提到的两步格式是被广泛使用的以模拟退火著称的随机松弛技术的精华。</p><p>模拟退火的最初的目的是寻找刻画复杂大系统的代价函数的全局最小点。正式因为如此，他提供一个求解非凸最优化问题的有力工具</p><blockquote><p>当优化一个非常庞大的大系统（即具有许多自由度的系统）时不要求总是下降而是试图要求大部分时间在下降</p></blockquote><p>模拟退火在两方面与传统的迭代优化算法不同：</p><ul><li>算法不会陷入局部最小，因为当系统在非零度温度上运行时脱离局部最小总是可能的</li><li>模拟退火是自适应的，在高温时看见系统的终态的大致轮廓，而它的具体细节在低温度时才呈现出来。</li></ul><h2 id="演算步骤"><a href="#演算步骤" class="headerlink" title="演算步骤"></a>演算步骤</h2><p><strong><u>初始化</u></strong></p><p>生成一个可行的解作为当前解输入迭代过程，并定义一个足够大的数值作为初始温度。</p><p><strong><u>迭代过程</u></strong></p><p>迭代过程是模拟退火算法的核心步骤，分为新解的产生和接受新解两部分：</p><ul><li>由一个产生函数从当前解产生一个位于解空间的新解；为便于后续的计算和接受，减少算法耗时，通常选择由当前新解经过简单地变换即可产生新解的方法，如对构成新解的全部或部分元素进行置换、互换等，注意到产生新解的变换方法决定了当前新解的邻域结构，因而对冷却进度表的选取有一定的影响。</li><li>计算与新解所对应的目标函数差。因为目标函数差仅由变换部分产生，所以目标函数差的计算最好按增量计算。事实表明，对大多数应用而言，这是计算目标函数差的最快方法。</li><li>判断新解是否被接受，判断的依据是一个接受准则，最常用的接受准则是Metropolis准则：若Δt′&lt;0则接受S′作为新的当前解S，否则以概率exp（-Δt′/T）接受S′作为新的当前解S。</li><li>当新解被确定接受时，用新解代替当前解，这只需将当前解中对应于产生新解时的变换部分予以实现，同时修正目标函数值即可。此时，当前解实现了一次迭代。可在此基础上开始下一轮试验。而当新解被判定为舍弃时，则在原当前解的基础上继续下一轮试验。</li></ul><p>模拟退火算法与初始值无关，算法求得的解与初始解状态S（是算法迭代的起点）无关；模拟退火算法具有渐近收敛性，已在理论上被证明是一种以概率1收敛于全局最优解的全局优化算法；模拟退火算法具有并行性。</p><p><strong><u>停止准则</u></strong></p><p>温度T降至某最低值时，完成给定数量迭代中无法接受新解，停止迭代，接受当前寻找的最优解为最终解。</p><p><strong><u>退火方案</u></strong></p><p>在某个温度状态T下，当一定数量的迭代操作完成后，降低温度T，在新的温度状态下执行下一个批次的迭代操作。</p><h2 id="退火进度表"><a href="#退火进度表" class="headerlink" title="退火进度表"></a>退火进度表</h2><p>如前面所提到的，模拟退火过程的基础是 Metropolis 算法，其间温度 T 慢慢下降。也就是说，温度 T 起调节作用。假定温度下降没有对数快，则模拟退火过程将手链于一个具有最小能量的构型。遗憾的是这种退火进度太慢了–慢的不切实际。实际上，我们必须求诸于算法的渐进收敛的有限时间逼近。这种逼近所付出的代价是算法不再以概率 1 保证找到全局最小点。然而算法的逼近结果在许多实际应用中能产生近似最优解。</p><p>为了实现模拟退火算法的有限时间接近，我们必须设定一系列控制算法的有限序列值，以及每一温度值下有限的转移尝试次数。Kirlpatrick 等给出的退火进度表的感兴趣的参数设定如下：</p><ul><li><strong>温度的初始值</strong>。温度的初始值 T<sub>0</sub> 选的足够高使得所有提出的转移实际都能被模拟退火算法所接受</li><li><strong>温度的下降</strong>。一般地说，冷却是按指数形式完成的，并且温度值的改变量都很小。特别地，下降函数定义为： T<sub>k</sub> = α * T<sub>k-1</sub>, k = 1,2,3… 其中 α 小于但接近于1。α 的典型值介于 0.8 到 0.99 之间。对每一温度，有足够的转移的尝试，使得平均每次实验有 10 次转移被接受。</li><li><strong>温度的最后值</strong>。如果在三次相连的温度下没有得到预期的接收次数，则系统被冻结且退火停止。</li></ul><p>后一个标准可以改进，要求接受率小于一预定值，而接受率定义为转移接受的次数除以提出转移的次数。</p><h2 id="模拟退火用于组合优化"><a href="#模拟退火用于组合优化" class="headerlink" title="模拟退火用于组合优化"></a>模拟退火用于组合优化</h2><p>模拟退火特别适用于解组合优化问题。组合优化的目标是针对有很多可能解的有限离散系统，最小化它的代价函数。本质上讲模拟退火利用 Metropolis 算法通过多粒子物理系统和组合优化问题间的类比一些列解。</p><p>在模拟退火中，我们把上面式子中的 p<sub>i</sub> 的 Gibbs 分布中的能量 E<sub>i</sub> 解释成能量的代价，而温度 T 解释为控制参数。在组合优化问题中对每一构型赋予一数值的代价以描述这个特殊的构型和解的差异。模拟退火程序中下一个需要考虑的问题是确认构型和从已有构型以局部方式产生新的构型。这就是 Metropolis 算法发挥作用之处。因此我们概括统计物理的术语和组合优化术语之间的关系如下表所示。</p><p><img src="/2017/04/21/模拟退火/1.png" alt=""></p><h2 id="伪代码"><a href="#伪代码" class="headerlink" title="伪代码"></a>伪代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line"> *  J(y)：在状态y时的评价函数值</span><br><span class="line"> *  Y(i)：表示当前状态</span><br><span class="line"> *  Y(i+1)：表示新的状态</span><br><span class="line"> *  r： 用于控制降温的快慢</span><br><span class="line"> *  T： 系统的温度，系统初始应该要处于一个高温的状态</span><br><span class="line"> *  T_min ：温度的下限，若温度T达到T_min，则停止搜索</span><br><span class="line">*/</span><br><span class="line">while( T &gt; T_min ) &#123;</span><br><span class="line">　　dE = J( Y(i+1) ) - J( Y(i) ) ;  </span><br><span class="line"> //表达移动后得到更优解，则总是接受移动</span><br><span class="line">　　if ( dE &gt;= 0 ) </span><br><span class="line">　　//接受从Y(i)到Y(i+1)的移动</span><br><span class="line">        Y(i+1) = Y(i) ;  </span><br><span class="line">　　else &#123;t</span><br><span class="line">    // 函数exp( dE/T )的取值范围是(0,1) ，dE/T越大，则exp( dE/T )也越大</span><br><span class="line">        if ( exp( dE/T ) &gt; random( 0 , 1 ) ) </span><br><span class="line">         //接受从Y(i)到Y(i+1)的移动</span><br><span class="line">            Y(i+1) = Y(i); </span><br><span class="line">　　&#125;</span><br><span class="line">　　//降温退火 ，0&lt;r&lt;1 。r越大，降温越慢；r越小，降温越快</span><br><span class="line">　　T = r * T ;  </span><br><span class="line">　　//若r过大，则搜索到全局最优解的可能会较高，但搜索的过程也就较长。</span><br><span class="line">　　//若r过小，则搜索的过程会很快，但最终可能会达到一个局部最优值</span><br><span class="line">　　i ++ ;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>模拟退火用白话来讲就像当于一个进化版的爬山算法（爬山算法：只能搜索到局部的最优解），而模拟退火在搜索过程中引入了随机因素。</p><p>即模拟算法以一定的概率来接受一个比当前解要差的解，因此有可能会跳出这个局部的最优解，从而达到全局的最优解。</p><p>简单描述：</p><ul><li>若J( Y(i+1) )&gt;= J( Y(i) )  (即移动后得到更优解)，则总是接受该移动</li><li>若J( Y(i+1) )&lt; J( Y(i) )  (即移动后的解比当前解要差)，则以一定的概率接受移动，而且这个概率随着时间推移逐渐降低（逐渐降低才能趋向稳定）</li></ul><p>模拟退火算法是一种随机算法，并不一定能找到全局的最优解，可以比较快的找到问题的近似最优解。 如果参数设置得当，模拟退火算法搜索效率比穷举法要高。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;模拟退火是一种通用概率算法，用来在固定时间内寻求一个大的寻找空间内找到的最优解。&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;模拟退火来自冶金学的专有名词退火。退火是将材料加热后再经特定速率冷却，目的是增大晶粒体积，并且减少晶格中的缺陷。材料中的原子原来会停留在使内能有局部最小值的位置，而随机在其他位置中移动。退火冷却时速度较慢，使得原子有较多可能可以找到内能比原来更低的位置。&lt;/p&gt;
&lt;p&gt;模拟退火的原理也和金属退火的原理近似：我们将热力学的理论套用到统计学上，将搜寻空间内每一点想像成空气内的分子；分子的能量，就是它本身的动能；而搜寻空间内的每一点，也像空气分子一样带有“能量”，以表示该点对命题的合适程度。算法先以搜寻空间内一个任意点作起始：每一步先选择一个“邻居”，然后再计算从现有位置到达“邻居”的概率。&lt;/p&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="算法" scheme="https://saberda.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="Machine Learning" scheme="https://saberda.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>支持向量机与核函数</title>
    <link href="https://saberda.github.io/2017/04/17/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>https://saberda.github.io/2017/04/17/支持向量机/</id>
    <published>2017-04-17T08:53:54.000Z</published>
    <updated>2017-04-17T13:32:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>支持向量机，support vector machines，简称SVM。一下全文基本都是用SVM一词。</p><p>先将这个复杂的概念化小，个人理解，SVM就是很好的现成的分类器，这里的“现成”指的是分类器不加修改即可直接使用。同时，这就意味着数据上应用基本形式的SVM分类器就可以得到低错误率的结果。SVM能够对训练集之外的数据点做出很好的分类决策。</p><p>本文主要介绍当前比较流行的SMO算法与核函数。</p><p>关于具体的SVM算法可以参考我的这篇笔记<br><a href="http://www.saberismywife.com/2016/12/19/Machine-Learning-7/" target="_blank" rel="noopener">SVM and Kernels</a></p><h2 id="原理–基于最大间隔分隔数据"><a href="#原理–基于最大间隔分隔数据" class="headerlink" title="原理–基于最大间隔分隔数据"></a>原理–基于最大间隔分隔数据</h2><a id="more"></a><pre><code>支持向量机优点：泛华错误率低，计算开销不大，结果易解释缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题使用数据类型：数值型和标称型数据</code></pre><p>在介绍SVM这个主题之前，先解释几个概念。</p><p>假设现在有一组二维数据，它的数据点之间已经分隔的足够开，因此很容易就可以画出一条直线将其分为两组数据。在这种情况下，这组数据被称为<strong>线性可分(linearly separable)</strong>。大家先不用担心上诉假设过于完美，稍后当直线不能将数据点分开时，我们会对上诉假设做出一些调整。</p><p>上诉将数据集分开的直线称为<strong>分隔超平面(separating hyperplane)</strong>。在上面给出的例子中，由于给的是二维的数据集，因此此时的超平面就只是一条直线。如果所给的数据集是三维的，那么他就是一个平面。显而易见，更高维的以此类推。但是超过四维的就已经超出可描绘的范围了，此时我们称该可以区分边界的对象为<strong>超平面(hyperplane)</strong>，也就是分类的决策边界。分布在超平面一侧的所有数据都属于某个类别，而分布在另一侧的所有数据则属于另一个类别。</p><p>我们希望采用这种方式来构建分类器，即如果数据点离决策边界越远，那么其最后预测的结果就越可信。</p><p>但是有时能区分数据的直线或者平面不止一个，此时我们首先可能想到的就是做类似于直线拟合的操作，但是这并非最佳方案。我们希望能找到离分隔超平面最近的点，确保他们离分隔面尽可能远。这里点到分隔面的距离称为“间隔”(margin)。我们希望间隔尽可能的大，这是因为如果我们犯错或者在有限数据集上训练分类器的话，我们希望分类器尽可能功能完善并健壮。</p><p><strong>支持向量(support vector)</strong>就是离分隔超平面最近的那些点。接下来我们要试着最大化支持向量到分隔面的距离，需要寻求此问题的优化求解方法。</p><h2 id="寻找最大间隔"><a href="#寻找最大间隔" class="headerlink" title="寻找最大间隔"></a>寻找最大间隔</h2><p>关于这个问题的理论求解在本文开头时的那篇链接的文章中已经介绍的很全面了，这里就不多费口舌讲解集体的数学过程。</p><p>所以我们直接跳到SVM的一般流程，若你对数学推导很有兴趣那么就强烈建议你先去看那篇文章。</p><p><strong>SVM的一般流程</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1）收集数据</span><br><span class="line">2）准备数据：需要数值型数据</span><br><span class="line">3）分析数据：有助于可视化分隔超平面</span><br><span class="line">4）训练算法：SVM的大部分时间都源自训练，该过程主要实现两个参数的调优</span><br><span class="line">5）测试算法：十分简单的计算过程</span><br><span class="line">6）使用算法：几乎所有分类问题都可以使用SVM，值得一提的是，SVM本身是一个二类分类器，对多类问题应用SVM需要对代码进行一些修改</span><br></pre></td></tr></table></figure><h2 id="SMO–高效优化算法"><a href="#SMO–高效优化算法" class="headerlink" title="SMO–高效优化算法"></a>SMO–高效优化算法</h2><p>1996年，John Platt发布了一个称谓SMO的强大算法，用于训练SVM。SMO表示<strong>序列最小优化(sequential minimal optimization)</strong>。Platt的SMO算法是将大优化问题分解为多个小优化问题来求解的。这些小优化问题往往很容易求解，并且对他们进行顺序求解的结果与将他们作为整体来求解的结果完全一致。与此同时，SMO算法的求解时间短很多。</p><p><strong>SMO算法的目标</strong>是求出一系列 alpha 和 b ， 一旦求出了这些 alpha，就很容易计算出权重向量 w 并得到分隔超平面。</p><p><strong>SMO算法的工作原理</strong>：每次循环中选择两个 alpha 进行优化处理。一旦找到一对合适的 alpha，那么久增大其中一个同时减小另一个。这里所谓的“合适”是指两个 alpha 必须要符合一定的条件，条件之一就是这两个 alpha 必须要在间隔边界之外，而其中第二个条件是这两个 alpha 还没有进行过区间化处理或者不在边界上。</p><p>Platt的SMO算法的完整实现需要实现大量的代码。所以我们由浅入深，先将其进行简化处理，以便了解算法的基本工作思路，之后再基于简化版给出完整版。简化版的代码虽然量少，但是执行速度慢。Platt SMO算法中的外循环确定要优化的最佳 alpha 对。而简化版却会跳过这一部分，首先在数据集上做一次遍历，遍历每一个 alpha，然后在剩余的 alpha 集合中随机选择另一个 alpha，从而构成 alpha 对。这里有一点相当重要，就是我们要同时改变两个 alpha。之所以这样做是因为我们有个约束条件：</p><center>∑α<sub>i</sub>*label<sup>(i)</sup> = 0</center><p>由于单独改变一个 alpha 可能会导致该约束条件失效，因此我们总是同时调整两个 alpha。</p><p>此外，我们构建一个辅助函数，用于在某个区间范围内随机选择一个整数。同时，我们也需要了另一个辅助函数，用于在数值太大时对其进行调整。下面的代码实现了这两个函数。首先新建一个叫 svmMLiA.py 的文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def loadDataSet(fileName):</span><br><span class="line">    dataMat = []; labelMat = []</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    for line in fr.readlines():</span><br><span class="line">        lineArr = line.strip().split(&apos;\t&apos;)</span><br><span class="line">        dataMat.append([float(lineArr[0]), float(lineArr[1])])</span><br><span class="line">        labelMat.append(float(lineArr[2]))</span><br><span class="line">    return dataMat,labelMat</span><br><span class="line"></span><br><span class="line">def selectJrand(i,m):</span><br><span class="line">    j=i #we want to select any J not equal to i</span><br><span class="line">    while (j==i):</span><br><span class="line">        j = int(random.uniform(0,m))</span><br><span class="line">    return j</span><br><span class="line"></span><br><span class="line">def clipAlpha(aj,H,L):</span><br><span class="line">    if aj &gt; H: </span><br><span class="line">        aj = H</span><br><span class="line">    if L &gt; aj:</span><br><span class="line">        aj = L</span><br><span class="line">    return aj</span><br></pre></td></tr></table></figure><p>上诉代码中，第一个函数就是常见的 loadDatSet() 函数，该函数的功能显而易见。</p><p>第二个函数 selectJrand() 有两个参数值，其中 i 是第一个 alpha 的下标，m 是所有 alpha 的数目。只要函数值不等于输入值 i，函数就会进行随机选择。</p><p>最后一个辅助函数是 clipAlpha()，它是用于调整大于H 或小于 L 的 alpha 值。</p><p>尽管上诉三个辅助函数本身做的事情不是很多，但在分类器中却很有用。</p><p>然后我们进入到python环境中，输入如下语句，看看预期效果。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import svmPLiA</span><br><span class="line">dataArr, labelArr = avmPLiA.loadDataSet(&apos;testSet.txt&apos;)</span><br><span class="line">labelArr</span><br></pre></td></tr></table></figure><p>从输出结果中我们可以看出，这里采用的类别标签是-1和1，而不是0和1。</p><p>上诉工作完成后就可以实现SMO的第一个版本了，伪代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">创建一个 alpha 向量并将其初始化为零向量</span><br><span class="line">当迭代次数小于最大迭代次数时（外循环）</span><br><span class="line">对数据集中的每个数据向量（内循环）：</span><br><span class="line">如果该数据可以被优化：</span><br><span class="line">随机选择另外一个数据向量</span><br><span class="line">同时优化这两个向量</span><br><span class="line">如果两个向量都不能被优化，退出内循环</span><br><span class="line">如果所有向量都没有被优化，增加迭代数目，进行下一次循环</span><br></pre></td></tr></table></figure><p>接下来就是代码部分了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">def smoSimple(dataMatIn, classLabels, C, toler, maxIter):</span><br><span class="line">    dataMatrix = mat(dataMatIn); labelMat = mat(classLabels).transpose()</span><br><span class="line">    b = 0; m,n = shape(dataMatrix)</span><br><span class="line">    alphas = mat(zeros((m,1)))</span><br><span class="line">    iter = 0</span><br><span class="line">    while (iter &lt; maxIter):</span><br><span class="line">        alphaPairsChanged = 0</span><br><span class="line">        for i in range(m):</span><br><span class="line">            fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b</span><br><span class="line">            Ei = fXi - float(labelMat[i])#if checks if an example violates KKT conditions</span><br><span class="line">            # 如果 alpha 可以更改进入优化过程</span><br><span class="line">            if ((labelMat[i]*Ei &lt; -toler) and (alphas[i] &lt; C)) or ((labelMat[i]*Ei &gt; toler) and (alphas[i] &gt; 0)):</span><br><span class="line">            # 随机选择第二个 alpha</span><br><span class="line">                j = selectJrand(i,m)</span><br><span class="line">                fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b</span><br><span class="line">                Ej = fXj - float(labelMat[j])</span><br><span class="line">                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();</span><br><span class="line">                # 保证 alpha 在 0 与 C 之间</span><br><span class="line">                if (labelMat[i] != labelMat[j]):</span><br><span class="line">                    L = max(0, alphas[j] - alphas[i])</span><br><span class="line">                    H = min(C, C + alphas[j] - alphas[i])</span><br><span class="line">                else:</span><br><span class="line">                    L = max(0, alphas[j] + alphas[i] - C)</span><br><span class="line">                    H = min(C, alphas[j] + alphas[i])</span><br><span class="line">                if L==H: print &quot;L==H&quot;; continue</span><br><span class="line">                eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T</span><br><span class="line">                if eta &gt;= 0: print &quot;eta&gt;=0&quot;; continue</span><br><span class="line">                alphas[j] -= labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">                alphas[j] = clipAlpha(alphas[j],H,L)</span><br><span class="line">                if (abs(alphas[j] - alphaJold) &lt; 0.00001): print &quot;j not moving enough&quot;; continue</span><br><span class="line">                # 对 i 进行修改，修改量与 j 相同，但方向相反</span><br><span class="line">                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])</span><br><span class="line">                # 设置常数项</span><br><span class="line">                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T</span><br><span class="line">                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T</span><br><span class="line">                if (0 &lt; alphas[i]) and (C &gt; alphas[i]): b = b1</span><br><span class="line">                elif (0 &lt; alphas[j]) and (C &gt; alphas[j]): b = b2</span><br><span class="line">                else: b = (b1 + b2)/2.0</span><br><span class="line">                alphaPairsChanged += 1</span><br><span class="line">                print &quot;iter: %d i:%d, pairs changed %d&quot; % (iter,i,alphaPairsChanged)</span><br><span class="line">        if (alphaPairsChanged == 0): iter += 1</span><br><span class="line">        else: iter = 0</span><br><span class="line">        print &quot;iteration number: %d&quot; % iter</span><br><span class="line">    return b,alphas</span><br></pre></td></tr></table></figure><p>这个函数可能比较大，咱们慢慢来看。</p><p>上诉函数首先先将多个列表和输入参数转换成 NumPy 矩阵，这样就可以简化很多数学操作。由于转置了类别标签，因此我们得到的是一个列向量而不是列表。于是类别标签向量的每行元素都与数据矩阵中的行一一对应。我们也可以通过矩阵 dataMatIn 的 shape 属性得到常数 m 和 n。最后我们可以构建一个 alpha 列矩阵，矩阵元素都初始化为0，并建立一个 iter 变量。该变量存储的则是在没有任何 alpha 改变的情况下的遍历数据集的次数。当该变量达到输入值 maxIter 时函数结束运行并退出。</p><p>最后，在优化过程结束的同时，必须确保在合适的时机结束循环。如果程序执行到 for 循环的最后一行都不执行 continue 语句，那么就已经成功的改变了一对 alpha，同时可以增加 alphaPairsChanged 的值。在 for 循环之外，需要检查 alpha 值是否做了更新，如果有更新则将 iter 设为0后继续运行程序。只有在所有数据集上遍历 maxIter 次，且不再发生任何 alpha 修改之后，程序才会停止并退出 while 循环。</p><p>由于SMO算法的随机性，大家在多运行几次后所得到的结果可能会不一样。</p><p><strong>利用完整的 Platt SMO算法加速优化</strong> </p><p>在几百个点组成的小规模数据集中，简化版SMO算法运行的时间是没有什么问题的，但是在更大的数据集上的运行速度就会变慢。刚才已经讨论了SMO的简化版算法，现在来谈谈完整版的SMO算法。在这两个版本中，实现 alpha 的更改和代数运算的优化环节一模一样。在优化过程中唯一的不同就是选择 alpha 的方式。完整版的 Platt SMO 算法应用了一些能够提速的启发方法。或许有些技术厉害的读者已经意识到上诉代码还可以优化运行时间。</p><p>Platt SMO 算法是通过一个外循环来选择第一个 alpha 值的，并且其选择过程会在两种方式之间进行交替：一种方式是在所有数据集上进行单遍扫描，另一种方式则是在非边界 alpha 中实现单遍扫描。而所谓的非边界 alpha 值的就是那些不等与边界0或C的 alpha 值。对整个数据集的扫描相当容易，而实现非边界 alpha 值的扫描时，首先需要建立这些 alpha 的值的列表，然后对这个表进行遍历。同时，该步骤会跳过那些已知的不会改变的 alpha 的值。</p><p>在选择第一个 alpha 值后，算法会通过一个内循环来选择第二个 alpha 值。在优化过程中，会通过<strong>最大化步长</strong>的方式来获得第二个 alpha 值。在简化版SMO算法中，我们会在选择 j 之后计算错误率 Ej。但在这里，我们会建立一个全局的缓存用于保存误差值，并从中选择步长或者说 Ei-Ej 最大的 alpha 值。</p><p><strong>完整版 SMO 的辅助函数</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">class optStruct:</span><br><span class="line">    def __init__(self,dataMatIn, classLabels, C, toler, kTup):  </span><br><span class="line">        self.X = dataMatIn</span><br><span class="line">        self.labelMat = classLabels</span><br><span class="line">        self.C = C</span><br><span class="line">        self.tol = toler</span><br><span class="line">        self.m = shape(dataMatIn)[0]</span><br><span class="line">        self.alphas = mat(zeros((self.m,1)))</span><br><span class="line">        self.b = 0</span><br><span class="line">        self.eCache = mat(zeros((self.m,2))) #first column is valid flag</span><br><span class="line">        self.K = mat(zeros((self.m,self.m)))</span><br><span class="line">        for i in range(self.m):</span><br><span class="line">        # 缓存误差</span><br><span class="line">            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)</span><br><span class="line">        </span><br><span class="line">def calcEk(oS, k):</span><br><span class="line">    fXk = float(multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)</span><br><span class="line">    Ek = fXk - float(oS.labelMat[k])</span><br><span class="line">    return Ek</span><br><span class="line">        </span><br><span class="line">def selectJ(i, oS, Ei):        </span><br><span class="line"># 内循环的启发式方法</span><br><span class="line">    maxK = -1; maxDeltaE = 0; Ej = 0</span><br><span class="line">    oS.eCache[i] = [1,Ei]  </span><br><span class="line">    validEcacheList = nonzero(oS.eCache[:,0].A)[0]</span><br><span class="line">    if (len(validEcacheList)) &gt; 1:</span><br><span class="line">        for k in validEcacheList:   </span><br><span class="line">            if k == i: continue </span><br><span class="line">            Ek = calcEk(oS, k)</span><br><span class="line">            deltaE = abs(Ei - Ek)</span><br><span class="line">            # 选择具有最长步长的 j</span><br><span class="line">            if (deltaE &gt; maxDeltaE):</span><br><span class="line">                maxK = k; maxDeltaE = deltaE; Ej = Ek</span><br><span class="line">        return maxK, Ej</span><br><span class="line">    else:  </span><br><span class="line">        j = selectJrand(i, oS.m)</span><br><span class="line">        Ej = calcEk(oS, j)</span><br><span class="line">    return j, Ej</span><br><span class="line"></span><br><span class="line">def updateEk(oS, k):</span><br><span class="line">    Ek = calcEk(oS, k)</span><br><span class="line">    oS.eCache[k] = [1,Ek]</span><br></pre></td></tr></table></figure><p>首要的事情就就是建立一个数据结构来保存所有的重要值，而这个过程可以通过一个对象来完成。这里使用对象的目的并不是为了面向对象编程，而只是作为一个数据结构来使用对象。在将值传给函数时，我们可以通过将所有数据移到另一个结构中来实现，这样就可以省掉手工输入的麻烦了。而此时，数据就可以通过一个对象来进行传递。</p><p>对于给定的 alpha 值，第一个辅助函数 caleEk() 能够计算 E 值并返回。以前，该过程是采用内嵌的方式来完成的，但是由于该过程在这个版本的SMO算法中出现频繁，这里必须要将其单独实现。</p><p>下一个函数 selectJ() 用于选择第二个 alpha 值。</p><p>最后一个辅助函数 updateEk()，它会计算误差值并存入缓存中。在对 alpha 进行优化之后就会用到这个值。</p><p><strong>完整 Platt SMO 算法中的优化</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">def innerL(i, oS):</span><br><span class="line">    Ei = calcEk(oS, i)</span><br><span class="line">    if ((oS.labelMat[i]*Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i]*Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)):</span><br><span class="line">        j,Ej = selectJ(i, oS, Ei) </span><br><span class="line">        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();</span><br><span class="line">        if (oS.labelMat[i] != oS.labelMat[j]):</span><br><span class="line">            L = max(0, oS.alphas[j] - oS.alphas[i])</span><br><span class="line">            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])</span><br><span class="line">        else:</span><br><span class="line">            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)</span><br><span class="line">            H = min(oS.C, oS.alphas[j] + oS.alphas[i])</span><br><span class="line">        if L==H: print &quot;L==H&quot;; return 0</span><br><span class="line">        eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j] </span><br><span class="line">        if eta &gt;= 0: print &quot;eta&gt;=0&quot;; return 0</span><br><span class="line">        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)</span><br><span class="line">        updateEk(oS, j) </span><br><span class="line">        if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001): print &quot;j not moving enough&quot;; return 0</span><br><span class="line">        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])</span><br><span class="line">        updateEk(oS, i) </span><br><span class="line">        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]</span><br><span class="line">        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]</span><br><span class="line">        if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]): oS.b = b1</span><br><span class="line">        elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]): oS.b = b2</span><br><span class="line">        else: oS.b = (b1 + b2)/2.0</span><br><span class="line">        return 1</span><br><span class="line">    else: return 0</span><br></pre></td></tr></table></figure><p>上面这段代码几乎与简化版中的 smoSimple() 函数一模一样，但是这里的代码已经使用了自己的数据结构。该结构在参数 oS 中传递。第二个重要的修改就是使用了辅助函数 selectJ() 来改变第二个 alpha 的值。最后在 alpha 值改变时更新 Ecache。</p><p><strong>完整版 SMO 的外循环代码</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def smoP(dataMatIn, classLabels, C, toler, maxIter,kTup=(&apos;lin&apos;, 0)):    </span><br><span class="line">    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler, kTup)</span><br><span class="line">    iter = 0</span><br><span class="line">    entireSet = True; alphaPairsChanged = 0</span><br><span class="line">    while (iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entireSet)):</span><br><span class="line">        alphaPairsChanged = 0</span><br><span class="line">        if entireSet:   </span><br><span class="line">            for i in range(oS.m):        </span><br><span class="line">                alphaPairsChanged += innerL(i,oS)</span><br><span class="line">                print &quot;fullSet, iter: %d i:%d, pairs changed %d&quot; % (iter,i,alphaPairsChanged)</span><br><span class="line">            iter += 1</span><br><span class="line">        else:</span><br><span class="line">            nonBoundIs = nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0]</span><br><span class="line">            for i in nonBoundIs:</span><br><span class="line">                alphaPairsChanged += innerL(i,oS)</span><br><span class="line">                print &quot;non-bound, iter: %d i:%d, pairs changed %d&quot; % (iter,i,alphaPairsChanged)</span><br><span class="line">            iter += 1</span><br><span class="line">        if entireSet: entireSet = False #toggle entire set loop</span><br><span class="line">        elif (alphaPairsChanged == 0): entireSet = True  </span><br><span class="line">        print &quot;iteration number: %d&quot; % iter</span><br><span class="line">    return oS.b,oS.alphas</span><br></pre></td></tr></table></figure><p>其输入函数与 smoSimple() 完全一样。函数一开始构建一个数据结构来容纳所有的数据，然后需要对控制函数的退出的一些变量进行初始化。整个代码的主题也是 while 循环，但是退出条件更多一些。当迭代次数超过指定最大值，或者遍历整个集合都未对 alpha 对进行修改时，就退出循环。</p><p>接下来，我们对 for 循环在非边界循环和完整遍历之间进行切换，并打印出迭代次数。最后程序会返回边界0或C上的值。</p><p>大家可能会想，刚才我们花了大量时间来计算 alpha 值，但是如何利用他们进行分类呢？这不成问题，首先必须基于 alpha 值得到超平面，这也包括了 w 的计算。下面的一个小函数可以实现上诉任务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def calcWs(alphas,dataArr,classLabels):</span><br><span class="line">    X = mat(dataArr); labelMat = mat(classLabels).transpose()</span><br><span class="line">    m,n = shape(X)</span><br><span class="line">    w = zeros((n,1))</span><br><span class="line">    for i in range(m):</span><br><span class="line">        w += multiply(alphas[i]*labelMat[i],X[i,:].T)</span><br><span class="line">    return w</span><br></pre></td></tr></table></figure><p>我们现在成功训练出分类器了，我想指出的就是，这里的两个类中的数据点分布在一条直线的两边。但是倘若两类数据点分别分布在一个圆的内部与外部，那么会得到什么样子的分类面呢？下面我会介绍一种方法对分类器进行修改，以说明类别区域形状不同情况下的数据集分隔问题。</p><h2 id="在复杂数据上使用核函数"><a href="#在复杂数据上使用核函数" class="headerlink" title="在复杂数据上使用核函数"></a>在复杂数据上使用核函数</h2><p>这里我主要介绍的是<strong>径向基核函数</strong>，关于核函数的详细介绍还是参见本文开头的那个连接的文章。</p><p><strong>核函数的原理–将数据映射到高维空间</strong></p><p>从某个特征空间到另一个特征空间的映射是通过核函数来实现的。大家可以把核函数想象为一个<strong>包装器(wrapper)</strong>或者是<strong>接口(interface)</strong>，它能把数据从某个难处理的形式转换为另一个较容易处理的形式。如果上诉特征空间听起来很模糊的话，那么可以将它想象成一种计算距离的方法。前面我们提到过距离计算的方法。距离计算的方法有很多种，核函数也有很多种。经过空间转换后，我们可以在高维空间中解决线性问题，这也就等价于在低维空间中解决非线性问题。</p><p>SVM 优化中一个特别好的地方就是，所有的运算都可以写成<strong>内积(inner product 或者点积)</strong>的形式。向量的内积值的是两个向量相乘，之后得到单个标量或者数值。我们可以把内积运算替换成核函数，而不做简化处理。将内积替换为核函数的方式称为<strong>核技巧(kernal trick)</strong>。</p><p>核函数并不仅仅支持 SVM，很多其他的机器学习算法都用到核函数。</p><p><strong>径向基核函数</strong></p><p>径向基核函数是 SVM 中常用的一个核函数。径向基函数是一个采用向量作为自变量的函数，能够基于向量距离运算输出一个标量。这个距离可以是从 &lt;0,0&gt; 向量或者其他向量开始计算的距离。</p><p>我们想要使用径向基核函数，就要在原代码中进行适当的修改。</p><p><strong>核转换函数</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def kernalTrains(X, A, kTup):</span><br><span class="line">    m, n = shape(X)</span><br><span class="line">    K = mat(zeros((m,1)))</span><br><span class="line">    if kTup[0] == &apos;lin&apos;: </span><br><span class="line">        K = X * A.T</span><br><span class="line">    elif kTup[0] == &apos;rbf&apos;:</span><br><span class="line">        for j in range(m):</span><br><span class="line">            deltaRow = X[j,:] - A</span><br><span class="line">            K[j] = deltaRow * deltaRow.T</span><br><span class="line">        K = exp(K / (-1*kTup[1]**2))</span><br><span class="line">    else:</span><br><span class="line">        raise NameError(&apos;Wrong&apos;)</span><br><span class="line">    return K</span><br><span class="line"></span><br><span class="line">class optStructK:</span><br><span class="line">    def __init__(self,dataMatIn, classLabels, C, toler):  # Initialize the structure with the parameters </span><br><span class="line">        self.X = dataMatIn</span><br><span class="line">        self.labelMat = classLabels</span><br><span class="line">        self.C = C</span><br><span class="line">        self.tol = toler</span><br><span class="line">        self.m = shape(dataMatIn)[0]</span><br><span class="line">        self.alphas = mat(zeros((self.m,1)))</span><br><span class="line">        self.b = 0</span><br><span class="line">        self.eCache = mat(zeros((self.m,2)))</span><br></pre></td></tr></table></figure><p>在初始化方法结束后，矩阵K先被建立，然后再通过调用函数 kernalTrains() 进行填充。全局的K只需要计算一次。然后当想使用核函数时，就可以对它进行调用。</p><p>最后如果遇到一个无法识别的元组，程序会抛出一个异常。</p><p>同时，为了使用核函数，先前的 innerL() 与 calcEk() 函数需要进行修改</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">def innerLK(i, oS):</span><br><span class="line">    Ei = calcEk(oS, i)</span><br><span class="line">    if ((oS.labelMat[i]*Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i]*Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)):</span><br><span class="line">        j,Ej = selectJ(i, oS, Ei) #this has been changed from selectJrand</span><br><span class="line">        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();</span><br><span class="line">        if (oS.labelMat[i] != oS.labelMat[j]):</span><br><span class="line">            L = max(0, oS.alphas[j] - oS.alphas[i])</span><br><span class="line">            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])</span><br><span class="line">        else:</span><br><span class="line">            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)</span><br><span class="line">            H = min(oS.C, oS.alphas[j] + oS.alphas[i])</span><br><span class="line">        if L==H: print &quot;L==H&quot;; return 0</span><br><span class="line">        eta = 2.0 * oS.X[i,:]*oS.X[j,:].T - oS.X[i,:]*oS.X[i,:].T - oS.X[j,:]*oS.X[j,:].T</span><br><span class="line">        if eta &gt;= 0: print &quot;eta&gt;=0&quot;; return 0</span><br><span class="line">        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta</span><br><span class="line">        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)</span><br><span class="line">        updateEk(oS, j) #added this for the Ecache</span><br><span class="line">        if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001): print &quot;j not moving enough&quot;; return 0</span><br><span class="line">        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])#update i by the same amount as j</span><br><span class="line">        updateEk(oS, i) #added this for the Ecache                    #the update is in the oppostie direction</span><br><span class="line">        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[i,:].T - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.X[i,:]*oS.X[j,:].T</span><br><span class="line">        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[j,:].T - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.X[j,:]*oS.X[j,:].T</span><br><span class="line">        if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]): oS.b = b1</span><br><span class="line">        elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]): oS.b = b2</span><br><span class="line">        else: oS.b = (b1 + b2)/2.0</span><br><span class="line">        return 1</span><br><span class="line">    else: return 0</span><br><span class="line"></span><br><span class="line">def calcEkK(oS, k):</span><br><span class="line">    fXk = float(multiply(oS.alphas,oS.labelMat).T*(oS.X*oS.X[k,:].T)) + oS.b</span><br><span class="line">    Ek = fXk - float(oS.labelMat[k])</span><br><span class="line">    return Ek</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>SVM 是一个分类器。之所以成为“machine”是因为它会产生一个二值决策效果，即是一种决策“机”。SVM的泛化错误率低，这就意味着它有着良好的学习能力，且得到的结果具有很好的推广性。这些优点使得SVM十分流行。</p><p>SVM 的主要求解方式就是试图通过求解一个二次优化问题来最大化分类间隔。</p><p>核方法会将数据从一个低维空间映射到一个高维空间，可以将一个在低维空间中的非线性问题转换为高维空间下的线性问题求解。</p><p>SVM 是一个二类分类器。当用其解决多类问题时，则需要额外的方法对其进行扩展。这点我在本篇文章开头时的那个连接文章中的最后详细介绍了。</p><p>看，其实支持向量机与核函数主要就这些。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;支持向量机，support vector machines，简称SVM。一下全文基本都是用SVM一词。&lt;/p&gt;
&lt;p&gt;先将这个复杂的概念化小，个人理解，SVM就是很好的现成的分类器，这里的“现成”指的是分类器不加修改即可直接使用。同时，这就意味着数据上应用基本形式的SVM分类器就可以得到低错误率的结果。SVM能够对训练集之外的数据点做出很好的分类决策。&lt;/p&gt;
&lt;p&gt;本文主要介绍当前比较流行的SMO算法与核函数。&lt;/p&gt;
&lt;p&gt;关于具体的SVM算法可以参考我的这篇笔记&lt;br&gt;&lt;a href=&quot;http://www.saberismywife.com/2016/12/19/Machine-Learning-7/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;SVM and Kernels&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;原理–基于最大间隔分隔数据&quot;&gt;&lt;a href=&quot;#原理–基于最大间隔分隔数据&quot; class=&quot;headerlink&quot; title=&quot;原理–基于最大间隔分隔数据&quot;&gt;&lt;/a&gt;原理–基于最大间隔分隔数据&lt;/h2&gt;
    
    </summary>
    
      <category term="ML" scheme="https://saberda.github.io/categories/ML/"/>
    
    
      <category term="Machine Learning" scheme="https://saberda.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
</feed>
