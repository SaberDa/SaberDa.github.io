<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <meta name="description" content="阿尔托利亚是我老婆~" />
  

  
  
  
  
  
  
  <title>PGM(一)--Introduction and Overview | SaberDa的幻想乡</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="&amp;#x540C;&amp;#x6837;&amp;#xFF0C;&amp;#x8FD9;&amp;#x4E2A;&amp;#x7CFB;&amp;#x5217;&amp;#x4E5F;&amp;#x662F;&amp;#x8BFE;&amp;#x7A0B;&amp;#x7B14;&amp;#x8BB0;&amp;#xFF0C;coursera&amp;#x4E0A;Stanford&amp;#x7684;PGM
BackgroundRequired
basic probability theory
Its goin">
<meta property="og:type" content="article">
<meta property="og:title" content="PGM(一)--Introduction and Overview">
<meta property="og:url" content="https://saberda.github.io/2017/01/29/PGM-1/index.html">
<meta property="og:site_name" content="SaberDa的幻想乡">
<meta property="og:description" content="&amp;#x540C;&amp;#x6837;&amp;#xFF0C;&amp;#x8FD9;&amp;#x4E2A;&amp;#x7CFB;&amp;#x5217;&amp;#x4E5F;&amp;#x662F;&amp;#x8BFE;&amp;#x7A0B;&amp;#x7B14;&amp;#x8BB0;&amp;#xFF0C;coursera&amp;#x4E0A;Stanford&amp;#x7684;PGM
BackgroundRequired
basic probability theory
Its goin">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/1.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/2.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/3.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/4.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/5.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/6.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/7.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/8.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/9.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/10.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/11.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/12.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/13.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/14.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/15.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/16.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/17.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/18.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/19.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/20.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/21.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/22.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/23.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/24.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/25.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/26.png">
<meta property="og:image" content="https://saberda.github.io/2017/01/29/PGM-1/27.png">
<meta property="og:updated_time" content="2017-01-29T09:17:01.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PGM(一)--Introduction and Overview">
<meta name="twitter:description" content="&amp;#x540C;&amp;#x6837;&amp;#xFF0C;&amp;#x8FD9;&amp;#x4E2A;&amp;#x7CFB;&amp;#x5217;&amp;#x4E5F;&amp;#x662F;&amp;#x8BFE;&amp;#x7A0B;&amp;#x7B14;&amp;#x8BB0;&amp;#xFF0C;coursera&amp;#x4E0A;Stanford&amp;#x7684;PGM
BackgroundRequired
basic probability theory
Its goin">
<meta name="twitter:image" content="https://saberda.github.io/2017/01/29/PGM-1/1.png">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
  <script src='//push.zhanzhang.baidu.com/push.js'></script>
</head>
<body class="home blog custom-background custom-font-enabled single-author">
  <div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="SaberDa的幻想乡" rel="home">SaberDa的幻想乡</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">　　iOS/ ML　　|　　二次元　　|　　saberda@qq.com　　|　　虽则如云，匪我思存</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives">所有文章</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">主页</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/编程/">编程</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/iOS/">iOS</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/ML/">ML</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/PGM/">PGM</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/MAC/">MAC</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/御宅文化/">御宅文化</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/LIFE/">LIFE</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-PGM-1" class="post-PGM-1 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      PGM(一)--Introduction and Overview
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://saberda.github.io/2017/01/29/PGM-1/" data-id="cj98sd8iz002ckvb33gx5yici" class="leave-reply bdsharebuttonbox" data-cmd="more">Share</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <pre><code>&#x540C;&#x6837;&#xFF0C;&#x8FD9;&#x4E2A;&#x7CFB;&#x5217;&#x4E5F;&#x662F;&#x8BFE;&#x7A0B;&#x7B14;&#x8BB0;&#xFF0C;coursera&#x4E0A;Stanford&#x7684;PGM
</code></pre><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p><strong>Required</strong></p>
<p><u><em>basic probability theory</em></u></p>
<p>Its going to be really hard to do this class without some understanding of basic probability theory.<br>This also have to be very advance stuff we&#x2019;re talking about things like independence and days role and just basics of discreet distributions.</p>
<a id="more"></a>
<p><u><em>some programming</em></u></p>
<p>The programming assignments will require that you have some experience programming before because this is not programming class.</p>
<p><u><em>some algorithms and data structures</em></u></p>
<p>And because this class merges ideas from both probability theory and computer science, it&#x2019;s really important you have some background in algorithms and data structures.</p>
<p><strong>Recommended</strong></p>
<p><u><em>machine learning and simple optimization</em></u></p>
<p>Recommended but not strict necessary and we certainly don&#x2019;t require it.And we give you the background as we go, is a little bit of experience for how some machine learning, may be some simple optimization like gradient descent, nothing very sophisticated. </p>
<p><u><em>Matlab or Octave</em></u></p>
<p>And it will be helpful to have some experience and Matlab or Octave.</p>
<h2 id="What-are-probabilistic-graphical-models"><a href="#What-are-probabilistic-graphical-models" class="headerlink" title="What are probabilistic graphical models?"></a>What are probabilistic graphical models?</h2><p>Probabilistic Graphical Models are a bit of a mouthful, so before we define them, let&#x2019;s first figure out what they might be used for.</p>
<p>So, one example application, which in fact is the one where probabilistic graphical models, or PGMs as they&#x2019;re called, first made its way into computer science and artificial intelligence, and that as medical diagnosis.</p>
<p>Consider a doctor who&#x2019;s faced with a patient. The doctor has a fair amount of information at her disposal when she looks up at a patient.</p>
<p><img src="/2017/01/29/PGM-1/1.png" alt=""></p>
<p>A very different application where PGMs have also been used is that of image segmentation. </p>
<p>Here we might have an image such as this that has thousands or even hundreds of thousands of pixels, and what we&#x2019;d like to do is we&#x2019;d like to figure out what each pixel corresponds to. </p>
<p>For example, if we break up the image into these fairly larger regions to have less stuff to reason about, we want to figure out which of these corresponds to glass, sky, cow, or horse.</p>
<p><img src="/2017/01/29/PGM-1/2.png" alt=""> </p>
<p>What do these two problem have in common?</p>
<p>First, they have very large number of variables that we have to reason about.</p>
<p>In the context of a doctor, it&#x2019;s all these predisposing factors, text result, possible diseases, and so on. And in the context of the image segmentation, it&#x2019;s the labels for these different pixels or these larger regions called superpixels. </p>
<p>The second thing that these applications have in common is that fundamentally there is going to be significant uncertainty about the right answer, no matter how clever the algorithms that we design.</p>
<hr>
<p>So, probabilistic graphical models are a framework for dealing with this kind of application.So, let&#x2019;s first understand what each of these words mean in the context of this framework.</p>
<p>So first, let&#x2019;s consider the word models.</p>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><p>The model is a declarative representation of our understanding of the world.</p>
<p>So it is a representation within the computer that captures our understanding of what these variables are and how they interact with each other. </p>
<p>And the fact that it&#x2019;s declarative means that the representation stands on its own, which means that we can look into it and make sense of it aside from any algorithm that we might choose to apply on.</p>
<p>So, why is that important? It&#x2019;s important because that same representation, that same model can then be used in the context of one algorithm that answers any one kind of question. Or other algorithms that might answer different kinds of questions or the same question in more efficient ways, or that make different trade-offs between accuracy and complicational cause.</p>
<p>The other advantage of having a stand alone model is that we can separate out the construction of the model from the algorithms that are used to reason over it.</p>
<p>So, we can construct methodologies that elicit these models from a human expert or ones that learn it from historical data using statistical machine learning techniques or a combination of the two. And once again, the separation between the algorithm and the model and the learning in the model allows us to tackle each of these problems separately.</p>
<p><img src="/2017/01/29/PGM-1/3.png" alt=""></p>
<p>So, that was the word model, what about probabilistic?</p>
<h2 id="probabilistic"><a href="#probabilistic" class="headerlink" title="probabilistic"></a>probabilistic</h2><p><strong>Partial knowledge of state of the world</strong></p>
<p>The word probabilistic is in there because these models are designed to help us deal with large amounts of uncertainty. So uncertainty comes in many forms and for many different reasons.</p>
<p>So, first it comes because we just have partial knowledge of the state of the world, for example the doctor doesn&#x2019;t get to measure every symptom or every test result and she&#x2019;s certainly uncertain<br>about the diseases that the patient has.</p>
<p><strong>Noisy obsrevations</strong></p>
<p>Uncertainty comes because of noisy observations.</p>
<p>So even when we get to observe certain things like blood pressure, those observations are often subject to significant amounts of noise.</p>
<p><strong>Phenomena not covered by our model</strong></p>
<p>Uncertainty also comes in because of modeling limitations, so we&#x2019;re going to have phenomena that are just not covered by our model.</p>
<p>All sorts of different obscure diseases for example that might cause<br>the same set of symptoms. It&#x2019;s impossible for us to write down the model that is so detailed that includes every possible contingency in every possible factor.</p>
<p>And so you&#x2019;re going to have uncertainty and variability that is simply due to modeling limitations.</p>
<p><strong>Inherent stochasticity</strong></p>
<p>And finally, some people would argue that the world is inherently stochastic. Certainly, if you go down to the quantum level, that&#x2019;s true.</p>
<p>But even at a higher level, the modeling limitations of complicated systems are such that one might as well view the world as inherently stochastic.</p>
<h2 id="Probability-theory"><a href="#Probability-theory" class="headerlink" title="Probability theory"></a>Probability theory</h2><p>Probability theory is a framework that allows us to deal with uncertainty in ways that are principled and that bring to bear important and valuable tools.</p>
<p><strong>Declarative representation with clear semantics</strong></p>
<p>So first, probabilistic models provide us again this word declarative.</p>
<p>A declarative representation, that is stand alone, where you could look at a probability distribution and it has clear semantics that represent our uncertainty about different state that the world might be in.</p>
<p><strong>Powerful reasoning patterns</strong></p>
<p>It also provides us with a toolbox comprising powerful reasoning patterns that include, for example, conditioning on new forms of evidence or decision making under uncertainty. </p>
<p><strong>Established learning methods</strong></p>
<p>And because of the intricate connection between probability theory and statistics, you can bring to bear a range of powerful learning methodologies from statistical learning to allow us to learn these models effectively from historical data. Avoiding the need for a human to specify every single aspect of the model by hand.</p>
<h2 id="Complex-systems"><a href="#Complex-systems" class="headerlink" title="Complex systems"></a>Complex systems</h2><p>Finally, the word graphical.</p>
<p>The word graphical is here from the perspective of computer science, because probabilistic graphical models are a synthesis between ideas from probability theory in statistics and ideas from computer science.</p>
<p>And the idea here is to use the connections computer science, specifically that of graphs to allow us to represent systems that are very complicated that involved large numbers of variables.</p>
<p>And we&#x2019;d already seen those large number of variable in both of the applications that we use examples. Both in the medical example, as well as in the image segmentation example.</p>
<p><img src="/2017/01/29/PGM-1/4.png" alt=""></p>
<p>And so in order to capture probability distributions over spaces involving such a large number of factors, we need to have probability distributions over what are called random variables.</p>
<p>And so the focus of this class and what we&#x2019;ll do for most of it is to think about the world as represented by a set of random variables, X1 up to Xn, each of which captures some facet of the world.</p>
<p>So, one symptom that may be present or absent, or a test result that might have a continuous set of possible values or a pixel that might have one of several labels.</p>
<p>So each of these is a random variable and our goal is to capture our uncertainty about the possible states of the world in terms of their probability distribution or what&#x2019;s called a joint distribution over the possible assignments to the set of random variables.</p>
<p>Now, the important thing to realize when looking at this, is that even in the simplest case where each of these is, say, binary valued, which is not on the case, but say just for sake of the argument. If you have n binary value variable then this is a distribution, Over to to the n possible states of the world. One for each possible assignment.</p>
<p>And so we have to deal with objects that are intrinsically, exponentially large. And our only way to do that is by exploiting data structures that encode, that use ideas from computer science in this case to exploit the structure and distribution and represent and manipulate it in an effective way.</p>
<h2 id="Graphical-models"><a href="#Graphical-models" class="headerlink" title="Graphical models"></a>Graphical models</h2><p>Let&#x2019;s look at a couple of very simple examples, so here&#x2019;s a toy Bayesian network, one that will accompany us through much of the first part of this course.</p>
<p><img src="/2017/01/29/PGM-1/5.png" alt=""></p>
<p>A Bayesian network is one of the two main classes of probabilistic graphical models, and it uses a directed graph as the intrinsic representation.</p>
<p>In this case, remember we had a set of random variables X1 up to Xn. The random variables are represented by nodes in the graph. </p>
<p>So, to take a look at this very simple example which we&#x2019;ll discuss again later, here we have a situation where we have a student who takes a course and gets a grade in the course, and so that&#x2019;s one of our random variables. We have other random variables that are also related to that. For example, the intelligence of the student&#x2019;s in the course, the difficulty of the course. And others that might also be of interest, for example the quality of the recommendation letter that the student gets in the course which is dependent on things, perhaps the students&#x2019; grade, and these score that the students might receive on the SAT.</p>
<p>So, this is a representation of a probability distribution, in this case over these five random variables. And the edges in this graph represent the probabilistic connections between those random variables in a way that is very formal as we&#x2019;ll define later on.</p>
<p>The other main class of probabilistic graphical model is what&#x2019;s called the Markov network and that uses an undirected graph.</p>
<p><img src="/2017/01/29/PGM-1/6.png" alt=""></p>
<p>And in this case, we have an undirected graph over 4 random variables A, B, C, D and will give an example of this type of network maybe a little bit later on.</p>
<p>So these were toy examples, here are some real examples of the same type of framework.</p>
<p>So, this is a real Bayesian network.</p>
<p><img src="/2017/01/29/PGM-1/7.png" alt=""></p>
<p>It&#x2019;s a network that&#x2019;s actually called CPCS, it&#x2019;s a real medical diagnosis network. It was designed at Stanford University for the purpose of diagnosis of internal diseases and it has 480 some nodes, and a little bit over 900 edges. And it was used for diagnosing internal diseases by physicians here.</p>
<p>Another real graphical model, in this case on the Markov network side, is one that&#x2019;s used for the image segmentation tasks that we talked about before.</p>
<p><img src="/2017/01/29/PGM-1/8.png" alt=""></p>
<p>Here, the random variables represent the labels of pixels or superpixels. So, one per each superpixel say. And the edges represent, again probabilistic relationships between the label of a pixel and the label of an adjacent pixel since these are likely to be related to each other.</p>
<h2 id="Graphical-representation"><a href="#Graphical-representation" class="headerlink" title="Graphical representation"></a>Graphical representation</h2><p><strong>Intuitive &amp; compact data structure</strong></p>
<p>So to summarize, the graphical representation gives us an intuitive and compact data structure for capturing these high dimensional probability distributions.</p>
<p><strong>Efficient reasoning using general-purpose algorithms</strong></p>
<p>It provides us at the same time, as we&#x2019;ll see later in the course, a suite of methods for efficient reasoning, using general purpose algorithms that exploit the graphical structure.</p>
<p><strong>Sparse parameterization</strong></p>
<p>&#x2003; feasible elicitation<br>&#x2003; learning from data</p>
<p>And because of the way in which the graph structure encodes the parameterization of the probability distribution, we can represent these high-dimensional probability distribution efficiently using a very small number of parameters.</p>
<p>Which allows us though feasible elicitation by hand from an expert as well as automatically learning from data. And in both cases a reduction in the number of parameters is very valuable.</p>
<h2 id="Many-applications"><a href="#Many-applications" class="headerlink" title="Many applications"></a>Many applications</h2><p>This framework is a very general one and has been applied to a very broad range of applications and I&#x2019;m not going to list all of the ones on the slide, and there is many others that I could have put on the slide had there been more space.</p>
<p><img src="/2017/01/29/PGM-1/9.png" alt=""></p>
<p>Let me just very briefly mention a few of them on subsequent slides.</p>
<p>So, we&#x2019;ve already discussed the image segmentation. So, just to motivate the benefits of the PGM framework in this context, let&#x2019;s look at these two images as an example.</p>
<p>Here is the original images, this is the division of these images into what I mentioned were called <strong>superpixels</strong>, which are these sort of slightly larger coherent regions.</p>
<p><img src="/2017/01/29/PGM-1/10.png" alt=""></p>
<p><img src="/2017/01/29/PGM-1/11.png" alt=""></p>
<p>And this is what you get if you apply a state of the art machine learning framework. Individual super pixels separately. </p>
<p>So, just trying to classify each superpixel separately and you can see that you get, especially in the case of the cow, a total mess with different superpixels having drastically different labels.</p>
<p>You can&#x2019;t even see the cow in this segmented image. Whereas if you construct the probabilistic graphical model to capture the global structure of the scene and the correlations, the probabilistic relationships between these superpixels. You end up with a much more coherent segmentation that captures the structure of the scene.</p>
<p><img src="/2017/01/29/PGM-1/12.png" alt=""></p>
<h2 id="Textual-information-extraction"><a href="#Textual-information-extraction" class="headerlink" title="Textual information extraction"></a>Textual information extraction</h2><p>A very different application is one of textual information extraction.</p>
<p>Where we might have an article from, say, a newspaper and we&#x2019;d like to take this unstructured data and convert it into a structured form, where we have some representation of the people locations, organizations and perhaps relationships.</p>
<p><img src="/2017/01/29/PGM-1/13.png" alt=""></p>
<p>So one such task might be take this kind of sentence and recognize that these two words together form a person, which might not be that hard, given the presence of the word, missus. But, this is a little bit harder because Green also a word and yet, we want to identify it as a person. We might want to then infer that this is a location and perhaps that this is an organization.</p>
<p><img src="/2017/01/29/PGM-1/14.png" alt=""></p>
<p>It turns out that the state of the art methodology for solving this problem is as a probabilistic classical model where we have a variable for each word that encodes the label for that word.</p>
<p><img src="/2017/01/29/PGM-1/15.png" alt=""></p>
<p>For example, here we have the beginning of a person unit and an intermediate label for the person unit. And here is another person unit whereas, here in this variable is we would like to label it as a location. But we would like to capture, importantly, the correlations between both adjacent words as well as between non-adjacent words by using this occurrence of the word Green to infer this occurrence of the word Green is also a name.</p>
<p><img src="/2017/01/29/PGM-1/16.png" alt=""></p>
<h2 id="Multi-Sensor-integration-Traffic"><a href="#Multi-Sensor-integration-Traffic" class="headerlink" title="Multi-Sensor integration: Traffic"></a>Multi-Sensor integration: Traffic</h2><p>A very different example all together is one that implicates data from multiple sensors. </p>
<p>This occurs in many applications, one such is for integrating data related to traffic from both sensors that we might have in the road or on the top of bridges, weather, information, incident report that we might get in some form. </p>
<p>We&#x2019;d like to take all this together and use a model that as it happens was learned from data. </p>
<p>And that model is then used to <strong>predict</strong> both <strong>current and future road speed</strong> including not only on roads that where we have sensors that measure traffic, but even more interestingly, on roads where traffic <strong>wasn&#x2019;t measured</strong>. </p>
<p>And it turns out that this was a very successful application that was fielded in several large cities with very good results.</p>
<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>So, let&#x2019;s conclude this introduction with an overview of what we&#x2019;ll learn in the class.</p>
<p>So, we will cover three main pieces related to probabilistic graphical models. We&#x2019;ll cover representation of PGMs, including both directed and undirected representation.</p>
<p>We&#x2019;ll also talk about higher level structures that allows to encode more complicated scenarios. Including ones that involve temporal structure as well as ones that involve repeated or relational structure, specifically a class of model called plate model.</p>
<p>We&#x2019;ll talk about inference or reasoning using these models. Covering both exact reasoning, where exact probabilities are the outcome, as well as approximate methods that provide the different trade off regarding accuracy in computation.</p>
<p>And we&#x2019;ll also talk about how this class of models can be used for decision making under uncertainty.</p>
<p>And finally, we&#x2019;ll talk about how you might learn these models from historical statistical data.</p>
<p>And we&#x2019;ll talk about learning both parameters, as well as structure of these models automatically. And dealing with both the somewhat simpler scenario where we have complete data that is all of the variables are always observed as well as the more difficult case where some of the variables might not be observed all the time, or perhaps not at all. </p>
<p>And that introduces an interesting set of complications but also a wealth of very exciting applications as we&#x2019;ll see.</p>
<h2 id="probability-distribution"><a href="#probability-distribution" class="headerlink" title="probability distribution"></a>probability distribution</h2><p>So before we get into the details of  probabilistic graphical models, we need to talk a little bit about what a  probability distribution is, just so we have a shared vocabulary. So, let&#x2019;s start with a very simple example of a joint distribution.</p>
<h2 id="Joint-distribution"><a href="#Joint-distribution" class="headerlink" title="Joint distribution"></a>Joint distribution</h2><p>One that is going to be extended in examples later on in the, in other parts  of the course. and let&#x2019;s start with an example that  involves just three random variables.</p>
<p><img src="/2017/01/29/PGM-1/17.png" alt=""></p>
<p>This is what I call the student example  and you have a student who has, who can be described, in this case, by a variable  representing his intelligence. And that could be high or low. </p>
<p>The student is taking a class. The class might be difficult or not so  this random variable, B. So, the random variable I has two values.  Difficulty variable also has two values and then, there is the grade that the  student gets in the course, and that has three values.</p>
<p>In this case, we&#x2019;re going to assume A, B, and C.</p>
<p>Now here&#x2019;s an example, joint distribution over this over this set of three random  variables. So this is an example of P of I, D, G.</p>
<p><img src="/2017/01/29/PGM-1/18.png" alt=""></p>
<p>It&#x2019;s a joint distribution. And let&#x2019;s think about how many entries  are in such a joint distribution. Well since we have three variables and we  want to, we need to represent the probability of every combination of  values for each of these three variables, and so we have 2 <em> 2 </em> 3 possible combinations. For a total of twelve possible values that we need to assign a probability to. So there&#x2019;s twelve total parameters in  this probability and I&#x2019;m going to introduce a notion of independence  parameters which we&#x2019;re going to talk about later, as well.</p>
<p>Independent parameters are parameters whose value is not completely determined by the value of other parameters.</p>
<p>So in this case, because this thing is a probability distribution, we know that all of the numbers here on the right have to sum to one. And therefore if you tell me eleven out of the twelve, I know what the twelfth is, and so the number of independent parameters is eleven. And we&#x2019;ll see that, that is a useful notion later on when we start evaluating the relative expressive power of different probability distributions.</p>
<p><strong>What are things that we can do with probability distributions?</strong></p>
<p>Well, one important thing that we can do  is condition the probability distribution on a particular observation. </p>
<p>So, for example assume that we observe that the student got an A. And so we have now an assignment to the variable G which is G1. And that immediately eliminates all possible assignments, but they&#x2019;re not consistent, with my observations.</p>
<p>So everything but the G1 observations, okay? And so that gives me a reduced<br>probability distribution, and so this is an operation that&#x2019;s called <strong>reduction</strong>.I&#x2019;ve taken the probability distribution, I&#x2019;ve reduced away stuff that is not consistent with what I&#x2019;ve observed.</p>
<p>Now, that by itself doesn&#x2019;t give me a probability distribution, because notice that these numbers no longer sum to one, because they summed to one before I threw out a bunch of stuff.</p>
<p><img src="/2017/01/29/PGM-1/19.png" alt=""></p>
<p>So what I do in order to get a probability distribution, what I do is I<br>take this &#x2014;- Normalized measure. An indication the word measure indicates<br>that it&#x2019;s a form of distribution but the fact that it&#x2019;s un-normalized means that it doesn&#x2019;t sum to one, it doesn&#x2019;t normalize to one.</p>
<p>So this un-normalized measure if we want to turn it into a probability distribution, the obvious thing to do is to normalize it.</p>
<p>And so what we&#x2019;re going to do is take all of these entries and we&#x2019;re going to sum them up. And that&#x2019;s going to give us a number, which in this case is 0.447. And we can now divide each of these by 0.447.</p>
<p>And that&#x2019;s going to give us a normalized distribution. Which in this case corresponds to the probability of I, D given G1. </p>
<p><img src="/2017/01/29/PGM-1/20.png" alt=""></p>
<p>So that&#x2019;s a way of taking an un-normalized measure, and turning it into a normalizing a normalized probability distribution. We&#x2019;ll see that this operation is one of the more important ones that we were using, throughout the course.</p>
<p><strong>Marginalization</strong></p>
<p>Okay, the final operation I&#x2019;m going to talk about regarding probability distribution is the operation of marginalization, and that is an operation that takes a probability distribution over a larger subset of variables and produces a probability distribution over a subset of those.</p>
<p><img src="/2017/01/29/PGM-1/21.png" alt=""></p>
<p>So in this case we have a probability distribution over IND. And say that we want to marginalize I which means we&#x2019;re going to basically sum up we&#x2019;re going to throw away, I&#x2019;m going to restrict the tension to D.</p>
<p>And so what that does.</p>
<p>For example. If I want to compute the probability of d0. I&#x2019;m going to add up both of the entries that have the d0, associated with them. And that&#x2019;s, the one corresponding to I0, and the one corresponding to I1. And that&#x2019;s the marginalization of this probability distribution.</p>
<h2 id="Factors"><a href="#Factors" class="headerlink" title="Factors"></a>Factors</h2><p>A critical building block in a lot of what we&#x2019;ll do both in terms of definition probability distributions and in terms of manipulating them for inference in the notion of a factor. So let&#x2019;s define what a factor is and the kind&#x2019;s of operations that you can do on factors.</p>
<p>So a factor really is a function, or a table. It takes a bunch of arguments.</p>
<p><img src="/2017/01/29/PGM-1/22.png" alt=""></p>
<p>In this case, a set of random variables X1 up to XK, and just like any function it gives us a value for every assignment to those random variables. So it takes all possible assignments in the cross products space of of X1 up to XK. That is all possible combinations of assignments and in this case it gives me a real value for each such combination.</p>
<p>And the set of variables X1 up to XK is called the scope of the factor. That is it&#x2019;s the set of arguments, that a factor takes. </p>
<p>Let&#x2019;s look at some examples of factors. We&#x2019;ve already seen a joint distribution, a joint distribution is a factor. For every combination for example here of the variables I, D, and G, it gives me a number. As it happens this number is a probability. And it happens that it sums to one but that doesn&#x2019;t matter. What&#x2019;s what&#x2019;s, what&#x2019;s important is that for every value of I, D, and G, a combination of values, I get a number. That&#x2019;s why it&#x2019;s a factor.</p>
<p>Here&#x2019;s a different factor and a normalized measure is a factor also. In this case we have a factor such as the probability of ID, G1 and notice that in this case the scope of the factor is actually I and D. Because there is no dependence of the factor on the value of the variable G because the variable G in this case is constant. So this is a factor who&#x2019;s scope is IND.</p>
<p>Finally, a type of factor that we will use extensively is what&#x2019;s called a <strong>conditional probability distribution</strong>, typically abbreviated <strong>CPD</strong>.</p>
<p><img src="/2017/01/29/PGM-1/23.png" alt=""></p>
<p>This as it happens is a CPD that&#x2019;s written as a table. Although, that&#x2019;s not necessarily the case. and this is a CPD that gives us the conditional probability of the variable G given I and D, so what does that mean? It means for every combination of values to the variables I and D, we have a probability distribution over G.</p>
<p>So, for example, if I have an intelligent student I a difficult class, which is this last line over here, this tells us that the probability of getting an A is 0.5, B is 0.3 and a C is 0.2 and as we can see, these numbers sum to 1 as they should because this is a probability distribution over G for this particular condition and context.</p>
<p>And you can easily verify that this is, that this is true for all of the other lines in this table. So this is again, a particular type of, of factor, one that satisfies certain constraints, in this case that each row sums to 1.</p>
<p><strong>General Factors</strong></p>
<p>Now this is, these are, the factors that we&#x2019;re dealing with will not always correspond to probabilities. So here is an example of a general factor that, that really doesn&#x2019;t match in any way to probability because the numbers aren&#x2019;t even in the range 0,1.</p>
<p><img src="/2017/01/29/PGM-1/24.png" alt=""></p>
<p>As we&#x2019;ll see these kinds of factors are nonetheless useful. This is a factor whose scope is the set of variable A, B. And it still goes need a real valued number for each of those for each assignment A and B.</p>
<p><strong>Factor Product</strong></p>
<p>Some operations that we&#x2019;re going to do on factors. One of the most common operations is what&#x2019;s called factor product.</p>
<p><img src="/2017/01/29/PGM-1/25.png" alt=""></p>
<p>It&#x2019;s taking two factors say &#x3A6;1 and &#x3A6;2 and multiplying them together. So let&#x2019;s think what that means. Here we have a factor &#x3A6;1. It has a scope of ab, phi two has a scope of bc. And what we&#x2019;re doing is we&#x2019;re kind of like multiplying a function f of xy times g of yz you&#x2019;re going to get a function that is of all three arguments xyz.</p>
<p>So in this case we have a factor whose scope is A, B, and C. And if we want to figure out. Oops, that didn&#x2019;t come out good. If we want to figure out, for example, the value of the row A1, B1, C1. It&#x2019;s going to come by taking the A1, B1 row from here. The B1 C1 row from here, and multiplying them together. So we&#x2019;re going to get 0.25.</p>
<p>So this is effectively taking the functions or the tables, and just multiplying them together.</p>
<p><strong>Factor Marginalization</strong></p>
<p>Another important operation is factor marginalization factor marginalization is very similar to in fact identical to the marginalization of probability distributions so here we accept that it doesn&#x2019;t have to be a probability distribution</p>
<p><img src="/2017/01/29/PGM-1/26.png" alt=""></p>
<p>So for example if we have a factor here who&#x2019;s scope is A, B, and C. And we want to, marginalize out B to get a, a factor whose scope is AC, what we&#x2019;re going to be doing is again, taking both possible, values of B in this case there&#x2019;s on, because B is binary, there&#x2019;s only two values and we add them up, In order to get the entry for A1, C1 so 00.25 + 0.08.</p>
<p>In the same, all the other rows in this in this table are acquired. are computed in exactly the same way, from the corresponding rows, in the, original larger factor.</p>
<p><strong>Factor Reduction</strong></p>
<p>Finally, factor reduction, again, very similar to the context of probability distributions, we want to reduce.</p>
<p><img src="/2017/01/29/PGM-1/27.png" alt=""></p>
<p>For example, to the context C1, so we&#x2019;re going to only focus on the rows that have the value C equals C1, and that&#x2019;s going to give us a reduced factor. Which only has c1 and once again the scope of this factor is AB because there&#x2019;s no longer any dependence on C. That&#x2019;s basically the final operation.</p>
<p><strong>Why Factors</strong></p>
<p>It turns out that factors are the fundamental building block in defining these distributions and high dimensional spaces.</p>
<p>That is the way in which we&#x2019;re going to define an exponentially large probability distribution over N random variables is by taking a bunch of little pieces and putting them together by multiplying factors in order to define these high dimensional probability distributions.</p>
<p>It turns out also that the same set of basic operations that we use to define the probability distributions in these high dimensional spaces are also what we use for manipulating them in order to give us a set of basic inference algorithms.</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2017/01/29/PGM-1/">
    <time datetime="2017-01-29T04:35:51.000Z" class="entry-date">
        2017-01-29
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/PGM/">PGM</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Probabilistic-Graphical-Models/">Probabilistic Graphical Models</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2017/02/13/iOS-protocol/" rel="prev"><span class="meta-nav">←</span> iOS-Protocol</a></span>
    
    
        <span class="nav-next"><a href="/2016/12/31/2016年终总结/" rel="next">Legend 2016 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s" />
        <input type="submit" id="searchsubmit" value="搜索" />
    </div>
</form></aside>
  
    
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/LIFE/">LIFE</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/MAC/">MAC</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML/">ML</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/PGM/">PGM</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/iOS/">iOS</a><span class="category-list-count">22</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/御宅文化/">御宅文化</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">58</span></li></ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2017/10/27/CNN-Structure-2/">CNN_Structure_2</a>
          </li>
        
          <li>
            <a href="/2017/10/25/CNN-Structure/">CNN Structure -- 1</a>
          </li>
        
          <li>
            <a href="/2017/10/16/MapReduce-RDBMs/">MapReduce VS RDBMs</a>
          </li>
        
          <li>
            <a href="/2017/09/28/Cloud-Service-Models/">Cloud Service Models</a>
          </li>
        
          <li>
            <a href="/2017/09/18/Cloud-Introduction/">Cloud Introduction</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cloud-Computing/">Cloud Computing</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cocoapods/">Cocoapods</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/HTML/">HTML</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a><span class="tag-list-count">15</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matlab/">Matlab</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Networks/">Neural Networks</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Graphical-Models/">Probabilistic Graphical Models</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIColor/">UIColor</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIView/">UIView</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIView-圆角/">UIView-圆角</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vim/">Vim</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bug/">bug</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/openCV/">openCV</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/二维码/">二维码</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/指针/">指针</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/操作系统/">操作系统</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据库/">数据库</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据结构/">数据结构</a><span class="tag-list-count">20</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a><span class="tag-list-count">9</span></li></ul>
    </div>
  </aside>

  
    
<div class="widget tag">
<h3 class="title">blogroll</h3>
<ul class="entry">


<li><a href="https://github.com/" target="_blank">github</a></li>


<li><a href="http://www.jianshu.com/users/41cd7711ed44/latest_articles" target="_blank">我的简书主页</a></li>

</ul>
</div>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2017 SaberDa
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
</footer>
    <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

<script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
</body>
</html>