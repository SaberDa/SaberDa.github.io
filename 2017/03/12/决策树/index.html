<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
  <meta name="description" content="阿尔托利亚是我老婆~">
  

  
  
  
  
  
  
  <title>决策树 | SaberDa的幻想乡</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="决策树最经常使用的数据挖掘算法 其只要优势在于数据形式非常容易理解，分为判断模块(decision block)与终止模块(terminating block) 决策树  优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据 缺点：可能会产生过度匹配问题 适用数据类型：数值型和标称型">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树">
<meta property="og:url" content="https://saberda.github.io/2017/03/12/决策树/index.html">
<meta property="og:site_name" content="SaberDa的幻想乡">
<meta property="og:description" content="决策树最经常使用的数据挖掘算法 其只要优势在于数据形式非常容易理解，分为判断模块(decision block)与终止模块(terminating block) 决策树  优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据 缺点：可能会产生过度匹配问题 适用数据类型：数值型和标称型">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://saberda.github.io/2017/03/12/决策树/1.png">
<meta property="og:image" content="https://saberda.github.io/2017/03/12/决策树/2.png">
<meta property="og:image" content="https://saberda.github.io/2017/03/12/决策树/3.png">
<meta property="og:image" content="https://saberda.github.io/2017/03/12/决策树/5.png">
<meta property="og:updated_time" content="2017-03-12T06:43:21.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树">
<meta name="twitter:description" content="决策树最经常使用的数据挖掘算法 其只要优势在于数据形式非常容易理解，分为判断模块(decision block)与终止模块(terminating block) 决策树  优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据 缺点：可能会产生过度匹配问题 适用数据类型：数值型和标称型">
<meta name="twitter:image" content="https://saberda.github.io/2017/03/12/决策树/1.png">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
  <script src="//push.zhanzhang.baidu.com/push.js"></script>
</head></html>
<body class="home blog custom-background custom-font-enabled single-author">
  <div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="SaberDa的幻想乡" rel="home">SaberDa的幻想乡</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">　　iOS/ ML　　|　　二次元　　|　　saberda@qq.com　　|　　明明是己不由心 怎能说身不由己</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives">所有文章</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">主页</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/编程/">编程</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/iOS/">iOS</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/ML/">ML</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/PGM/">PGM</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/MAC/">MAC</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/御宅文化/">御宅文化</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/LIFE/">LIFE</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-决策树" class="post-决策树 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      决策树
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://saberda.github.io/2017/03/12/决策树/" data-id="cjoqmj911004yr0b3izont1tc" class="leave-reply bdsharebuttonbox" data-cmd="more">Share</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><p>最经常使用的数据挖掘算法</p>
<p>其只要优势在于数据形式非常容易理解，分为判断模块(decision block)与终止模块(terminating block)</p>
<p><strong>决策树</strong></p>
<ul>
<li>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据</li>
<li>缺点：可能会产生过度匹配问题</li>
<li>适用数据类型：数值型和标称型</li>
</ul>
<a id="more"></a>
<p>该决策树算法能够读取数据集合</p>
<p>决策树的一个重要任务就是了解数据中所蕴含的知识信息，因此其可以使用不熟悉的数据集合并从中提取出一些列规则</p>
<p><a href="https://github.com/SaberDa/jueCeShu" target="_blank" rel="noopener">本文全部代码以及测试文档</a></p>
<h2 id="决策树的一般流程"><a href="#决策树的一般流程" class="headerlink" title="决策树的一般流程"></a>决策树的一般流程</h2><ul>
<li>收集数据：可以使用任何方法</li>
<li>准备数据：树构造算法只使用于标称型数据，因此数值型数据必须离散化</li>
<li>分析数据：可以使用任何方法，构造树完成后，我们应该检查图形是否符合预期</li>
<li>训练算法：构造树的数据结构</li>
<li>测试算法：使用经验树计算错误率</li>
<li>使用算法：此步骤可以试用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义</li>
</ul>
<p>构造决策树时需要解决的第一个问题就是：当前数据集上哪个特征在划分数据类型时起决定性作用</p>
<h2 id="计算熵"><a href="#计算熵" class="headerlink" title="计算熵"></a>计算熵</h2><p><strong>信息增益</strong>：在划分数据集之前之后信息发生的变化称为信息增益</p>
<p><strong>熵</strong>：集合信息的度量方式称为Shannon熵，简称熵</p>
<p>现在当前目录下建立tree.py文件，并在里面写下如下代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">from math import log</span><br><span class="line"></span><br><span class="line"># 计算给定数据集的shannon熵</span><br><span class="line">def caluShannonEnt(dataSet):</span><br><span class="line">    numEntries = len(dataSet)</span><br><span class="line">    # 为所有可能分类创建字典</span><br><span class="line">    labelCounts = &#123;&#125;</span><br><span class="line">    for featVec in dataSet:</span><br><span class="line">        currentLabel = featVec[-1]</span><br><span class="line">        if currentLabel not in labelCounts.keys():</span><br><span class="line">            labelCounts[currentLabel] = 0</span><br><span class="line">        labelCounts[currentLabel] += 1</span><br><span class="line">    shannonEnt = 0.0</span><br><span class="line">    for key in labelCounts:</span><br><span class="line">        prob = float(labelCounts[key])/numEntries</span><br><span class="line">        shannonEnt -= prob * log(prob,2)    # 以2为底求对数</span><br><span class="line">    return shannonEnt</span><br></pre></td></tr></table></figure>
<p>上诉代码的每一段作用都在里面用注释标明了</p>
<p>为了测试上端函数，我们可以在tree.py文件中建立自己的简单数据集</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def createDataSet():</span><br><span class="line">    dataSet = [[1,1,&apos;yes&apos;], [1,1,&apos;yes&apos;], [1,0,&apos;no&apos;], [0,1,&apos;no&apos;],[0,1,&apos;no&apos;]]</span><br><span class="line">    labels = [&apos;no surfacing&apos;, &apos;flippers&apos;]</span><br><span class="line">    return dataSet,labels</span><br></pre></td></tr></table></figure>
<p>然后在命令行中进入当前目录，然后进入python环境</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">python</span><br><span class="line">import trees</span><br><span class="line">meDat, labels = trees.createDataSet()</span><br></pre></td></tr></table></figure>
<p>效果图如下：</p>
<p><img src="/2017/03/12/决策树/1.png" alt=""></p>
<p>熵越高则混合的数据越多</p>
<h2 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h2><p>按照给定特征划分数据集，当我们按照某个特征划分数据集时，就需要将所有符合要求的元素抽取出来</p>
<p>参数：</p>
<p>dataSet：待划分的数据集<br>axis：划分数据集的特征<br>value：需要返回的特征的值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def splitDataSet(dataSet, axis, value):</span><br><span class="line">    retDataSet = []</span><br><span class="line">    for featVec in dataSet:</span><br><span class="line">        if featVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featVec[:axis]  # chop out axis used for splitting</span><br><span class="line">            reducedFeatVec.extend(featVec[axis + 1:])</span><br><span class="line">            retDataSet.append(reducedFeatVec)</span><br><span class="line">    return retDataSet</span><br></pre></td></tr></table></figure>
<p>上面的函数中创建新的list的对象，这是因为：</p>
<p>在python中函数传递的是列表的引用，若该列表在函数中被修改则会影响该列表对象的整个生存周期。为了消除这个不良影响，我们创建一个新的列表对象，以不修改原始数据</p>
<p>接下来我们要选择最好的数据划分方式，遍历整个数据集，循环计算Shannon熵和spiltDataSet()函数，找到最好的特征划分方式</p>
<p>从数据集构造决策树算法所需要的子功能模块，其工作原理如下：</p>
<ul>
<li>得到原始数据集</li>
<li>然后基于最好的属性值划分数据集</li>
<li>由于特征值可能多余两个，因此可能存在大于两个分支的数据集划分</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def chooseBestFeatureToSpilt(dataSet):</span><br><span class="line">    numFeatures = len(dataSet[0]) - 1</span><br><span class="line">    baseEntropy = caluShannonEnt(dataSet)</span><br><span class="line">    bestInfoGain = 0.0; bestFeature = -1</span><br><span class="line">    for i in range(numFeatures):</span><br><span class="line">        # 创建唯一的分类标签列表</span><br><span class="line">        featList = [example[i] for example in dataSet]</span><br><span class="line">        uniqueVals = set(featList)</span><br><span class="line">        # 从列表中创建集合是python中得到列表中唯一元素值最快的方法</span><br><span class="line">        newEntropy = 0.0</span><br><span class="line">        for value in uniqueVals:</span><br><span class="line">            # 计算每种划分方式的信息熵</span><br><span class="line">            subDataSet = splitDataSet(dataSet, i, value)</span><br><span class="line">            prob = len(subDataSet)/float(len(dataSet))</span><br><span class="line">            newEntropy += prob * caluShannonEnt(subDataSet)</span><br><span class="line">        infoGain = baseEntropy - newEntropy</span><br><span class="line">        if (infoGain &gt; bestInfoGain):</span><br><span class="line">            # 计算最好的信息增益</span><br><span class="line">            bestInfoGain = infoGain</span><br><span class="line">            bestFeature = i</span><br><span class="line">    return bestFeature</span><br></pre></td></tr></table></figure>
<p>第一次划分后，数据将被向下传递到树分支的下一个节点，在这个节点上我们可以再次划分数据。因此，我们可以采用递归的原则处理数据集</p>
<p><strong>数据需要满足的要求</strong></p>
<ul>
<li>数据必须是一种由列表元素组成的列表，而且所有的列表元素都要具备相同的数据长度</li>
<li>数据的最后一列或者每个实例的最后一个元素是当前实例的类型标签</li>
</ul>
<h2 id="递归构建决策树"><a href="#递归构建决策树" class="headerlink" title="递归构建决策树"></a>递归构建决策树</h2><p>使用分类名称的列表，然后创建键值为classList中唯一值的数据字典，字典对象存储了classCount中每个类标签出现的频率</p>
<p>然后使用operator操作键值排序字典，并返回出现最多的分类名称</p>
<p>还是在tree.py文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">import operator</span><br><span class="line"></span><br><span class="line">def majorityCnt(classList):</span><br><span class="line">    classCount = &#123;&#125;</span><br><span class="line">    for vote in classList:</span><br><span class="line">        if vote not in classCount.keys():</span><br><span class="line">            classCount[vote] = 0</span><br><span class="line">        classCount[vote] += 1</span><br><span class="line">    sortedClassCount = sorted(classCount.iteritems(), key = operator.itemgetter(1),reversed = True)</span><br><span class="line">    return sortedClassCount</span><br></pre></td></tr></table></figure>
<p>递归结束的条件是：</p>
<p>程序遍历完所有划分数据集的属性，或者下个分支下的所有实例都具有相同的分类。如果所有实例具有相同的分类，则得到一个叶子结点或者终止块</p>
<h2 id="创建树的函数代码"><a href="#创建树的函数代码" class="headerlink" title="创建树的函数代码"></a>创建树的函数代码</h2><p>参数：</p>
<p>dataSet：数据集，数据集的要求同上<br>labels：标签列表，标签列表包含了数据集中所有特征的标签，算法本身并不需要这个变量，但是为了给出数据明确的含义，我们将它作为一个参数输入</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def createTree(dataSet, labels):</span><br><span class="line">    classList = [example[-1] for example in dataSet]</span><br><span class="line">    # 类别完全相同则停止划分</span><br><span class="line">    if classList.count(classList[0]) == len(classList):</span><br><span class="line">        return classList[0]</span><br><span class="line">    # 停止条件：使用完了所有特征，但仍然不能讲数据集划分成仅包含唯一类别的分组</span><br><span class="line">    if len(dataSet[0]) == 1:</span><br><span class="line">        # 遍历完所有特征是返回出现次数最多的类别</span><br><span class="line">        return majorityCnt(classList)</span><br><span class="line">    bestFeat = chooseBestFeatureToSpilt(dataSet)</span><br><span class="line">    bestFeatLabel = labels[bestFeat]</span><br><span class="line">    # 得到列表包含的所有属性值</span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;</span><br><span class="line">    del(labels[bestFeat])</span><br><span class="line">    featValues = [example[bestFeat] for example in dataSet]</span><br><span class="line">    uniqueVals = set(featValues)</span><br><span class="line">    for value in uniqueVals:</span><br><span class="line">        subLabels = labels[:]   # 复制类标签，为保护原始数据</span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value),subLabels)</span><br><span class="line">    return myTree</span><br></pre></td></tr></table></figure>
<p>变量myTree包含了很多代表数结构的嵌套的字典，从左边开始，第一个关键字是第一个划分数据集的特征名称，该关键字也是另一个数据字典。第二个关键字是第一个特征划分的数据集，这些关键字是第一个关键字结点的子节点。这些值有可能是类标签，也可能是另一个数据字典。如果值是类标签，则该子节点是叶子结点；反之则是一个判断结点。这种格式不断重复构成了整棵树。</p>
<p>构造树的部分先到此，接下来介绍如何绘制图形</p>
<h2 id="Matplotlib的使用"><a href="#Matplotlib的使用" class="headerlink" title="Matplotlib的使用"></a>Matplotlib的使用</h2><p>默认的python环境是不自带Matplotlib的，所以我们需要安装，这里以macOS系统为例</p>
<p>首先先要安装pip，如果你已经安装过pip则直接向下看</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo easy_install pip</span><br></pre></td></tr></table></figure>
<p>接下来使用pip来继续安装</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip install matplotlib</span><br></pre></td></tr></table></figure>
<p>这里注意一下，一定要加上sudo，否则会提示你安装错误。网上很多教程都是直接pip，前面没有sudo，我在安装时出现了很多错误浪费了很多时间</p>
<p>安装好之后就可以使用Matplotlib来绘制图形了</p>
<h2 id="使用文本注解绘制树节点"><a href="#使用文本注解绘制树节点" class="headerlink" title="使用文本注解绘制树节点"></a>使用文本注解绘制树节点</h2><p>在当前目录下创建新的文件，treePlotter.py，我们关于绘制树的相关函数都会在这个文件中实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># 定义文本框与箭头格式</span><br><span class="line">decisionNode = dict(boxstyle = &quot;sawtooth&quot;, fc = &quot;0.8&quot;)</span><br><span class="line">leafNode = dict(boxstyle = &quot;round4&quot;, fc = &quot;0.8&quot;)</span><br><span class="line">arrow_args = dict(arrowstyle = &quot;&lt;-&quot;)</span><br><span class="line"></span><br><span class="line"># 绘制带箭头的注解</span><br><span class="line">def plotNode(nodeTxt, centerPt, parentPt, nodeType):</span><br><span class="line">    createPlot.axl.annotate(nodeTxt, xy=parentPt,  xycoords=&apos;axes fraction&apos;,</span><br><span class="line">             xytext=centerPt, textcoords=&apos;axes fraction&apos;,</span><br><span class="line">             va=&quot;center&quot;, ha=&quot;center&quot;, bbox=nodeType, arrowprops=arrow_args )</span><br></pre></td></tr></table></figure>
<h2 id="构造注解树"><a href="#构造注解树" class="headerlink" title="构造注解树"></a>构造注解树</h2><p>构造注解树时必需解决的第一个问题是：</p>
<p>我们必须知道有多少个叶子结点，以便可以正确确定X轴长度；以及树的层数来确定y轴长度</p>
<p>获取叶子结点的数目和树的层数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">def getNumLeaves(myTree):</span><br><span class="line">    numLeaves = 0</span><br><span class="line">    firstStr = myTree.keys()[0]</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    # 测试结点的数据类型是否为字典</span><br><span class="line">    for key in secondDict.keys():</span><br><span class="line">        if type(secondDict[key]).__name__==&apos;dict&apos;:</span><br><span class="line">            numLeaves += getNumLeaves(secondDict[key])</span><br><span class="line">        else:</span><br><span class="line">            numLeaves += 1</span><br><span class="line">    return numLeaves</span><br><span class="line"></span><br><span class="line">def getTreeDepth(myTree):</span><br><span class="line">    maxDepth = 0</span><br><span class="line">    firststr = myTree.keys()[0]</span><br><span class="line">    secondDict = myTree[firststr]</span><br><span class="line">    for key in secondDict.keys():</span><br><span class="line">        if type(secondDict[key]).__name__==&apos;dict&apos;:</span><br><span class="line">            thisDepth = 1 + getTreeDepth(secondDict[key])</span><br><span class="line">        else:</span><br><span class="line">            thisDepth = 1</span><br><span class="line">        if thisDepth&gt;maxDepth:</span><br><span class="line">            maxDepth = thisDepth</span><br><span class="line">    return maxDepth</span><br></pre></td></tr></table></figure>
<h2 id="预创建树"><a href="#预创建树" class="headerlink" title="预创建树"></a>预创建树</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">def retrieveTree(i):</span><br><span class="line">    listOfTrees =[&#123;&apos;no surfacing&apos;: &#123;0: &apos;no&apos;, 1: &#123;&apos;flippers&apos;: &#123;0: &apos;no&apos;, 1: &apos;yes&apos;&#125;&#125;&#125;&#125;,</span><br><span class="line">                  &#123;&apos;no surfacing&apos;: &#123;0: &apos;no&apos;, 1: &#123;&apos;flippers&apos;: &#123;0: &#123;&apos;head&apos;: &#123;0: &apos;no&apos;, 1: &apos;yes&apos;&#125;&#125;, 1: &apos;no&apos;&#125;&#125;&#125;&#125;</span><br><span class="line">                  ]</span><br><span class="line">    return listOfTrees[i]</span><br></pre></td></tr></table></figure>
<p>保存文件后回到命令行，输入</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import treePlotter</span><br><span class="line">treePlotter.retrieveTree(1)</span><br><span class="line">myTree = treePlotter.retrieveTree(0)</span><br><span class="line">treePlotter.getNumLeaves(myTree)</span><br><span class="line">treePlotter.getTreeDepth(myTree)</span><br></pre></td></tr></table></figure>
<p>效果图如下，函数retrieveTree()主要用于测试，返回预定义的树结构</p>
<p><img src="/2017/03/12/决策树/2.png" alt=""></p>
<p>然后我们要完善刚刚构造的注解树，将下面函数在treePlotter.py中实现，同时重写一下先前的createPlot函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"># 在父子结点间填充文本信息</span><br><span class="line">def plotMidText(cntrPt, parentPt, txtString):</span><br><span class="line">    xMid = (parentPt[0] - cntrPt[0])/2.0 + cntrPt[0]</span><br><span class="line">    yMid = (parentPt[1] - cntrPt[1])/2.0 + cntrPt[1]</span><br><span class="line">    createPlot.axl.text(xMid, yMid, txtString)</span><br><span class="line"></span><br><span class="line">def plotTree(myTree, parentPt, nodeTxt):</span><br><span class="line">    # 计算宽与高</span><br><span class="line">    numLeaves = getNumLeaves(myTree)</span><br><span class="line">    depth = getTreeDepth(myTree)</span><br><span class="line">    firstStr = myTree.keys()[0]</span><br><span class="line">    cntrPt = (plotTree.xOff + (1.0 + float(numLeaves))/2.0/plotTree.totalW,</span><br><span class="line">              plotTree.yOff)</span><br><span class="line">    # 标记子节点属性值</span><br><span class="line">    plotMidText(cntrPt, parentPt, nodeTxt)</span><br><span class="line">    plotNode(firstStr, cntrPt, parentPt, decisionNode)</span><br><span class="line">    secondDict = myTree[firstStr]</span><br><span class="line">    # 减少y偏移</span><br><span class="line">    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD</span><br><span class="line">    for key in secondDict.keys():</span><br><span class="line">        if type(secondDict[key]).__name__ == &apos;dict&apos;:</span><br><span class="line">            plotTree(secondDict[key], cntrPt, str(key))</span><br><span class="line">        else:</span><br><span class="line">            plotTree.xOff = plotTree.xOff + 1.0/plotTree.totalW</span><br><span class="line">            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff),cntrPt, leafNode)</span><br><span class="line">            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))</span><br><span class="line">    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD</span><br><span class="line"></span><br><span class="line"># 绘制图形</span><br><span class="line"># x轴与y轴的范围都是0.0 ~ 1.0</span><br><span class="line">def createPlot(inTree):</span><br><span class="line">    fig = plt.figure(1, facecolor = &apos;white&apos;)</span><br><span class="line">    fig.clf()</span><br><span class="line">    axprops = dict(xticks=[], yticks=[])</span><br><span class="line">    createPlot.axl = plt.subplot(111, frameon = False, **axprops)</span><br><span class="line">    # 储存树的宽度</span><br><span class="line">    # 树的宽度用于计算放置判断结点的位置，主要的计算原则是将它放在所有叶子结点的中间，而不仅仅是他子节点的中间</span><br><span class="line">    plotTree.totalW = float(getNumLeaves(inTree))</span><br><span class="line">    # 储存树的深度</span><br><span class="line">    plotTree.totalD = float(getTreeDepth(inTree))</span><br><span class="line">    # 全局变量xOff, yOff 来追踪已经绘制的结点的位置，以及下一个结点的恰当位置</span><br><span class="line">    plotTree.xOff = -0.5/plotTree.totalW; plotTree.yOff = 1.0</span><br><span class="line">    plotTree(inTree, (0.5, 1.0), &apos;&apos;)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>
<p>现在让我们验证一下实际效果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">reload(treePlotter)</span><br><span class="line">myTree = treePlotter.retrieveTree(0)</span><br><span class="line">treePlotter.createPlot(myTree)</span><br></pre></td></tr></table></figure>
<p>这样，我们就会得到如下的注解树</p>
<p><img src="/2017/03/12/决策树/3.png" alt=""></p>
<h2 id="使用决策树执行分类"><a href="#使用决策树执行分类" class="headerlink" title="使用决策树执行分类"></a>使用决策树执行分类</h2><p>绘制树的部分结束了，我们重新回到tree.py中，我们接下来要实现使用决策树算法来分类数据集了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def classify(inputTree, featLabels, testVec):</span><br><span class="line">    firstStr = inputTree.keys()[0]</span><br><span class="line">    secondDict = inputTree[firstStr]</span><br><span class="line">    featIndex = featLabels.index(firstStr)</span><br><span class="line">    for key in secondDict.keys():</span><br><span class="line">        if testVec[featIndex] == key:</span><br><span class="line">            if type(secondDict[key]).__name__ == &apos;dict&apos;:</span><br><span class="line">                classLabel = classify(secondDict[key], featLabels, testVec)</span><br><span class="line">            else:</span><br><span class="line">                classLabel = secondDict[key]</span><br><span class="line">    return classLabel</span><br></pre></td></tr></table></figure>
<p>在上面的函数中我们将标签类型转换为引索，这是为了解决程序无法确定特征在数据集中的位置。</p>
<p>特征标签列表将帮助程序处理这个问题</p>
<h2 id="决策树的储存"><a href="#决策树的储存" class="headerlink" title="决策树的储存"></a>决策树的储存</h2><p>我们可以将分类器储存在硬盘上，而不用每次对数据分类是重新学习一遍，这也是决策树的优点之一，k-means算法就无法持久化分类器</p>
<p>我们可以预选提炼并储存数据集中包含的知识信息，在需要对事物进行分类是再使用这些知识</p>
<p>我们使用pickle模块来储存决策树</p>
<p>储存决策树的原因：为节省时间，能够在每次执行分类时调用已经构造好的决策树</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def storeTree(inputTree, fileName):</span><br><span class="line">    import pickle</span><br><span class="line">    fw = open(fileName,&apos;w&apos;)</span><br><span class="line">    pickle.dump(inputTree, fw)</span><br><span class="line">    fw.close()</span><br><span class="line"></span><br><span class="line">def grabTree(fileName):</span><br><span class="line">    import pickle</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    return pickle.load(fr)</span><br></pre></td></tr></table></figure>
<h2 id="示例：使用决策树预测隐形眼镜类型"><a href="#示例：使用决策树预测隐形眼镜类型" class="headerlink" title="示例：使用决策树预测隐形眼镜类型"></a>示例：使用决策树预测隐形眼镜类型</h2><p>步骤：</p>
<ul>
<li>收集数据：提供的文本文件</li>
<li>准备数据：解析tab键分隔的数据行</li>
<li>分析数据：快速检查数据，确保正确的解析数据内容，使用creatrPlot()函数绘制最终的树形图</li>
<li>训练算法：使用createTree()</li>
<li>测试算法：编写测试函数验证决策树可以正确分类给定的数据实例</li>
<li>使用算法：储存树的结构，以便下次使用时无序重新构造树</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">reload(treePlotter)</span><br><span class="line">fr = open(&apos;lenses.txt&apos;)</span><br><span class="line">lenses = [inst.strip().split(&apos;\t&apos;) for inst in fr.readlines()]</span><br><span class="line">lensesLabels = [&apos;age&apos;, &apos;prescript&apos;, &apos;astigmatic&apos;, &apos;tearRate&apos;]</span><br><span class="line">lensesTree = trees.createTree(lenses, lensesLabels)</span><br><span class="line">treePlotter.createPlot(lensesLabels)</span><br></pre></td></tr></table></figure>
<p>得到的注解树如下所示：</p>
<p><img src="/2017/03/12/决策树/5.png" alt=""></p>
<p>上图所示的决策树非常好的匹配了实验数据，然而这些匹配选项可能太多了。我们将这种情况称之为过拟合(overfitting)。</p>
<p>为了避免过拟合问题，我们可以裁剪决策树，去掉一些没必要的叶子结点。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>决策树分类器就像带有终止块的流程图，终止块表示结果。</p>
<p>开始处理数据时，我们要先测量一下数据集中的熵，然后寻找最优方案划分数据集，知道数据集中的所有数据属于同一分类。</p>
<p>本文采用的构建决策树的算法是ID3算法，该算法可以用于划分标称型数据集。</p>
<p>在构建决策树时，我们通常采用递归的方式将数据集转化为决策树。一般我们不使用新的数据结构，因为python中的字典储存树结点信息已经足够了</p>
<p>决策树算法和以前整理的k-means算法都是结果确定的分类算法，数据实例最终会被明确划分到某个分类中。</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2017/03/12/决策树/">
    <time datetime="2017-03-12T05:13:42.000Z" class="entry-date">
        2017-03-12
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/ML/">ML</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2017/03/15/Logistic回归与Sigmord函数/" rel="prev"><span class="meta-nav">←</span> Logistic回归与Sigmord函数</a></span>
    
    
        <span class="nav-next"><a href="/2017/03/07/k-means算法的实现/" rel="next">K-Means算法的实现 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s">
        <input type="submit" id="searchsubmit" value="搜索">
    </div>
</form></aside>
  
    
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/LIFE/">LIFE</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/MAC/">MAC</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML/">ML</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/PGM/">PGM</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/iOS/">iOS</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/御宅文化/">御宅文化</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">35</span></li></ul>
    </div>
  </aside>

  
    
<div class="widget tag">
<h3 class="title">blogroll</h3>
<ul class="entry">


<li><a href="https://github.com/" target="_blank">我的github</a></li>


<li><a href="http://www.jianshu.com/users/41cd7711ed44/latest_articles" target="_blank">我的简书主页</a></li>


<li><a href="http://uuzdaisuki.com" target="_blank">leticia’s blog</a></li>


<li><a href="http://www.helloyzy.cn" target="_blank">acery</a></li>


<li><a href="http://xjin.wang" target="_blank">WXJACKIE</a></li>


<li><a href="http://www.stephenzhang.me" target="_blank">stephenzhang</a></li>


<li><a href="blog.keybrl.com" target="_blank">keybrl</a></li>


<li><a href="http://blog.ciaran.cn" target="_blank">Ciaran</a></li>


<li><a href="http://1.dev.blog.qinka.pro" target="_blank">Qinka</a></li>


<li><a href="http://tobiasLee.top" target="_blank">TobiasLee</a></li>


<li><a href="http://blog.boileryao.com" target="_blank">bingo</a></li>

</ul>
</div>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2018/11/21/白话迁移学习/">简述迁移学习</a>
          </li>
        
          <li>
            <a href="/2018/07/30/reinforcement-learning-and-self-play/">Reinforcement Learning &amp; Self-Play</a>
          </li>
        
          <li>
            <a href="/2018/03/23/EMTC-md/">Extreme Multi-label Text Classification:Kim-CNN &amp; XML-CNN</a>
          </li>
        
          <li>
            <a href="/2017/12/17/FastRCNN-FasterRCNN/">谈一谈 Fast R-CNN 和 Faster R-CNN</a>
          </li>
        
          <li>
            <a href="/2017/11/17/如何在Xcode中添加-bits-stdc-h-头文件/">如何在Xcode中添加&lt;bits/stdc++.h&gt;头文件</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cloud-Computing/">Cloud Computing</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cocoapods/">Cocoapods</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matlab/">Matlab</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Networks/">Neural Networks</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Graphical-Models/">Probabilistic Graphical Models</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIColor/">UIColor</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIView/">UIView</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIView-圆角/">UIView-圆角</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vim/">Vim</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bug/">bug</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/openCV/">openCV</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/二维码/">二维码</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/指针/">指针</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/操作系统/">操作系统</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据结构/">数据结构</a><span class="tag-list-count">19</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a><span class="tag-list-count">8</span></li></ul>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2018 SaberDa
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
</footer>
    <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

<script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
</body>
</html>