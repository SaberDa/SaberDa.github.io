<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  
  <meta name="description" content="阿尔托利亚是我老婆~">
  

  
  
  
  
  
  
  <title>PCA降维 | SaberDa的幻想乡</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="关于PCA算法的具体介绍在下面这篇文章中，这里我就简单介绍一下，如果大家想要完整的了解PCA，戳下面那个连接就是了 Machine Learning-8 本文中的代码以及数据文件在这里 代码与数据 在介绍PCA之前，先谈谈 什么是降维？举个例子，比如你现在再看手机，手机正在播放复联。众所周知，屏幕是由无数个像素点组成的，比如100W个吧，不算多。而你在看复联的时候你关注的是英雄的动作以及周围的场面">
<meta name="keywords" content="Machine Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="PCA降维">
<meta property="og:url" content="https://saberda.github.io/2017/03/22/PCA降维/index.html">
<meta property="og:site_name" content="SaberDa的幻想乡">
<meta property="og:description" content="关于PCA算法的具体介绍在下面这篇文章中，这里我就简单介绍一下，如果大家想要完整的了解PCA，戳下面那个连接就是了 Machine Learning-8 本文中的代码以及数据文件在这里 代码与数据 在介绍PCA之前，先谈谈 什么是降维？举个例子，比如你现在再看手机，手机正在播放复联。众所周知，屏幕是由无数个像素点组成的，比如100W个吧，不算多。而你在看复联的时候你关注的是英雄的动作以及周围的场面">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://saberda.github.io/2017/03/22/PCA降维/1.png">
<meta property="og:updated_time" content="2017-03-22T17:22:18.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PCA降维">
<meta name="twitter:description" content="关于PCA算法的具体介绍在下面这篇文章中，这里我就简单介绍一下，如果大家想要完整的了解PCA，戳下面那个连接就是了 Machine Learning-8 本文中的代码以及数据文件在这里 代码与数据 在介绍PCA之前，先谈谈 什么是降维？举个例子，比如你现在再看手机，手机正在播放复联。众所周知，屏幕是由无数个像素点组成的，比如100W个吧，不算多。而你在看复联的时候你关注的是英雄的动作以及周围的场面">
<meta name="twitter:image" content="https://saberda.github.io/2017/03/22/PCA降维/1.png">
  
    <link rel="alternative" href="/atom.xml" title="SaberDa的幻想乡" type="application/atom+xml">
  
  
    <link rel="icon" href="/css/images/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  

  
  <!-- baidu webmaster push -->
  <script src="//push.zhanzhang.baidu.com/push.js"></script>
</head></html>
<body class="home blog custom-background custom-font-enabled single-author">
  <div id="page" class="hfeed site">
      <header id="masthead" class="site-header" role="banner">
    <hgroup>
      <h1 class="site-title">
        <a href="/" title="SaberDa的幻想乡" rel="home">SaberDa的幻想乡</a>
      </h1>
      
        <h2 class="site-description">
          <a href="/" id="subtitle">　　C++/ JS　　|　　呐呐呐　　|　　gli97@gwmail.gwu.edu　　|　　没有什么胜利可言  挺住就意味着一切</a>
        </h2>
      
    </hgroup>

    <nav id="site-navigation" class="main-navigation" role="navigation">
            <button class="menu-toggle">菜单</button>
            <a class="assistive-text" href="/#content" title="跳至内容">跳至内容</a><!--TODO-->
            <div class="menu-main-container">
                <ul id="menu-main" class="nav-menu">
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/archives">所有文章</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/">主页</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/编程/">编程</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/iOS/">iOS</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/ML/">ML</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/PGM/">PGM</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/MAC/">MAC</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/御宅文化/">御宅文化</a></li>
                
                    <li class="menu-item menu-item-type-post_type menu-item-object-page"><a href="/categories/LIFE/">LIFE</a></li>
                
                </ul>
            </div>
    </nav>
</header>
      <div id="main" class="wrapper">
        <div id="primary" class="site-content"><div id="content" role="main"><article id="post-PCA降维" class="post-PCA降维 post type-post status-publish format-standard hentry">
    <!---->

      <header class="entry-header">
        
        
  
    <h1 class="entry-title article-title">
      PCA降维
    </h1>
  

        
        <div class="comments-link">
            
            <a href="javascript:void(0);" data-url="https://saberda.github.io/2017/03/22/PCA降维/" data-id="ck7slgtxq002tijhzl9vtdq77" class="leave-reply bdsharebuttonbox" data-cmd="more">Share</a>
        </div><!-- .comments-link -->
      </header><!-- .entry-header -->

    <div class="entry-content">
      
        <p>关于PCA算法的具体介绍在下面这篇文章中，这里我就简单介绍一下，如果大家想要完整的了解PCA，戳下面那个连接就是了</p>
<p><a href="http://www.saberismywife.com/2016/12/31/Machine-Learning-8/" target="_blank" rel="noopener">Machine Learning-8</a></p>
<p>本文中的代码以及数据文件在这里</p>
<p><a href="https://github.com/SaberDa/PCA-in-python" target="_blank" rel="noopener">代码与数据</a></p>
<p>在介绍PCA之前，先谈谈</p>
<h2 id="什么是降维？"><a href="#什么是降维？" class="headerlink" title="什么是降维？"></a>什么是降维？</h2><p>举个例子，比如你现在再看手机，手机正在播放复联。众所周知，屏幕是由无数个像素点组成的，比如100W个吧，不算多。而你在看复联的时候你关注的是英雄的动作以及周围的场面，这是个三维的场景。</p>
<p>在这个过程中，人脑已经将数据从100W维降到了3维。</p>
<p>这就是降维(dimensionality reduction)。</p>
<a id="more"></a>
<h2 id="为什么要降维？"><a href="#为什么要降维？" class="headerlink" title="为什么要降维？"></a>为什么要降维？</h2><ul>
<li>使得数据集更易使用</li>
<li>降低很多算法的计算开销</li>
<li>去除噪声</li>
<li>使得结果易懂</li>
<li>将其特征显现出来（将其特征显现出来通常是我们设计机器学习算法的第一步。）</li>
</ul>
<p>在这篇文章中我将主要介绍未标注数据集的降维技术，当然，该技术同样也可以应用于已标注的数据。</p>
<h2 id="先介绍一些常见的算法"><a href="#先介绍一些常见的算法" class="headerlink" title="先介绍一些常见的算法"></a>先介绍一些常见的算法</h2><p>第一种降维算法称为<strong>主成分分析(Principal Component Analysis)，简称PCA</strong>。</p>
<p>在PCA中，数据从原来的坐标系转换到了新的坐标系，这个新坐标系的选择是由数据本身决定的。第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。该过程一直重复，重复次数为原始数据中特征的数目。之后我们会发现，大部分方差都包含在最前面的几个新坐标轴中。因此我们可以忽略余下的坐标轴，即对数据做了降维处理。</p>
<p>另外一种降维算法是<strong>因子分析(Factor Analysis)。</strong></p>
<p>在因子分析中，我么假设在观察数据的生成中含有一些观测不到的隐变量(latent variable)。假设观察数据是这些隐变量和某些噪声的线性组合。那么隐变量的数据可能比观察数据的数目要少，也就是说通过找到隐变量就可以是实现数据的降维。这种方法已经应用于社会科学、金融等领域中了。</p>
<p>还有一种降维算法是<strong>独立成分分析(Independent Component Analysis)，简称ICA。</strong></p>
<p>ICA假设数据是从n个数据源产生的，这一点与因子分析类似。假设数据为多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，而在PCA中只假设数据是不相关的。同因子分析一样，如果数据源的数目少于观察数据的数目，则可以实现降维过程。</p>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><p>优点：降低数据的复杂性，识别最重要的多个特征<br>缺点：不一定需要，且有可能损失有用信息<br>试用数据类型：数值型数据</p>
<p><strong>通过PCA进行降维处理，我们就可以同时获得SVM和决策树的优点</strong>：</p>
<ul>
<li>一方面，得到了和决策树一样的分类器</li>
<li>另一方面，分类间隔和SVM一样好</li>
</ul>
<p>既然叫主成分分析，那么</p>
<h2 id="主成分有哪些？"><a href="#主成分有哪些？" class="headerlink" title="主成分有哪些？"></a>主成分有哪些？</h2><p>第一个主成分是从数据差异性最大（即方差最大）的方向中提取出来的<br>第二个主成分则来自与数据差异性次大的方向，并且该方向与第一个主成分方向正交。</p>
<h2 id="主成分怎么获得？"><a href="#主成分怎么获得？" class="headerlink" title="主成分怎么获得？"></a>主成分怎么获得？</h2><p>通过数据集的协方差矩阵及其特征值分析，我们就可以得出这些主成分的值</p>
<p>一旦得到了协方差矩阵的特征向量，我们就可以保留最大的N个值。这些特征向量也给出了N个最重要特征的真实结构。我们可以通过将数据乘上这N个特征向量将它转换到新的向量空间</p>
<p><strong>将数据转换为N个主成分的大概步骤如下</strong></p>
<ul>
<li>去除平均值</li>
<li>计算协方差矩阵</li>
<li>计算协方差矩阵的特征值和特征向量</li>
<li>将特征值从大到小排序</li>
<li>保留最上面的N个特征向量</li>
<li>将数据转换到上诉N个特征向量构建的新空间中</li>
</ul>
<p>先建立一个名为pca.py的文件</p>
<h2 id="然后代码如下："><a href="#然后代码如下：" class="headerlink" title="然后代码如下："></a>然后代码如下：</h2><p>当然第一步还是导入数据，这里有些不同的是使用了两个list comprehension 来构建矩阵</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from numpy import *</span><br><span class="line"></span><br><span class="line">def loadDataSet(fileName, delim = &apos;\t&apos;):</span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    stringArr = [line.strip().split(delim) for line in fr.readlines()]</span><br><span class="line">    datArr = [map(float,line) for line in stringArr]</span><br><span class="line">    return mat(datArr)</span><br></pre></td></tr></table></figure>
<p>pca函数有两个参数：</p>
<p>第一个参数是用于进行pca操作的数据集</p>
<p>第二个参数是个可选特征，即应用的N个特征。如果不指定该特征，那么函数会返回前9999999个特征，或者原始数据中的全部特征</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">def pca(dataMat, topNfeat = 9999999):</span><br><span class="line">    meanVals = mean(dataMat, axis = 0)</span><br><span class="line">    # 首先去平均值</span><br><span class="line">    meanRemoved = dataMat - meanVals</span><br><span class="line">    covMat = cov(meanRemoved, rowvar =False)</span><br><span class="line">    eigVals, eigVects = linalg.eig(mat(covMat))</span><br><span class="line">    eigValInd = argsort(eigVals)</span><br><span class="line">    # 从小到大对N个值排序</span><br><span class="line">    eigValInd = eigValInd[: -(topNfeat+1) : -1]</span><br><span class="line">    redEigVects = eigVects[:, eigValInd]</span><br><span class="line">    # 将数据切换到新的空间</span><br><span class="line">    lowDDataMat = meanRemoved * redEigVects</span><br><span class="line">    reconMat = (lowDDataMat * redEigVects.T) + meanVals</span><br><span class="line">    return lowDDataMat, reconMat</span><br></pre></td></tr></table></figure>
<p>然后在终端里输入以下命令检验一下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">import pca</span><br><span class="line">from numpy import *</span><br><span class="line">dataMat = pca.loadDataSet(&apos;testSet.txt&apos;)</span><br><span class="line">lowDMat, reconMat = pca.pca(dataMat, 1)</span><br><span class="line">shape(lowDMat)</span><br><span class="line">import matplotlib</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">fig = figure()</span><br><span class="line">ax = fig.add_subplot(111)</span><br><span class="line">ax.scatter(dataMat[:,0].flatten().A[0], dataMat[:,1].flatten().A[0], marker = &apos;^&apos;, s = 90)</span><br><span class="line">ax.scatter(reconMat[:,0].flatten().A[0], reconMat[:,1].flatten().A[0], marker = &apos;o&apos;, s = 50, c = &apos;red&apos;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>运行的结果如下</p>
<p><img src="/2017/03/22/PCA降维/1.png" alt=""></p>
<h2 id="Matlab实现"><a href="#Matlab实现" class="headerlink" title="Matlab实现"></a>Matlab实现</h2><p>代码具体思想见我在开篇时的那篇链接</p>
<p>算法公式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">function [U, S] = pca(X)</span><br><span class="line"></span><br><span class="line">	[m, n] = size(X);</span><br><span class="line"></span><br><span class="line">	U = zeros(n);</span><br><span class="line">	S = zeros(n);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">	Sigma = 1 / m * X&apos; * X;</span><br><span class="line">	[U,S,V] = svd(Sigma);</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>数据处理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">function Z = projectData(X, U, K)</span><br><span class="line"></span><br><span class="line">	Z = zeros(size(X, 1), K);</span><br><span class="line"></span><br><span class="line">	for i = 1:size(X,1)</span><br><span class="line">  		x = X(i,:)&apos;;</span><br><span class="line">  		Z(i,:) = x&apos; * U(:,1:K);</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function X_rec = recoverData(Z, U, K)</span><br><span class="line"></span><br><span class="line">X_rec = zeros(size(Z, 1), size(U, 1));</span><br><span class="line"></span><br><span class="line">	for i=1:size(Z, 1)</span><br><span class="line">		v = Z(i, :)&apos;;</span><br><span class="line">    	</span><br><span class="line">    		for j=1: size(U, 1)</span><br><span class="line">    		</span><br><span class="line">       	 	X_rec(i, j) = v&apos; * U(j, 1:K)&apos;;</span><br><span class="line">       	 	</span><br><span class="line">    		end</span><br><span class="line">	end</span><br><span class="line"></span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>降维思想使得数据变得更容易使用，并且他们往往能够去除数据中的噪声，使得其他的机器学习更加准确。</p>
<p>PCA算法的本质就是：从数据中识别其主要特征，它是通过沿着数据最大方差方向旋转坐标轴来实现的。选择方差最大的方向作为第一条坐标轴，后续坐标轴则与前面的坐标轴正交。协方差矩阵上的特征值分析可以用一系列的正交坐标来获取。</p>
<h2 id="实战：数据集中的特征中几乎每个特征都有NaN-Not-a-Number-该怎样处理"><a href="#实战：数据集中的特征中几乎每个特征都有NaN-Not-a-Number-该怎样处理" class="headerlink" title="实战：数据集中的特征中几乎每个特征都有NaN(Not a Number)该怎样处理"></a>实战：数据集中的特征中几乎每个特征都有NaN(Not a Number)该怎样处理</h2><p>关于有些数据不能用的处理方法我在上篇博客中已经解释了，但是还是要因地制宜。所以就拿这个实战数据来解释一下。</p>
<p>首先，因为在这些特征（共590个）中，几乎所有样本都有 NaN ，因此除去不完整的样本这个方法不太现实。<br>其次，尽管我们可以用0来进行替换，但是由于我们不知道这些值的意义，所以这也是个下策。举个例子来证明是下策，比如它们都是开氏温度，那么将它们设置成0未免有些太扯淡了。</p>
<p>这时，我们可以采用用平均值来替代这个方法来解决这个问题。</p>
<p><a href="http://archive.ics.uci.edu/ml/machine-learning-databases/secom/" target="_blank" rel="noopener">该实战的数据来源</a></p>
<p>这是替代为平均值的函数，添加到pca.py文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def replaceNanWithMean():</span><br><span class="line">    dataMat = loadDataSet(&apos;secom.data.txt&apos;, &apos;&apos;)</span><br><span class="line">    numFeat = shape(dataMat)[1]</span><br><span class="line">    for i in range(numFeat):</span><br><span class="line">        # 计算所有非 NaN 的平均值</span><br><span class="line">        meanVal = mean(dataMat[nonzero(~isnan(dataMat[:,i].A))[0],i])</span><br><span class="line">        # 将所有 NaN 置为平均值</span><br><span class="line">        dataMat[nonzero(isnan(dataMat[:,i].A))[0], i] = meanVal</span><br><span class="line">    return dataMat</span><br></pre></td></tr></table></figure>
<p>接下来在终端里输入如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dataMat = pca.replaceNanWithMean()</span><br><span class="line">meanVals = mean(dataMat, axis = 0)</span><br><span class="line">meanRemoved = dataMat - meanVals</span><br><span class="line">covMat = cov(meanRemoved, rowvar = 0)</span><br><span class="line">eigVals, eigVects = linalg.eig(mat(covMat))</span><br><span class="line">eigVals</span><br></pre></td></tr></table></figure>
<p>这样你就可以看到效果了</p>
<h2 id="另需要注意的事"><a href="#另需要注意的事" class="headerlink" title="另需要注意的事"></a>另需要注意的事</h2><p>PCA会给出数据中所包含的信息量。</p>
<p>需要特别强调的是，<strong>数据(data)</strong>和<strong>信息(information)</strong>之间有着巨大的差别。</p>
<ul>
<li>数据是指接受的原始材料，其中可能包含噪声和不相关的信息。</li>
<li>信息是指数据中的相关部分。</li>
</ul>
<p>这些并非是抽象概念，我们还可以定量的计算数据中所包含的信息并决定保留的比例</p>

      
    </div><!-- .entry-content -->

    <footer class="entry-meta">
    <a href="/2017/03/22/PCA降维/">
    <time datetime="2017-03-23T02:35:47.000Z" class="entry-date">
        2017-03-22
    </time>
</a>
    
  <span class="article-delim">&#8226;</span>
  <div class="article-category">
  <a class="article-category-link" href="/categories/ML/">ML</a>
  </div>

    
  <span class="article-delim">&#8226;</span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a></li></ul>

    </footer>
</article>


    
<nav class="nav-single">
    <h3 class="assistive-text">文章导航</h3>
    
        <span class="nav-previous"><a href="/2017/03/27/install-opencv3/" rel="prev"><span class="meta-nav">←</span> Problems in Installing openCV on Mac</a></span>
    
    
        <span class="nav-next"><a href="/2017/03/15/Logistic回归与Sigmord函数/" rel="next">Logistic回归与Sigmord函数 <span class="meta-nav">→</span></a></span>
    
</nav><!-- .nav-single -->







</div></div>
        <div id="secondary" class="widget-area" role="complementary">
  
    <aside id="search" class="widget widget_search"><form role="search" method="get" accept-charset="utf-8" id="searchform" class="searchform" action="//google.com/search">
    <div>
        <input type="text" value="" name="s" id="s">
        <input type="submit" id="searchsubmit" value="搜索">
    </div>
</form></aside>
  
    
  
    
  <aside class="widget">
    <h3 class="widget-title">Categories</h3>
    <div class="widget-content">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/LIFE/">LIFE</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/MAC/">MAC</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ML/">ML</a><span class="category-list-count">29</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/PGM/">PGM</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/iOS/">iOS</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/御宅文化/">御宅文化</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/算法/">算法</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/编程/">编程</a><span class="category-list-count">39</span></li></ul>
    </div>
  </aside>

  
    
<div class="widget tag">
<h3 class="title">blogroll</h3>
<ul class="entry">


<li><a href="https://github.com/" target="_blank">我的github</a></li>


<li><a href="http://www.jianshu.com/users/41cd7711ed44/latest_articles" target="_blank">我的简书主页</a></li>


<li><a href="http://uuzdaisuki.com" target="_blank">leticia’s blog</a></li>


<li><a href="http://www.helloyzy.cn" target="_blank">acery</a></li>


<li><a href="http://xjin.wang" target="_blank">WXJACKIE</a></li>


<li><a href="http://www.stephenzhang.me" target="_blank">stephenzhang</a></li>


<li><a href="blog.keybrl.com" target="_blank">keybrl</a></li>


<li><a href="http://blog.ciaran.cn" target="_blank">Ciaran</a></li>


<li><a href="http://1.dev.blog.qinka.pro" target="_blank">Qinka</a></li>


<li><a href="http://tobiasLee.top" target="_blank">TobiasLee</a></li>


<li><a href="http://blog.boileryao.com" target="_blank">bingo</a></li>

</ul>
</div>

  
    
  <aside class="widget">
    <h3 class="widget-title">Recents</h3>
    <div class="widget-content">
      <ul>
        
          <li>
            <a href="/2020/03/15/LeetCode刷题记录-Part2/">LeetCode刷题记录-Part2</a>
          </li>
        
          <li>
            <a href="/2020/02/20/HashCode-2020/">HashCode 2020</a>
          </li>
        
          <li>
            <a href="/2020/01/19/LeetCode刷题记录-Part1/">LeetCode刷题记录-Part1</a>
          </li>
        
          <li>
            <a href="/2019/08/13/前方乃是未踏之旅/">前方乃是未踏之旅</a>
          </li>
        
          <li>
            <a href="/2019/03/31/毕设挖坑笔记-Git-lfs的使用/">毕设填坑笔记-Git lfs的使用</a>
          </li>
        
      </ul>
    </div>
  </aside>

  
    
  <aside class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-content">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cloud-Computing/">Cloud Computing</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cocoapods/">Cocoapods</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LeetCode/">LeetCode</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/">Linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Machine-Learning/">Machine Learning</a><span class="tag-list-count">17</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Matlab/">Matlab</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Neural-Networks/">Neural Networks</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probabilistic-Graphical-Models/">Probabilistic Graphical Models</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIColor/">UIColor</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIView/">UIView</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UIView-圆角/">UIView-圆角</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Vim/">Vim</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/bug/">bug</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hexo/">hexo</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/openCV/">openCV</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/二维码/">二维码</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/填坑笔记/">填坑笔记</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/指针/">指针</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/操作系统/">操作系统</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据结构/">数据结构</a><span class="tag-list-count">19</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/算法/">算法</a><span class="tag-list-count">8</span></li></ul>
    </div>
  </aside>

  
</div>
      </div>
      <footer id="colophon" role="contentinfo">
    <p>&copy; 2020 SaberDa
    All rights reserved.</p>
    <p>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></p>
</footer>
    <script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"16"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>

<script src="/js/jquery-2.0.3.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

<script src="/js/navigation.js"></script>

<div id="bg"></div>

  </div>
</body>
</html>